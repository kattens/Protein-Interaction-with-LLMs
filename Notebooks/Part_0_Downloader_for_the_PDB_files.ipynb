{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ThRqdFlT-QvRDr-s08Azx8IbMEbFRxNx",
      "authorship_tag": "ABX9TyPp8qumUrSY8IgjR5PZm6z1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kattens/Protein-Interaction-with-LLMs/blob/main/Part_0_Downloader_for_the_PDB_files.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading the PDB Files\n",
        "\n",
        "The first step of our project is to gather the data we want to work with. We will start by using a list of 10 CSV files containing the names of all the files on the RCSB website. After obtaining these lists, we will perform some necessary cleanups.\n",
        "\n",
        "this file also contains the steps to seperate the chains in a pdb file since we need to work with seperated chains for our analysis."
      ],
      "metadata": {
        "id": "sJtj5_XqT7Kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install the needed library\n",
        "!pip install biopython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llXMVb92WFkb",
        "outputId": "32d1e6eb-e562-4787-8beb-b3557223cda8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting biopython\n",
            "  Downloading biopython-1.83-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.25.2)\n",
            "Installing collected packages: biopython\n",
            "Successfully installed biopython-1.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "from Bio import PDB\n",
        "import shutil\n",
        "import requests\n"
      ],
      "metadata": {
        "id": "dHzh8uhdWLTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# path of the csv file -> modify it as needed\n",
        "folder_path = \"/content/drive/MyDrive/pdb_entry_files.csv\"\n",
        "\n",
        "first_column_values = []\n",
        "\n",
        "# Loop through each file in the specified folder\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith(\".csv\"):  # Check if the file is a CSV\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "        with open(file_path, 'r') as csv_in:\n",
        "            reader = csv.reader(csv_in)\n",
        "            header = next(reader)  # Skip the header\n",
        "\n",
        "            for row in reader:\n",
        "                first_column_values.append(row[0])  # Append the value from the first column\n",
        "\n",
        "print(first_column_values)  #name of the pdb files\n",
        "print(len(first_column_values)) #number of pdb files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "UvN7M5MoWa_R",
        "outputId": "d130be15-c428-4b85-ec3f-52854262b628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotADirectoryError",
          "evalue": "[Errno 20] Not a directory: '/content/drive/MyDrive/pdb_entry_files.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-9e7eb0685af4>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Loop through each file in the specified folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Check if the file is a CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/content/drive/MyDrive/pdb_entry_files.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As shown above, we have the names of each PDB file in the list. We will send requests to download these files. Due to the large number of files, we will process them in 10 batches."
      ],
      "metadata": {
        "id": "zU1pEzk1XJGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PDBDownloader:\n",
        "    def __init__(self, output_directory):\n",
        "        self.output_directory = output_directory\n",
        "\n",
        "    def download_pdb_batch(self, protein_name_list, batch_number):\n",
        "        batch_size = 7500  # Assuming 75000 files divided into 10 batches\n",
        "        start_index = (batch_number - 1) * batch_size\n",
        "        end_index = min(batch_number * batch_size, len(protein_name_list))\n",
        "\n",
        "        for i in range(start_index, end_index):\n",
        "            protein_name = protein_name_list[i]\n",
        "            filename = f\"{protein_name}.pdb\"\n",
        "            file_path = os.path.join(self.output_directory, filename)\n",
        "\n",
        "            if os.path.exists(file_path):\n",
        "                print(f\"File already exists for {protein_name}, skipping download.\")\n",
        "                continue\n",
        "\n",
        "            url = f\"https://files.rcsb.org/download/{protein_name}.pdb\"\n",
        "\n",
        "            try:\n",
        "                response = requests.get(url)\n",
        "                response.raise_for_status()\n",
        "                pdb_content = response.text\n",
        "\n",
        "                if \"HEADER    \" not in pdb_content:\n",
        "                    print(f\"No PDB file found for {protein_name}\")\n",
        "                    continue\n",
        "\n",
        "                with open(file_path, \"w\") as file:\n",
        "                    file.write(pdb_content)\n",
        "                print(f\"Downloaded PDB file for {protein_name} to {self.output_directory}\")\n",
        "            except requests.HTTPError as e:\n",
        "                print(f\"Failed to download PDB file for {protein_name}\")\n",
        "                print(f\"HTTP Error: {e}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to download PDB file for {protein_name}\")\n",
        "                print(f\"Error: {e}\")\n"
      ],
      "metadata": {
        "id": "bV39EVhBW4xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notes for handling this part:\n",
        "\n",
        "since we dont want to overload the memory and have enough disk space, we will download everything in 10 batches, clear the previous directory, process the files as we want, then go to the next batch. we should do it manually by changing the batch numbers as a hypert parameter."
      ],
      "metadata": {
        "id": "esqFEZKnasSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#clear the folder that we are downloading the pdb files in to make sure we arent duplicating anything\n",
        "def clear_directory(directory):\n",
        "    for item in os.listdir(directory):\n",
        "        item_path = os.path.join(directory, item)\n",
        "        if os.path.isfile(item_path) or os.path.islink(item_path):\n",
        "            os.unlink(item_path)\n",
        "        elif os.path.isdir(item_path):\n",
        "            shutil.rmtree(item_path)"
      ],
      "metadata": {
        "id": "j7Sc53grauLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Folder containing the CSV files  -> change as needed\n",
        "folder_path = \"/home/f.ensafitakaldani001/Downloader_code/IDs\"\n",
        "first_column_values = []\n",
        "\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith(\".csv\"):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        with open(file_path, 'r') as csv_in:\n",
        "            reader = csv.reader(csv_in)\n",
        "            next(reader)  # Skip the header\n",
        "            for row in reader:\n",
        "                first_column_values.append(row[0])\n",
        "\n",
        "# Specify the output directory -> change as needed\n",
        "output_directory = \"/home/f.ensafitakaldani001/Downloader_code/newoutput\"\n",
        "clear_directory(output_directory)  # Clear the directory before downloading\n",
        "\n",
        "# Create an instance of PDBDownloader\n",
        "downloader = PDBDownloader(output_directory)\n",
        "\n",
        "# Define the batch number you want to download\n",
        "batch_number = 10  # Adjust this number to download different batches\n",
        "\n",
        "# Download the PDB files for the current batch\n",
        "downloader.download_pdb_batch(first_column_values, batch_number)\n",
        "\n",
        "print(f\"Batch {batch_number} download complete.\")"
      ],
      "metadata": {
        "id": "7wW72UC3bmO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chain Seperator:\n",
        "one of the most necessary parts of this project is to have seperated chains from pdb files. since we want to train a model base on the closest amino acids in different chains in one protein complex to understand the dynamics of amino acids that tend to get closer to one another."
      ],
      "metadata": {
        "id": "vipGuaorcKnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def separate_chains(source_dir, target_dir, limit=500):\n",
        "    os.makedirs(target_dir, exist_ok=True)  # Ensure the target directory exists\n",
        "    pdb_parser = PDB.PDBParser()\n",
        "    pdb_io = PDB.PDBIO()\n",
        "    processed_files = 0\n",
        "\n",
        "    for filename in sorted(os.listdir(source_dir)):  # Sort to ensure consistent order\n",
        "        if processed_files >= limit:\n",
        "            break  # Stop after processing the limit of files\n",
        "\n",
        "        if filename.endswith(\".pdb\"):\n",
        "            file_path = os.path.join(source_dir, filename)\n",
        "            structure = pdb_parser.get_structure(filename, file_path)\n",
        "\n",
        "            # Count chains to ensure there are 2 or more before proceeding\n",
        "            chain_count = sum(1 for _ in structure.get_chains())\n",
        "            if chain_count < 2:\n",
        "                continue  # Skip files with less than 2 chains\n",
        "\n",
        "            for model in structure:\n",
        "                for chain in model:\n",
        "                    chain_id = chain.id\n",
        "                    output_filename = f\"{os.path.splitext(filename)[0]}_{chain_id}.pdb\"\n",
        "                    output_path = os.path.join(target_dir, output_filename)\n",
        "\n",
        "                    # Set structure for output to just this chain\n",
        "                    pdb_io.set_structure(chain)\n",
        "                    pdb_io.save(output_path)\n",
        "\n",
        "            processed_files += 1  # Increment counter after processing each file\n"
      ],
      "metadata": {
        "id": "lXpYNHu4btha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we have to use this code to replace it with manually cleaning up files after we are done with them\n",
        "def clear_directory(directory):\n",
        "    \"\"\"Removes all files and directories in the specified directory.\"\"\"\n",
        "    for item in os.listdir(directory):\n",
        "        item_path = os.path.join(directory, item)\n",
        "        try:\n",
        "            if os.path.isfile(item_path) or os.path.islink(item_path):\n",
        "                os.unlink(item_path)  # Remove file or link\n",
        "            elif os.path.isdir(item_path):\n",
        "                shutil.rmtree(item_path)  # Remove directory and all its contents\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to delete {item_path}. Reason: {e}\")\n"
      ],
      "metadata": {
        "id": "uEFM7Rayc1V6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "source_dir = output_directory # Update this to your source directory path\n",
        "target_dir = '/home/f.ensafitakaldani001/Downloader_code/seperatedoutput'  # Update this to your target directory path\n",
        "# Clear everything in the target directory\n",
        "clear_directory(target_dir)\n",
        "\n",
        "# Assuming separate_chains is defined and handles the processing\n",
        "# For example, you call it with the source   directory, target directory, and a batch size\n",
        "separate_chains(source_dir, target_dir, total_files)\n",
        "\n",
        "\n",
        "print(f\"Chain separation complete for first {total_files} PDB files.\")\n",
        "\n",
        "\n",
        "#just to see: Specify the output directory\n",
        "output_directory = \"/home/f.ensafitakaldani001/Downloader_code/seperatedoutput\"\n",
        "import glob\n",
        "files = glob.glob(os.path.join(output_directory, '*'))\n",
        "total_files = len([f for f in files if os.path.isfile(f)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "JC-B0kXOc2T-",
        "outputId": "8560c5db-fcc4-4908-d05b-d7747c4eb072"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'output_directory' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-f09277b60205>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msource_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_directory\u001b[0m \u001b[0;31m# Update this to your source directory path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtarget_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/f.ensafitakaldani001/Downloader_code/seperatedoutput'\u001b[0m  \u001b[0;31m# Update this to your target directory path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Clear everything in the target directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclear_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'output_directory' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End Goal:\n",
        "At, this point we should have 10 different batch csv files with name of the protein_ID, coordinates and sequence. then we can go to create the dataset/dataframe in part 1"
      ],
      "metadata": {
        "id": "QV0ZUatVdewl"
      }
    }
  ]
}