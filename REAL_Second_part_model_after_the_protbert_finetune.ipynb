{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kattens/Protein-Interaction-with-LLMs/blob/main/REAL_Second_part_model_after_the_protbert_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "id": "_KZ0eV_ywnM0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3O1GOBJjjNNw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IS-wzrL6thZf",
        "outputId": "e5acc103-51ed-4699-fd8f-1fa0e67b7c71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CUDA not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0S1_IYhuzu6",
        "outputId": "8a531e58-fcee-4df0-ae88-4935f9afc431"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dD0ew8E8jXFJ"
      },
      "outputs": [],
      "source": [
        "csv_file = '/content/drive/MyDrive/pairs.csv'\n",
        "pairs_df = pd.read_csv(csv_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMXbw_ebK8lb",
        "outputId": "642b3251-4c92-499f-8d38-61bf9d3cccbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500\n"
          ]
        }
      ],
      "source": [
        "# Truncate each specified column to a maximum length of 500 characters\n",
        "columns = ['masked_sequence_A', 'masked_sequence_B', 'Sequence_A', 'Sequence_B']\n",
        "for col in columns:\n",
        "    pairs_df[col] = pairs_df[col].apply(lambda x: x[:500] if len(x) > 500 else x)\n",
        "\n",
        "# Find the longest string by length\n",
        "pairs_df['Length'] = pairs_df['masked_sequence_A'].apply(len)\n",
        "longest_string = pairs_df.loc[pairs_df['Length'].idxmax(), 'masked_sequence_A']\n",
        "print(len(longest_string))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pairs_df.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZMlodbXmDye",
        "outputId": "3d2ac3ea-e2f5-4861-e09b-aff2a2e8dd7e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58300"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r0l1YXCvOFH"
      },
      "source": [
        "Create dataset class that handles both global sequences and local sequences for protein pairs, and potentially prepares for the inclusion of 3D structural data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32Lx8aF-7PEj"
      },
      "source": [
        "#the base Model (without coordinates at this point):\n",
        "\n",
        "  ### Modeling Interactions:\n",
        "  The mdel could be trained to recognize which amino acids interact by learning representations of local sequences that highlight these interactions. During training, the MLM objective helps the model learn contextual embeddings that are rich in information about which amino acids tend to be near each other and under what structural contexts they interact.\n",
        "\n",
        "  ### Attention Mechanism:\n",
        "   The custom attention mechanism can be used to weigh the importance of different amino acids in the global context when predicting the masked amino acids in the local sequences. This allows your model to focus more on the parts of the global sequences that are relevant to the interactions highlighted by the local sequences.\n",
        "\n",
        "  ### Utilizing Global Sequences:\n",
        "  While the local sequences are your primary interest, the global sequences provide the context necessary for your model to understand the broader environment in which the interactions occur. Even during prediction, you should feed the model the global sequences to utilize the learned context.\n",
        "\n",
        "  #### This modular design not only meets your current requirements but also provides a scalable framework to incorporate additional dimensions of protein sequence data analysis in the future\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/Checkpoints"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lLTXwI4wUKU",
        "outputId": "5cec2c23-b2f2-40cd-879c-e30767c64bf8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "added_tokens.json     generation_config.json   tokenizer_config.json\n",
            "config.json\t      model.safetensors        tokenizer.json\n",
            "final_checkpoint.pth  special_tokens_map.json  vocab.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#in this block of code we will intialize our core model for the architecture\n",
        "import torch\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# Define the checkpoint directory and the specific checkpoint file\n",
        "checkpoint_path = '/content/drive/MyDrive/Checkpoints/final_checkpoint.pth'\n",
        "\n",
        "# Load the tokenizer first\n",
        "tokenizer = AutoTokenizer.from_pretrained('/content/drive/MyDrive/Checkpoints')\n",
        "\n",
        "# Initialize the model from the pre-trained configuration in the Checkpoints directory\n",
        "model = AutoModelForMaskedLM.from_pretrained('/content/drive/MyDrive/Checkpoints')\n",
        "model.resize_token_embeddings(len(tokenizer))  # Important if you've added tokens\n",
        "\n",
        "# Determine the device to use (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)  # Move model to the appropriate device\n",
        "\n",
        "# Load the checkpoint\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)  # Ensure checkpoint is loaded to the correct device\n",
        "\n",
        "# Ensure all keys in the checkpoint can be loaded to the model\n",
        "missing_keys, unexpected_keys = model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
        "if missing_keys or unexpected_keys:\n",
        "    print(f\"Missing keys in state dict: {missing_keys}\")\n",
        "    print(f\"Unexpected keys in state dict: {unexpected_keys}\")\n",
        "else:\n",
        "    print(\"Model state loaded successfully.\")\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "An Optimizer and a Learning rate scheduler are being set up and their states are loaded from a checkpoint.\n",
        "This allows for the continuation of model training with the exact parameters and learning rate adjustments that were in use when the\n",
        "training was last saved, ensuring a seamless transition and consistency in the training process.\n",
        "\"\"\"\n",
        "\n",
        "# If you need the optimizer and scheduler states\n",
        "optimizer = Adam(model.parameters(), lr=checkpoint.get('learning_rate', 0.001))  # Fallback to default if not in checkpoint\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
        "if 'scheduler_state_dict' in checkpoint:\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "else:\n",
        "    print(\"No scheduler state found in checkpoint; using default settings.\")\n",
        "\n",
        "print(\"Model, tokenizer, optimizer, and scheduler loaded from checkpoint.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rs_CVu1caUC",
        "outputId": "59f3f70f-228e-4af1-f93b-b8842660ca14"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model state loaded successfully.\n",
            "Model, tokenizer, optimizer, and scheduler loaded from checkpoint.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "ApUN1McupeKe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import logging\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "class ProteinInteractionDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, mask_probability=0.15, modes=None):\n",
        "        \"\"\"\n",
        "        Initializes the dataset.\n",
        "\n",
        "        Args:\n",
        "            dataframe (pandas.DataFrame): The dataframe containing protein sequences.\n",
        "            tokenizer (transformers.BertTokenizer): The tokenizer for encoding sequences.\n",
        "            mask_probability (float): The probability of masking a token for the masked language model.\n",
        "            modes (list of str): List of modes to prepare data. Options include:\n",
        "                                 'global_masked' - Returns sequences with random masking.\n",
        "                                 'local' - Returns non-masked sequences.\n",
        "                                 Modes can be combined.\n",
        "        \"\"\"\n",
        "        self.dataframe = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.mask_probability = mask_probability\n",
        "        self.modes = modes if modes else ['global']  # Default to only global if none specified\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      row = self.dataframe.iloc[idx]\n",
        "      data = {}\n",
        "\n",
        "      # Processing for 'global' mode\n",
        "      if 'global' in self.modes:\n",
        "          #local_seq = f\"[CLS] {row['masked_sequence_A']} [ENTITY1] [SEP] {row['masked_sequence_B']} [ENTITY2] [SEP]\"\n",
        "\n",
        "          global_seq = \"[CLS] {} [ENTITY1] [SEP] {} [ENTITY2] [SEP]\".format(\n",
        "              \" \".join(debug_df['Sequence_A']),\n",
        "              \" \".join(debug_df['Sequence_B'])\n",
        "          )\n",
        "\n",
        "\n",
        "          #logging.debug(f'Local sequence length before tokenization: {len(local_seq)}')\n",
        "          input_ids, attention_mask = self.tokenize_sequence(global_seq)\n",
        "          data['input_ids_local'] = input_ids\n",
        "          data['attention_mask_local'] = attention_mask\n",
        "\n",
        "          logging.debug(f'hello world')\n",
        "          logging.debug(f'Local mode: input_ids dimension {input_ids.shape}, attention mask dimension {attention_mask.shape}')\n",
        "\n",
        "      # Processing for 'local' mode\n",
        "      if 'local' in self.modes:\n",
        "          #local_seq = f\"[CLS] {row['masked_sequence_A']} [ENTITY1] [SEP] {row['masked_sequence_B']} [ENTITY2] [SEP]\"\n",
        "\n",
        "          local_seq = \"[CLS] {} [ENTITY1] [SEP] {} [ENTITY2] [SEP]\".format(\n",
        "              \" \".join(debug_df['masked_sequence_A']),\n",
        "              \" \".join(debug_df['masked_sequence_B'])\n",
        "          )\n",
        "\n",
        "          logging.debug(f'hello world')\n",
        "          #logging.debug(f'Local sequence: {local_seq} (length: {(local_seq)})')\n",
        "\n",
        "\n",
        "          #logging.debug(f'Local sequence length before tokenization: {len(local_seq)}')\n",
        "          input_ids, attention_mask = self.tokenize_sequence(local_seq)\n",
        "          data['input_ids_local'] = input_ids\n",
        "          data['attention_mask_local'] = attention_mask\n",
        "\n",
        "          logging.debug(f'hello world')\n",
        "          logging.debug(f'Local mode: input_ids dimension {input_ids.shape}, attention mask dimension {attention_mask.shape}')\n",
        "\n",
        "\n",
        "\n",
        "      # Example processing for 'coords' mode, assuming it's similar to 'local' mode but with different data\n",
        "      if 'coords' in self.modes:\n",
        "          coords_seq = f\"[CLS] {row['coords_A']} [ENTITY1] [SEP] {row['coords_B']} [ENTITY2] [SEP]\"\n",
        "          input_ids, attention_mask = self.tokenize_sequence(coords_seq)\n",
        "          data['input_ids_coords'] = input_ids\n",
        "          data['attention_mask_coords'] = attention_mask\n",
        "          logging.debug(f'Coords mode: input_ids dimension {input_ids.shape}, attention mask dimension {attention_mask.shape}')\n",
        "\n",
        "      return data\n",
        "\n",
        "\n",
        "    def tokenize_sequence(self, sequence):\n",
        "      max_length = 512  # Example fixed max length, adjust as necessary\n",
        "      encoded = self.tokenizer.encode_plus(\n",
        "          sequence,\n",
        "          add_special_tokens=False,\n",
        "          return_tensors='pt',\n",
        "          padding=False,\n",
        "          truncation=True,\n",
        "          max_length=max_length  # Ensure sequences are truncated to a maximum length\n",
        "      )\n",
        "      return encoded['input_ids'].squeeze(0), encoded['attention_mask'].squeeze(0)\n",
        "\n",
        "    def random_mask_sequence(self, sequence):\n",
        "        tokens = self.tokenizer.tokenize(sequence)\n",
        "        input_ids = torch.tensor(self.tokenizer.convert_tokens_to_ids(tokens), dtype=torch.long)\n",
        "        labels = torch.full(input_ids.shape, -100)  # Use -100 to ignore these indices in loss calculations\n",
        "        # Decide where to mask tokens\n",
        "        mask_indices = torch.rand(input_ids.shape) < self.mask_probability\n",
        "        labels[mask_indices] = input_ids[mask_indices]\n",
        "        # 80% of the time, replace masked input tokens with tokenizer.mask_token\n",
        "        actual_mask = mask_indices & (torch.rand(input_ids.shape) < 0.8)\n",
        "        input_ids[actual_mask] = self.tokenizer.convert_tokens_to_ids([self.tokenizer.mask_token])[0]\n",
        "        # 10% of the time, replace masked input tokens with a random token\n",
        "        random_tokens = torch.randint(2, self.tokenizer.vocab_size, input_ids.shape)\n",
        "        input_ids[mask_indices & ~actual_mask] = random_tokens[mask_indices & ~actual_mask]\n",
        "        return input_ids, torch.ones_like(input_ids), labels\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#this block is for debug deminstrations\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming pairs_df is your original DataFrame and tokenizer is already defined\n",
        "\n",
        "# This block is for debug demonstrations\n",
        "# Create a DataFrame from the first row\n",
        "debug_df = pairs_df.iloc[[0]]  # Use double brackets to keep it as a DataFrame\n",
        "\n",
        "\n",
        "# Print the length of each sequence column, considering each element as a string\n",
        "print(f\"Sequence_A length: {sum(len(seq) for seq in debug_df['Sequence_A'])}\")\n",
        "print(f\"Sequence_B length: {sum(len(seq) for seq in debug_df['Sequence_B'])}\")\n",
        "print(f\"Masked_sequence_A length: {sum(len(seq) for seq in debug_df['masked_sequence_A'])}\")\n",
        "print(f\"Masked_sequence_B length: {sum(len(seq) for seq in debug_df['masked_sequence_B'])}\")\n",
        "\n",
        "\n",
        "global_seq = \"[CLS] {} [ENTITY1] [SEP] {} [ENTITY2] [SEP]\".format(\n",
        "    \" \".join(debug_df['Sequence_A']),\n",
        "    \" \".join(debug_df['Sequence_B'])\n",
        ")\n",
        "\n",
        "local_seq = \"[CLS] {} [ENTITY1] [SEP] {} [ENTITY2] [SEP]\".format(\n",
        "    \" \".join(debug_df['masked_sequence_A']),\n",
        "    \" \".join(debug_df['masked_sequence_B'])\n",
        ")\n",
        "\n",
        "\n",
        "# Calculate the total length of the local sequence\n",
        "global_seq_length = len(global_seq)\n",
        "local_seq_length = len(local_seq)\n",
        "\n",
        "# Print the length\n",
        "'''\n",
        "print(f\"Global sequence length: {global_seq_length}\")\n",
        "print(f\"Local sequence length: {local_seq_length}\")\n",
        "\n",
        "'''\n",
        "# Initialize the dataset with specified modes\n",
        "dataset_global = ProteinInteractionDataset(debug_df, tokenizer, modes=['global'])\n",
        "dataset_local = ProteinInteractionDataset(debug_df, tokenizer, modes=['local'])\n",
        "dataset_coords = ProteinInteractionDataset(debug_df, tokenizer, modes=['coords'])\n",
        "\n",
        "# Fetch data for the first entry in the dataset\n",
        "data_global = dataset_global[0]  # Accessing the first item\n",
        "data_local = dataset_local[0]  # Accessing the first item\n",
        "data_coords = dataset_coords[0]  # Accessing the first item\n",
        "\n",
        "# Print the data for debugging\n",
        "print(\"Global Mode Data:\")\n",
        "for key, value in data_global.items():\n",
        "    print(f\"{key}: {value.shape}\")\n",
        "\n",
        "print(\"\\nLocal Mode Data:\")\n",
        "for key, value in data_local.items():\n",
        "    print(f\"{key}: {value.shape}\")\n",
        "\n",
        "print(\"\\ncoords Mode Data:\")\n",
        "for key, value in data_coords.items():\n",
        "    print(f\"{key}: {value.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F88JVYsZltdC",
        "outputId": "90e58536-5f62-4a0c-9b80-cec03964914c"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence_A length: 60\n",
            "Sequence_B length: 60\n",
            "Masked_sequence_A length: 60\n",
            "Masked_sequence_B length: 60\n",
            "Global Mode Data:\n",
            "input_ids_local: torch.Size([7])\n",
            "attention_mask_local: torch.Size([7])\n",
            "\n",
            "Local Mode Data:\n",
            "input_ids_local: torch.Size([125])\n",
            "attention_mask_local: torch.Size([125])\n",
            "\n",
            "coords Mode Data:\n",
            "input_ids_coords: torch.Size([512])\n",
            "attention_mask_coords: torch.Size([512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "uRffIAsuVZo6"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Initialize containers for the various data components\n",
        "    input_ids_global = []\n",
        "    attention_mask_global = []\n",
        "    labels_local = []\n",
        "    labels_coords = []\n",
        "\n",
        "    # Collect data for each sample in the batch\n",
        "    for item in batch:\n",
        "        input_ids_global.append(item['input_ids_global'])\n",
        "        attention_mask_global.append(item['attention_mask_global'])\n",
        "\n",
        "        if 'labels_local' in item:\n",
        "            labels_local.append(item['labels_local'])\n",
        "        if 'labels_coords' in item:\n",
        "            labels_coords.append(item['labels_coords'])\n",
        "\n",
        "    # Pad the sequences in the batch to the same length\n",
        "    input_ids_global = pad_sequence(input_ids_global, batch_first=True, padding_value=0)\n",
        "    attention_mask_global = pad_sequence(attention_mask_global, batch_first=True, padding_value=0)\n",
        "\n",
        "    # Prepare output dictionary\n",
        "    batch_data = {\n",
        "        'input_ids_global': input_ids_global,\n",
        "        'attention_mask_global': attention_mask_global,\n",
        "    }\n",
        "\n",
        "    # Only add labels to the batch if they are available\n",
        "    if labels_local:\n",
        "        labels_local = pad_sequence(labels_local, batch_first=True, padding_value=-100)\n",
        "        batch_data['labels_local'] = labels_local\n",
        "    if labels_coords:\n",
        "        labels_coords = pad_sequence(labels_coords, batch_first=True, padding_value=-100)\n",
        "        batch_data['labels_coords'] = labels_coords\n",
        "\n",
        "    return batch_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ7xnO7FseKP"
      },
      "source": [
        "#pretraining the bert model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSaaU5j6rHdO"
      },
      "source": [
        "#main class for training:\n",
        "  1. Process two sets of sequences (global and local) using BERT to extract contextual embeddings.\n",
        "  2. Integrate these two sets of embeddings using a custom attention mechanism that focuses on relevant parts of the global features for each part of the local features.\n",
        "  3. Predict an output (like interaction sites or effects) using the combined features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2qm1dRzz2pr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# Define a sequence processor that can handle one channel of input (either global, local, or coords)\n",
        "class SequenceProcessor(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(SequenceProcessor, self).__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        return self.model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
        "\n",
        "# Custom attention mechanism to integrate features from multiple channels\n",
        "class CustomAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(CustomAttention, self).__init__()\n",
        "        self.key_layer = nn.Linear(hidden_size, hidden_size)\n",
        "        self.query_layer = nn.Linear(hidden_size, hidden_size)\n",
        "        self.value_layer = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.context_layer = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, global_features, additional_features):\n",
        "        keys = self.key_layer(global_features)\n",
        "        queries = self.query_layer(additional_features)\n",
        "        values = self.value_layer(global_features)\n",
        "        attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / (keys.size(-1) ** 0.5)\n",
        "        attention_weights = self.softmax(attention_scores)\n",
        "        context = torch.matmul(attention_weights, values)\n",
        "        processed_context = self.context_layer(context)\n",
        "        return processed_context\n",
        "\n",
        "# Main model to process multiple input channels\n",
        "class ProteinInteractionModel(nn.Module):\n",
        "    def __init__(self, model_identifier_or_path):\n",
        "        super(ProteinInteractionModel, self).__init__()\n",
        "        self.base_model = AutoModel.from_pretrained(model_identifier_or_path)\n",
        "        hidden_size = self.base_model.config.hidden_size\n",
        "\n",
        "        self.sequence_processor_global = SequenceProcessor(self.base_model)\n",
        "        self.sequence_processor_local = SequenceProcessor(self.base_model)\n",
        "        self.custom_attention = CustomAttention(hidden_size)\n",
        "        self.mlm_head = nn.Linear(hidden_size, self.base_model.config.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids_global, attention_mask_global, input_ids_local=None, attention_mask_local=None):\n",
        "        global_features = self.sequence_processor_global(input_ids_global, attention_mask_global)\n",
        "\n",
        "        # Handle local input channel if provided\n",
        "        if input_ids_local is not None and attention_mask_local is not None:\n",
        "            local_features = self.sequence_processor_local(input_ids_local, attention_mask_local)\n",
        "            combined_features = self.custom_attention(global_features, local_features)\n",
        "        else:\n",
        "            combined_features = global_features  # Use global features directly if local is not provided\n",
        "\n",
        "        prediction_scores = self.mlm_head(combined_features)\n",
        "        return prediction_scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiPlAiY8j7aY"
      },
      "source": [
        "#Training and Validation data creator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3l4rd9Hej7Gj"
      },
      "outputs": [],
      "source": [
        "#the most important part to check if the class definition and data management is correctly working\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting the dataset into 80% training and 20% testing\n",
        "train_df, test_df = train_test_split(pairs_df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Assuming ProteinInteractionDataset is implemented to handle your DataFrame structure\n",
        "train_dataset = ProteinInteractionDataset(train_df, tokenizer)\n",
        "test_dataset = ProteinInteractionDataset(test_df, tokenizer)\n",
        "\n",
        "#since the model isnt runnint we reduced the batch size to half\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouA_WiDqRUqr"
      },
      "source": [
        "# Model Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5RnrPjjRhip"
      },
      "source": [
        "#Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l02vBOukRiqq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "266fb6d2-4fc1-4284-f5b9-2173b07205a3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Interaction_Site_A'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3653\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Interaction_Site_A'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-609a5917b15b>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-609a5917b15b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, optimizer, epochs)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             outputs = model(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-01d008eb7064>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask_global'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mlocal_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"[CLS] {row['Interaction_Site_A']} [ENTITY1] [SEP] {row['Interaction_Site_B']} [ENTITY2] [SEP]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0minput_ids_local\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask_local\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids_local'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids_local\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3655\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3656\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3657\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Interaction_Site_A'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoModel\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Training Loop\n",
        "def train(model, data_loader, optimizer, epochs=1):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for batch in data_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(\n",
        "                batch['input_ids_global'],\n",
        "                batch['attention_mask_global'],\n",
        "                batch['input_ids_local'],\n",
        "                batch['attention_mask_local']\n",
        "            )\n",
        "            loss = outputs.loss  # Assume model returns a loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "# Example Usage\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "dataframe = pairs_df\n",
        "dataset = ProteinInteractionDataset(dataframe, tokenizer, modes=['global', 'local'])\n",
        "data_loader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn, shuffle=True)\n",
        "model = ProteinInteractionModel('bert-base-uncased')\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "train(model, data_loader, optimizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fbn1dIZs4LVX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}