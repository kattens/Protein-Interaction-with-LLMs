{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kattens/Protein-Interaction-with-LLMs/blob/main/REAL_Second_part_model_after_the_protbert_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Goal of the project:\n",
        "the purpose of the model is to be able to efficiently change the number of channels which is a new way of introducing different variables and properties to the context of the protein-protein interaction, as well as being able to change the core model of the architecture to see what is the most suitable core for this approach\n",
        "\n",
        "###different core models to implement:\n",
        "1. ESM 2\n",
        "2. Llama 3\n",
        "3. ProtBert\n",
        "4. Bert\n",
        "\n",
        "###different versions of channels:\n",
        "1. local inputs with faiss\n",
        "2. local inputs from datasets based on the UniProt database\n",
        "3. coordinates of the amino acid's Calpha\n",
        "\n"
      ],
      "metadata": {
        "id": "Zm1sfYGEnRrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "id": "_KZ0eV_ywnM0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3O1GOBJjjNNw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IS-wzrL6thZf",
        "outputId": "68af47bb-3cb9-4bcd-b467-e4d933bb011f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "CUDA not available\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CUDA not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYTHU87r4LVG",
        "outputId": "e2847fe8-fce7-4468-f48c-970b2e58357f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dD0ew8E8jXFJ"
      },
      "outputs": [],
      "source": [
        "#this is the path for the file\n",
        "csv_file = '/content/drive/MyDrive/pairs.csv'\n",
        "pairs_df = pd.read_csv(csv_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read only the first 10 rows of the CSV file\n",
        "pairs_df = pd.read_csv(csv_file, nrows=10)\n"
      ],
      "metadata": {
        "id": "5ODHaGyfcLLb"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qcnp2I6FcKi5",
        "outputId": "9cde3e54-5de2-4c73-80e0-41ccbe94da36"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  pair_id Protein Name A Protein Name B  \\\n",
              "0    1H0J         1H0J_B         1H0J_C   \n",
              "1    1H0J         1H0J_B         1H0J_A   \n",
              "2    1H1A         1H1A_B         1H1A_A   \n",
              "3    1H1I         1H1I_A         1H1I_C   \n",
              "4    1H1I         1H1I_B         1H1I_C   \n",
              "5    1H1I         1H1I_B         1H1I_D   \n",
              "6    1H1L         1H1L_B         1H1L_C   \n",
              "7    1H1L         1H1L_B         1H1L_D   \n",
              "8    1H1L         1H1L_B         1H1L_A   \n",
              "9    1H1L         1H1L_C         1H1L_D   \n",
              "\n",
              "                                   masked_sequence_A  \\\n",
              "0  ---------------------------A------------------...   \n",
              "1  ---------------------------------V------------...   \n",
              "2  ?TL-----GTHN----------------------------------...   \n",
              "3  ----------------------------------------------...   \n",
              "4  ---------D--------------------------------G---...   \n",
              "5  ----------------------------------------------...   \n",
              "6  ----------------------------------------------...   \n",
              "7  ----------------------------------------------...   \n",
              "8  ----------------------------------------------...   \n",
              "9  TN------LA-------------------H-----------G----...   \n",
              "\n",
              "                                   masked_sequence_B  \\\n",
              "0  ----------------------------T-----------------...   \n",
              "1  ---------------------------------V------------...   \n",
              "2  ?TL-----GTHNG---------------------------------...   \n",
              "3  ----------------------------------------------...   \n",
              "4  ---------D--------------------------------G---...   \n",
              "5  ----------------------------------------------...   \n",
              "6  ----------------------------------------------...   \n",
              "7  ----------------------------------------------...   \n",
              "8  TN------L--------------------H-----------G----...   \n",
              "9  ----------------------------------------------...   \n",
              "\n",
              "                                            coords_A  \\\n",
              "0  [(15.18, 27.908, -3.171), (13.094, 28.367, -0....   \n",
              "1  [(15.18, 27.908, -3.171), (13.094, 28.367, -0....   \n",
              "2  [(1.704, 21.706, 18.319), (-0.901, 24.305, 19....   \n",
              "3  [(65.645, 40.726, 70.958), (67.22, 39.56, 67.6...   \n",
              "4  [(67.215, 3.653, 5.427), (68.427, 7.123, 4.339...   \n",
              "5  [(67.215, 3.653, 5.427), (68.427, 7.123, 4.339...   \n",
              "6  [(59.631, 22.412, 51.972), (61.593, 22.04, 48....   \n",
              "7  [(59.631, 22.412, 51.972), (61.593, 22.04, 48....   \n",
              "8  [(59.631, 22.412, 51.972), (61.593, 22.04, 48....   \n",
              "9  [(99.674, -54.557, 33.327), (95.992, -55.533, ...   \n",
              "\n",
              "                                            coords_B  \\\n",
              "0  [(31.71, 11.761, 52.412), (31.514, 9.193, 49.6...   \n",
              "1  [(47.873, 32.351, 21.457), (46.719, 29.304, 23...   \n",
              "2  [(-1.391, 21.605, 15.29), (1.068, 24.346, 14.3...   \n",
              "3  [(77.459, 39.1, 34.851), (76.18, 36.721, 37.4)...   \n",
              "4  [(77.459, 39.1, 34.851), (76.18, 36.721, 37.4)...   \n",
              "5  [(66.547, 10.408, -23.921), (67.719, 13.907, -...   \n",
              "6  [(99.674, -54.557, 33.327), (95.992, -55.533, ...   \n",
              "7  [(77.66, -28.403, 60.796), (75.28, -29.4, 57.9...   \n",
              "8  [(33.993, 36.681, 19.646), (37.624, 37.372, 18...   \n",
              "9  [(77.66, -28.403, 60.796), (75.28, -29.4, 57.9...   \n",
              "\n",
              "                                        Embeddings_A  \\\n",
              "0  tensor([ 0.1932, -0.0438,  0.7770,  0.2603, -0...   \n",
              "1  tensor([ 0.1932, -0.0438,  0.7770,  0.2603, -0...   \n",
              "2  tensor([ 0.7350, -0.2884, -0.7331, -0.7960, -0...   \n",
              "3  tensor([-0.7782, -2.3356, -0.4710,  0.6213, -2...   \n",
              "4  tensor([-0.4936,  1.1125,  1.4919,  0.1379, -1...   \n",
              "5  tensor([-0.4936,  1.1125,  1.4919,  0.1379, -1...   \n",
              "6  tensor([-0.5717, -1.4829, -0.5290, -0.2072, -0...   \n",
              "7  tensor([-0.5717, -1.4829, -0.5290, -0.2072, -0...   \n",
              "8  tensor([-0.5717, -1.4829, -0.5290, -0.2072, -0...   \n",
              "9  tensor([ 0.6414, -0.0384,  0.3825, -1.9274, -1...   \n",
              "\n",
              "                                          Sequence_A  \\\n",
              "0  LKCNKLVPLFYKTCPAGKNLCYKMFMVATPKVPVKRGCIDVCPKSS...   \n",
              "1  LKCNKLVPLFYKTCPAGKNLCYKMFMVATPKVPVKRGCIDVCPKSS...   \n",
              "2  ?TLTSSATGTHNGYYYSFWTDGQGNIRFNLESGGQYSVTWSGNGNW...   \n",
              "3  SSLIVEDAPDHVRPYVIRHYSHARAVTVDTQLYRFYVTGPSSGYAF...   \n",
              "4  SSLIVEDAPDHVRPYVIRHYSHARAVTVDTQLYRFYVTGPSSGYAF...   \n",
              "5  SSLIVEDAPDHVRPYVIRHYSHARAVTVDTQLYRFYVTGPSSGYAF...   \n",
              "6  SQTIDKINSCYPLFEQDEYQELFRNKRQLEEAHDAQRVQEVFAWTT...   \n",
              "7  SQTIDKINSCYPLFEQDEYQELFRNKRQLEEAHDAQRVQEVFAWTT...   \n",
              "8  SQTIDKINSCYPLFEQDEYQELFRNKRQLEEAHDAQRVQEVFAWTT...   \n",
              "9  TNATGERNLALIQEVLEVFPETARKERRKHMMVSDPKMKSVGKCII...   \n",
              "\n",
              "                                        Embeddings_B  \\\n",
              "0  tensor([-1.9606, -0.0789, -1.0516,  1.0233, -1...   \n",
              "1  tensor([ 0.2015,  0.3117, -0.5839,  1.0137, -0...   \n",
              "2  tensor([ 0.1946, -0.4597, -2.2509, -0.3440, -0...   \n",
              "3  tensor([-6.7764e-01,  3.7070e-02,  2.5174e-01,...   \n",
              "4  tensor([-6.7764e-01,  3.7070e-02,  2.5174e-01,...   \n",
              "5  tensor([-0.7012, -1.2548, -0.1944,  1.1893,  2...   \n",
              "6  tensor([ 0.6414, -0.0384,  0.3825, -1.9274, -1...   \n",
              "7  tensor([ 0.4332,  1.7368, -0.3487,  0.7947, -1...   \n",
              "8  tensor([ 1.3349,  0.3203,  0.4489,  1.4436,  1...   \n",
              "9  tensor([ 0.4332,  1.7368, -0.3487,  0.7947, -1...   \n",
              "\n",
              "                                          Sequence_B  \\\n",
              "0  LKCNKLVPLFYKTCPAGKNLCYKMFMVATPKVPVKRGCIDVCPKSS...   \n",
              "1  LKCNKLVPLFYKTCPAGKNLCYKMFMVATPKVPVKRGCIDVCPKSS...   \n",
              "2  ?TLTSSATGTHNGYYYSFWTDGQGNIRFNLESGGQYSVTWSGNGNW...   \n",
              "3  SSLIVEDAPDHVRPYVIRHYSHARAVTVDTQLYRFYVTGPSSGYAF...   \n",
              "4  SSLIVEDAPDHVRPYVIRHYSHARAVTVDTQLYRFYVTGPSSGYAF...   \n",
              "5  SLIVEDAPDHVRPYVIRHYSHARAVTVDTQLYRFYVTGPSSGYAFT...   \n",
              "6  TNATGERNLALIQEVLEVFPETARKERRKHMMVSDPKMKSVGKCII...   \n",
              "7  SQTIDKINSCYPLFEQDEYQELFRNKRQLEEAHDAQRVQEVFAWTT...   \n",
              "8  TNATGERNLALIQEVLEVFPETARKERRKHMMVSDPKMKSVGKCII...   \n",
              "9  SQTIDKINSCYPLFEQDEYQELFRNKRQLEEAHDAQRVQEVFAWTT...   \n",
              "\n",
              "                                tokenized_sequence_A  \\\n",
              "0  [5, 12, 23, 17, 12, 5, 8, 16, 5, 19, 20, 12, 1...   \n",
              "1  [5, 12, 23, 17, 12, 5, 8, 16, 5, 19, 20, 12, 1...   \n",
              "2  [1, 15, 5, 15, 10, 10, 6, 15, 7, 15, 22, 17, 7...   \n",
              "3  [10, 10, 5, 11, 8, 9, 14, 6, 16, 14, 22, 8, 13...   \n",
              "4  [10, 10, 5, 11, 8, 9, 14, 6, 16, 14, 22, 8, 13...   \n",
              "5  [10, 10, 5, 11, 8, 9, 14, 6, 16, 14, 22, 8, 13...   \n",
              "6  [10, 18, 15, 11, 14, 12, 11, 17, 10, 23, 20, 1...   \n",
              "7  [10, 18, 15, 11, 14, 12, 11, 17, 10, 23, 20, 1...   \n",
              "8  [10, 18, 15, 11, 14, 12, 11, 17, 10, 23, 20, 1...   \n",
              "9  [15, 17, 6, 15, 7, 9, 13, 17, 5, 6, 5, 11, 18,...   \n",
              "\n",
              "                                tokenized_sequence_B  \\\n",
              "0  [5, 12, 23, 17, 12, 5, 8, 16, 5, 19, 20, 12, 1...   \n",
              "1  [5, 12, 23, 17, 12, 5, 8, 16, 5, 19, 20, 12, 1...   \n",
              "2  [1, 15, 5, 15, 10, 10, 6, 15, 7, 15, 22, 17, 7...   \n",
              "3  [10, 10, 5, 11, 8, 9, 14, 6, 16, 14, 22, 8, 13...   \n",
              "4  [10, 10, 5, 11, 8, 9, 14, 6, 16, 14, 22, 8, 13...   \n",
              "5  [10, 5, 11, 8, 9, 14, 6, 16, 14, 22, 8, 13, 16...   \n",
              "6  [15, 17, 6, 15, 7, 9, 13, 17, 5, 6, 5, 11, 18,...   \n",
              "7  [10, 18, 15, 11, 14, 12, 11, 17, 10, 23, 20, 1...   \n",
              "8  [15, 17, 6, 15, 7, 9, 13, 17, 5, 6, 5, 11, 18,...   \n",
              "9  [10, 18, 15, 11, 14, 12, 11, 17, 10, 23, 20, 1...   \n",
              "\n",
              "                         tokenized_masked_sequence_A  \\\n",
              "0  [32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...   \n",
              "1  [32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...   \n",
              "2  [1, 15, 5, 32, 32, 32, 32, 32, 7, 15, 22, 17, ...   \n",
              "3  [32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...   \n",
              "4  [32, 32, 32, 32, 32, 32, 32, 32, 32, 14, 32, 3...   \n",
              "5  [32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...   \n",
              "6  [32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...   \n",
              "7  [32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...   \n",
              "8  [32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...   \n",
              "9  [15, 17, 32, 32, 32, 32, 32, 32, 5, 6, 32, 32,...   \n",
              "\n",
              "                         tokenized_masked_sequence_B  \\\n",
              "0  [32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...   \n",
              "1  [32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...   \n",
              "2  [1, 15, 5, 32, 32, 32, 32, 32, 7, 15, 22, 17, ...   \n",
              "3  [32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...   \n",
              "4  [32, 32, 32, 32, 32, 32, 32, 32, 32, 14, 32, 3...   \n",
              "5  [32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...   \n",
              "6  [32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...   \n",
              "7  [32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...   \n",
              "8  [15, 17, 32, 32, 32, 32, 32, 32, 5, 32, 32, 32...   \n",
              "9  [32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...   \n",
              "\n",
              "                            sum_tokenized_sequence_A  \\\n",
              "0  [37, 44, 55, 49, 44, 37, 40, 48, 37, 51, 52, 4...   \n",
              "1  [37, 44, 55, 49, 44, 37, 40, 48, 37, 51, 52, 4...   \n",
              "2  [2, 30, 10, 47, 42, 42, 38, 47, 14, 30, 44, 34...   \n",
              "3  [42, 42, 37, 43, 40, 41, 46, 38, 48, 46, 54, 4...   \n",
              "4  [42, 42, 37, 43, 40, 41, 46, 38, 48, 28, 54, 4...   \n",
              "5  [42, 42, 37, 43, 40, 41, 46, 38, 48, 46, 54, 4...   \n",
              "6  [42, 50, 47, 43, 46, 44, 43, 49, 42, 55, 52, 4...   \n",
              "7  [42, 50, 47, 43, 46, 44, 43, 49, 42, 55, 52, 4...   \n",
              "8  [42, 50, 47, 43, 46, 44, 43, 49, 42, 55, 52, 4...   \n",
              "9  [30, 34, 38, 47, 39, 41, 45, 49, 10, 12, 37, 4...   \n",
              "\n",
              "                            sum_tokenized_sequence_B  \n",
              "0  [37, 44, 55, 49, 44, 37, 40, 48, 37, 51, 52, 4...  \n",
              "1  [37, 44, 55, 49, 44, 37, 40, 48, 37, 51, 52, 4...  \n",
              "2  [2, 30, 10, 47, 42, 42, 38, 47, 14, 30, 44, 34...  \n",
              "3  [42, 42, 37, 43, 40, 41, 46, 38, 48, 46, 54, 4...  \n",
              "4  [42, 42, 37, 43, 40, 41, 46, 38, 48, 28, 54, 4...  \n",
              "5  [42, 37, 43, 40, 41, 46, 38, 48, 46, 54, 40, 4...  \n",
              "6  [47, 49, 38, 47, 39, 41, 45, 49, 37, 38, 37, 4...  \n",
              "7  [42, 50, 47, 43, 46, 44, 43, 49, 42, 55, 52, 4...  \n",
              "8  [30, 34, 38, 47, 39, 41, 45, 49, 10, 38, 37, 4...  \n",
              "9  [42, 50, 47, 43, 46, 44, 43, 49, 42, 55, 52, 4...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-25a5a7e4-4065-4aeb-a2e4-0d65b0d70a1d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pair_id</th>\n",
              "      <th>Protein Name A</th>\n",
              "      <th>Protein Name B</th>\n",
              "      <th>masked_sequence_A</th>\n",
              "      <th>masked_sequence_B</th>\n",
              "      <th>coords_A</th>\n",
              "      <th>coords_B</th>\n",
              "      <th>Embeddings_A</th>\n",
              "      <th>Sequence_A</th>\n",
              "      <th>Embeddings_B</th>\n",
              "      <th>Sequence_B</th>\n",
              "      <th>tokenized_sequence_A</th>\n",
              "      <th>tokenized_sequence_B</th>\n",
              "      <th>tokenized_masked_sequence_A</th>\n",
              "      <th>tokenized_masked_sequence_B</th>\n",
              "      <th>sum_tokenized_sequence_A</th>\n",
              "      <th>sum_tokenized_sequence_B</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1H0J</td>\n",
              "      <td>1H0J_B</td>\n",
              "      <td>1H0J_C</td>\n",
              "      <td>---------------------------A------------------...</td>\n",
              "      <td>----------------------------T-----------------...</td>\n",
              "      <td>[(15.18, 27.908, -3.171), (13.094, 28.367, -0....</td>\n",
              "      <td>[(31.71, 11.761, 52.412), (31.514, 9.193, 49.6...</td>\n",
              "      <td>tensor([ 0.1932, -0.0438,  0.7770,  0.2603, -0...</td>\n",
              "      <td>LKCNKLVPLFYKTCPAGKNLCYKMFMVATPKVPVKRGCIDVCPKSS...</td>\n",
              "      <td>tensor([-1.9606, -0.0789, -1.0516,  1.0233, -1...</td>\n",
              "      <td>LKCNKLVPLFYKTCPAGKNLCYKMFMVATPKVPVKRGCIDVCPKSS...</td>\n",
              "      <td>[5, 12, 23, 17, 12, 5, 8, 16, 5, 19, 20, 12, 1...</td>\n",
              "      <td>[5, 12, 23, 17, 12, 5, 8, 16, 5, 19, 20, 12, 1...</td>\n",
              "      <td>[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...</td>\n",
              "      <td>[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...</td>\n",
              "      <td>[37, 44, 55, 49, 44, 37, 40, 48, 37, 51, 52, 4...</td>\n",
              "      <td>[37, 44, 55, 49, 44, 37, 40, 48, 37, 51, 52, 4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1H0J</td>\n",
              "      <td>1H0J_B</td>\n",
              "      <td>1H0J_A</td>\n",
              "      <td>---------------------------------V------------...</td>\n",
              "      <td>---------------------------------V------------...</td>\n",
              "      <td>[(15.18, 27.908, -3.171), (13.094, 28.367, -0....</td>\n",
              "      <td>[(47.873, 32.351, 21.457), (46.719, 29.304, 23...</td>\n",
              "      <td>tensor([ 0.1932, -0.0438,  0.7770,  0.2603, -0...</td>\n",
              "      <td>LKCNKLVPLFYKTCPAGKNLCYKMFMVATPKVPVKRGCIDVCPKSS...</td>\n",
              "      <td>tensor([ 0.2015,  0.3117, -0.5839,  1.0137, -0...</td>\n",
              "      <td>LKCNKLVPLFYKTCPAGKNLCYKMFMVATPKVPVKRGCIDVCPKSS...</td>\n",
              "      <td>[5, 12, 23, 17, 12, 5, 8, 16, 5, 19, 20, 12, 1...</td>\n",
              "      <td>[5, 12, 23, 17, 12, 5, 8, 16, 5, 19, 20, 12, 1...</td>\n",
              "      <td>[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...</td>\n",
              "      <td>[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...</td>\n",
              "      <td>[37, 44, 55, 49, 44, 37, 40, 48, 37, 51, 52, 4...</td>\n",
              "      <td>[37, 44, 55, 49, 44, 37, 40, 48, 37, 51, 52, 4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1H1A</td>\n",
              "      <td>1H1A_B</td>\n",
              "      <td>1H1A_A</td>\n",
              "      <td>?TL-----GTHN----------------------------------...</td>\n",
              "      <td>?TL-----GTHNG---------------------------------...</td>\n",
              "      <td>[(1.704, 21.706, 18.319), (-0.901, 24.305, 19....</td>\n",
              "      <td>[(-1.391, 21.605, 15.29), (1.068, 24.346, 14.3...</td>\n",
              "      <td>tensor([ 0.7350, -0.2884, -0.7331, -0.7960, -0...</td>\n",
              "      <td>?TLTSSATGTHNGYYYSFWTDGQGNIRFNLESGGQYSVTWSGNGNW...</td>\n",
              "      <td>tensor([ 0.1946, -0.4597, -2.2509, -0.3440, -0...</td>\n",
              "      <td>?TLTSSATGTHNGYYYSFWTDGQGNIRFNLESGGQYSVTWSGNGNW...</td>\n",
              "      <td>[1, 15, 5, 15, 10, 10, 6, 15, 7, 15, 22, 17, 7...</td>\n",
              "      <td>[1, 15, 5, 15, 10, 10, 6, 15, 7, 15, 22, 17, 7...</td>\n",
              "      <td>[1, 15, 5, 32, 32, 32, 32, 32, 7, 15, 22, 17, ...</td>\n",
              "      <td>[1, 15, 5, 32, 32, 32, 32, 32, 7, 15, 22, 17, ...</td>\n",
              "      <td>[2, 30, 10, 47, 42, 42, 38, 47, 14, 30, 44, 34...</td>\n",
              "      <td>[2, 30, 10, 47, 42, 42, 38, 47, 14, 30, 44, 34...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1H1I</td>\n",
              "      <td>1H1I_A</td>\n",
              "      <td>1H1I_C</td>\n",
              "      <td>----------------------------------------------...</td>\n",
              "      <td>----------------------------------------------...</td>\n",
              "      <td>[(65.645, 40.726, 70.958), (67.22, 39.56, 67.6...</td>\n",
              "      <td>[(77.459, 39.1, 34.851), (76.18, 36.721, 37.4)...</td>\n",
              "      <td>tensor([-0.7782, -2.3356, -0.4710,  0.6213, -2...</td>\n",
              "      <td>SSLIVEDAPDHVRPYVIRHYSHARAVTVDTQLYRFYVTGPSSGYAF...</td>\n",
              "      <td>tensor([-6.7764e-01,  3.7070e-02,  2.5174e-01,...</td>\n",
              "      <td>SSLIVEDAPDHVRPYVIRHYSHARAVTVDTQLYRFYVTGPSSGYAF...</td>\n",
              "      <td>[10, 10, 5, 11, 8, 9, 14, 6, 16, 14, 22, 8, 13...</td>\n",
              "      <td>[10, 10, 5, 11, 8, 9, 14, 6, 16, 14, 22, 8, 13...</td>\n",
              "      <td>[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...</td>\n",
              "      <td>[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...</td>\n",
              "      <td>[42, 42, 37, 43, 40, 41, 46, 38, 48, 46, 54, 4...</td>\n",
              "      <td>[42, 42, 37, 43, 40, 41, 46, 38, 48, 46, 54, 4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1H1I</td>\n",
              "      <td>1H1I_B</td>\n",
              "      <td>1H1I_C</td>\n",
              "      <td>---------D--------------------------------G---...</td>\n",
              "      <td>---------D--------------------------------G---...</td>\n",
              "      <td>[(67.215, 3.653, 5.427), (68.427, 7.123, 4.339...</td>\n",
              "      <td>[(77.459, 39.1, 34.851), (76.18, 36.721, 37.4)...</td>\n",
              "      <td>tensor([-0.4936,  1.1125,  1.4919,  0.1379, -1...</td>\n",
              "      <td>SSLIVEDAPDHVRPYVIRHYSHARAVTVDTQLYRFYVTGPSSGYAF...</td>\n",
              "      <td>tensor([-6.7764e-01,  3.7070e-02,  2.5174e-01,...</td>\n",
              "      <td>SSLIVEDAPDHVRPYVIRHYSHARAVTVDTQLYRFYVTGPSSGYAF...</td>\n",
              "      <td>[10, 10, 5, 11, 8, 9, 14, 6, 16, 14, 22, 8, 13...</td>\n",
              "      <td>[10, 10, 5, 11, 8, 9, 14, 6, 16, 14, 22, 8, 13...</td>\n",
              "      <td>[32, 32, 32, 32, 32, 32, 32, 32, 32, 14, 32, 3...</td>\n",
              "      <td>[32, 32, 32, 32, 32, 32, 32, 32, 32, 14, 32, 3...</td>\n",
              "      <td>[42, 42, 37, 43, 40, 41, 46, 38, 48, 28, 54, 4...</td>\n",
              "      <td>[42, 42, 37, 43, 40, 41, 46, 38, 48, 28, 54, 4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1H1I</td>\n",
              "      <td>1H1I_B</td>\n",
              "      <td>1H1I_D</td>\n",
              "      <td>----------------------------------------------...</td>\n",
              "      <td>----------------------------------------------...</td>\n",
              "      <td>[(67.215, 3.653, 5.427), (68.427, 7.123, 4.339...</td>\n",
              "      <td>[(66.547, 10.408, -23.921), (67.719, 13.907, -...</td>\n",
              "      <td>tensor([-0.4936,  1.1125,  1.4919,  0.1379, -1...</td>\n",
              "      <td>SSLIVEDAPDHVRPYVIRHYSHARAVTVDTQLYRFYVTGPSSGYAF...</td>\n",
              "      <td>tensor([-0.7012, -1.2548, -0.1944,  1.1893,  2...</td>\n",
              "      <td>SLIVEDAPDHVRPYVIRHYSHARAVTVDTQLYRFYVTGPSSGYAFT...</td>\n",
              "      <td>[10, 10, 5, 11, 8, 9, 14, 6, 16, 14, 22, 8, 13...</td>\n",
              "      <td>[10, 5, 11, 8, 9, 14, 6, 16, 14, 22, 8, 13, 16...</td>\n",
              "      <td>[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...</td>\n",
              "      <td>[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...</td>\n",
              "      <td>[42, 42, 37, 43, 40, 41, 46, 38, 48, 46, 54, 4...</td>\n",
              "      <td>[42, 37, 43, 40, 41, 46, 38, 48, 46, 54, 40, 4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1H1L</td>\n",
              "      <td>1H1L_B</td>\n",
              "      <td>1H1L_C</td>\n",
              "      <td>----------------------------------------------...</td>\n",
              "      <td>----------------------------------------------...</td>\n",
              "      <td>[(59.631, 22.412, 51.972), (61.593, 22.04, 48....</td>\n",
              "      <td>[(99.674, -54.557, 33.327), (95.992, -55.533, ...</td>\n",
              "      <td>tensor([-0.5717, -1.4829, -0.5290, -0.2072, -0...</td>\n",
              "      <td>SQTIDKINSCYPLFEQDEYQELFRNKRQLEEAHDAQRVQEVFAWTT...</td>\n",
              "      <td>tensor([ 0.6414, -0.0384,  0.3825, -1.9274, -1...</td>\n",
              "      <td>TNATGERNLALIQEVLEVFPETARKERRKHMMVSDPKMKSVGKCII...</td>\n",
              "      <td>[10, 18, 15, 11, 14, 12, 11, 17, 10, 23, 20, 1...</td>\n",
              "      <td>[15, 17, 6, 15, 7, 9, 13, 17, 5, 6, 5, 11, 18,...</td>\n",
              "      <td>[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...</td>\n",
              "      <td>[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...</td>\n",
              "      <td>[42, 50, 47, 43, 46, 44, 43, 49, 42, 55, 52, 4...</td>\n",
              "      <td>[47, 49, 38, 47, 39, 41, 45, 49, 37, 38, 37, 4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1H1L</td>\n",
              "      <td>1H1L_B</td>\n",
              "      <td>1H1L_D</td>\n",
              "      <td>----------------------------------------------...</td>\n",
              "      <td>----------------------------------------------...</td>\n",
              "      <td>[(59.631, 22.412, 51.972), (61.593, 22.04, 48....</td>\n",
              "      <td>[(77.66, -28.403, 60.796), (75.28, -29.4, 57.9...</td>\n",
              "      <td>tensor([-0.5717, -1.4829, -0.5290, -0.2072, -0...</td>\n",
              "      <td>SQTIDKINSCYPLFEQDEYQELFRNKRQLEEAHDAQRVQEVFAWTT...</td>\n",
              "      <td>tensor([ 0.4332,  1.7368, -0.3487,  0.7947, -1...</td>\n",
              "      <td>SQTIDKINSCYPLFEQDEYQELFRNKRQLEEAHDAQRVQEVFAWTT...</td>\n",
              "      <td>[10, 18, 15, 11, 14, 12, 11, 17, 10, 23, 20, 1...</td>\n",
              "      <td>[10, 18, 15, 11, 14, 12, 11, 17, 10, 23, 20, 1...</td>\n",
              "      <td>[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...</td>\n",
              "      <td>[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...</td>\n",
              "      <td>[42, 50, 47, 43, 46, 44, 43, 49, 42, 55, 52, 4...</td>\n",
              "      <td>[42, 50, 47, 43, 46, 44, 43, 49, 42, 55, 52, 4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1H1L</td>\n",
              "      <td>1H1L_B</td>\n",
              "      <td>1H1L_A</td>\n",
              "      <td>----------------------------------------------...</td>\n",
              "      <td>TN------L--------------------H-----------G----...</td>\n",
              "      <td>[(59.631, 22.412, 51.972), (61.593, 22.04, 48....</td>\n",
              "      <td>[(33.993, 36.681, 19.646), (37.624, 37.372, 18...</td>\n",
              "      <td>tensor([-0.5717, -1.4829, -0.5290, -0.2072, -0...</td>\n",
              "      <td>SQTIDKINSCYPLFEQDEYQELFRNKRQLEEAHDAQRVQEVFAWTT...</td>\n",
              "      <td>tensor([ 1.3349,  0.3203,  0.4489,  1.4436,  1...</td>\n",
              "      <td>TNATGERNLALIQEVLEVFPETARKERRKHMMVSDPKMKSVGKCII...</td>\n",
              "      <td>[10, 18, 15, 11, 14, 12, 11, 17, 10, 23, 20, 1...</td>\n",
              "      <td>[15, 17, 6, 15, 7, 9, 13, 17, 5, 6, 5, 11, 18,...</td>\n",
              "      <td>[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...</td>\n",
              "      <td>[15, 17, 32, 32, 32, 32, 32, 32, 5, 32, 32, 32...</td>\n",
              "      <td>[42, 50, 47, 43, 46, 44, 43, 49, 42, 55, 52, 4...</td>\n",
              "      <td>[30, 34, 38, 47, 39, 41, 45, 49, 10, 38, 37, 4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1H1L</td>\n",
              "      <td>1H1L_C</td>\n",
              "      <td>1H1L_D</td>\n",
              "      <td>TN------LA-------------------H-----------G----...</td>\n",
              "      <td>----------------------------------------------...</td>\n",
              "      <td>[(99.674, -54.557, 33.327), (95.992, -55.533, ...</td>\n",
              "      <td>[(77.66, -28.403, 60.796), (75.28, -29.4, 57.9...</td>\n",
              "      <td>tensor([ 0.6414, -0.0384,  0.3825, -1.9274, -1...</td>\n",
              "      <td>TNATGERNLALIQEVLEVFPETARKERRKHMMVSDPKMKSVGKCII...</td>\n",
              "      <td>tensor([ 0.4332,  1.7368, -0.3487,  0.7947, -1...</td>\n",
              "      <td>SQTIDKINSCYPLFEQDEYQELFRNKRQLEEAHDAQRVQEVFAWTT...</td>\n",
              "      <td>[15, 17, 6, 15, 7, 9, 13, 17, 5, 6, 5, 11, 18,...</td>\n",
              "      <td>[10, 18, 15, 11, 14, 12, 11, 17, 10, 23, 20, 1...</td>\n",
              "      <td>[15, 17, 32, 32, 32, 32, 32, 32, 5, 6, 32, 32,...</td>\n",
              "      <td>[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...</td>\n",
              "      <td>[30, 34, 38, 47, 39, 41, 45, 49, 10, 12, 37, 4...</td>\n",
              "      <td>[42, 50, 47, 43, 46, 44, 43, 49, 42, 55, 52, 4...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-25a5a7e4-4065-4aeb-a2e4-0d65b0d70a1d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-25a5a7e4-4065-4aeb-a2e4-0d65b0d70a1d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-25a5a7e4-4065-4aeb-a2e4-0d65b0d70a1d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-29cbf944-9c79-496b-8613-2db1d71b153d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-29cbf944-9c79-496b-8613-2db1d71b153d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-29cbf944-9c79-496b-8613-2db1d71b153d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_61a2daf7-632b-41f6-89c1-026f182259d4\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('pairs_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_61a2daf7-632b-41f6-89c1-026f182259d4 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('pairs_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "pairs_df",
              "summary": "{\n  \"name\": \"pairs_df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"pair_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"1H1A\",\n          \"1H1L\",\n          \"1H0J\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Protein Name A\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"1H0J_B\",\n          \"1H1A_B\",\n          \"1H1L_C\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Protein Name B\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"1H0J_A\",\n          \"1H1L_C\",\n          \"1H0J_C\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"masked_sequence_A\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"----------------------------------------------------LNFR---LT----------------------L-Y---S--------------F-----CVSDS-TED---------M---L-------------------AE------------------------------PS-----------------------------------------------------------------------DT--------------------------------------------------------------------------------------------------------------------------------------------------------------------W--RS--FTR-----------KF-----L--------------------HH---------------------------------------------\",\n          \"---------------------------------V-------------L------------\",\n          \"-------------------------------------------------------------------------------------------------------------------D-------------------------TDTTH---I---------------------------------D----------------------------I------------------------------------------------------------V---------------E------------------S--------------------------E-SS-SF----\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"masked_sequence_B\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"TN------L--------------------H-----------G----------------RGCAY---KG---------------------Y-R--RR--Y-------SF---NFTS-FQ-------------K--EE---------------E----------------------------------GV------------------------------------------------------------------------ENTPF---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\",\n          \"---------------------------------V-------------L------------\",\n          \"--------------------------------------------------------------------------------------------------------------------D-------------------------TTHTP---S-----------------------------------A----------------------------P------------------------------------------------------------V---------------V------------------K--------------------------F-AD-\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"coords_A\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"[(15.18, 27.908, -3.171), (13.094, 28.367, -0.025), (14.194, 27.287, 3.457), (12.376, 27.246, 6.78), (13.433, 29.565, 9.596), (13.855, 28.657, 13.272), (10.063, 28.507, 13.604), (9.22, 26.247, 10.59), (5.886, 28.037, 10.185), (7.91, 30.686, 8.37), (10.378, 30.565, 5.496), (12.587, 32.72, 3.312), (13.688, 32.767, -0.313), (17.42, 32.129, -0.37), (19.611, 34.968, -1.697), (22.141, 34.483, -4.495), (24.969, 32.155, -3.525), (22.935, 30.08, -1.064), (21.355, 26.982, -2.619), (21.126, 24.751, 0.439), (18.858, 24.501, 3.494), (20.056, 23.276, 6.853), (18.983, 22.483, 10.395), (21.401, 22.584, 13.332), (20.945, 20.18, 16.248), (22.591, 20.201, 19.69), (23.191, 16.561, 20.751), (22.52, 17.342, 24.43), (19.176, 19.036, 23.636), (18.214, 17.435, 20.249), (14.432, 17.709, 20.578), (14.362, 20.494, 17.989), (16.898, 22.328, 15.824), (18.34, 25.483, 17.385), (19.029, 27.114, 14.031), (17.637, 26.781, 10.494), (17.993, 28.647, 7.228), (19.683, 29.036, 3.859), (23.419, 28.55, 3.401), (26.071, 27.909, 0.717), (27.966, 24.951, 2.152), (27.015, 22.249, 4.62), (28.735, 22.95, 7.965), (30.877, 20.111, 9.276), (29.133, 18.142, 12.029), (30.991, 17.409, 15.266), (30.651, 15.387, 18.464), (27.983, 17.624, 20.035), (26.63, 19.551, 17.03), (24.85, 17.861, 14.11), (23.835, 19.4, 10.796), (21.174, 18.167, 8.375), (21.516, 19.645, 4.882), (19.112, 19.429, 1.93), (18.293, 21.434, -1.206), (14.524, 21.148, -1.678), (11.945, 23.848, -0.918), (11.402, 23.89, 2.848), (13.097, 20.472, 3.271), (14.564, 21.644, 6.587)]\",\n          \"[(1.704, 21.706, 18.319), (-0.901, 24.305, 19.238), (-4.356, 23.862, 17.695), (-5.829, 26.741, 15.706), (-9.377, 25.395, 15.258), (-11.513, 22.395, 16.237), (-9.466, 19.337, 15.348), (-8.927, 15.605, 15.73), (-6.22, 13.04, 15.07), (-3.657, 11.084, 17.052), (-0.623, 12.486, 18.849), (2.117, 10.41, 20.435), (-0.158, 7.394, 20.182), (-3.184, 9.03, 21.775), (-6.283, 10.129, 19.919), (-7.188, 13.766, 20.465), (-10.378, 15.7, 19.934), (-10.796, 19.428, 20.419), (-14.13, 21.132, 19.825), (-14.616, 24.874, 20.193), (-17.382, 27.245, 19.13), (-14.574, 29.584, 18.098), (-15.398, 32.162, 20.759), (-13.45, 33.529, 23.694), (-9.719, 33.022, 24.137), (-8.141, 29.596, 24.0), (-4.877, 27.823, 23.339), (-5.106, 24.025, 23.312), (-1.635, 22.432, 23.189), (-0.65, 18.847, 22.382), (2.299, 17.782, 24.514), (4.445, 14.726, 25.281), (2.813, 11.305, 25.34), (-0.793, 11.397, 26.493), (-0.518, 14.942, 27.798), (-2.151, 18.181, 26.718), (-2.488, 21.626, 28.246), (-5.015, 24.387, 27.741), (-5.412, 27.992, 28.863), (-8.704, 29.773, 28.278), (-10.812, 32.811, 29.125), (-13.554, 34.969, 27.675), (-16.941, 33.864, 26.39), (-16.18, 30.542, 24.766), (-17.647, 27.046, 24.761), (-15.188, 24.201, 24.202), (-14.193, 20.65, 25.137), (-11.104, 18.625, 24.313), (-8.597, 16.057, 25.445), (-6.593, 12.933, 24.708), (-7.52, 9.292, 24.788), (-8.574, 6.641, 22.321), (-10.635, 6.08, 19.2), (-12.366, 3.759, 19.31), (-12.999, 3.581, 23.025), (-13.806, 0.259, 24.706), (-16.41, -0.632, 27.315), (-13.736, -2.271, 29.468), (-10.814, 0.147, 29.291), (-9.11, 1.068, 32.567), (-8.13, 4.725, 32.306), (-5.452, 6.248, 34.525), (-4.85, 9.984, 34.471), (-3.409, 12.899, 36.408), (-4.281, 16.558, 36.094), (-3.315, 20.006, 37.303), (-6.839, 21.374, 36.925), (-7.356, 25.074, 37.679), (-10.874, 26.099, 36.523), (-12.638, 29.39, 37.195), (-16.371, 29.092, 36.649), (-18.375, 26.6, 34.596), (-15.873, 23.891, 33.681), (-15.429, 20.167, 34.169), (-12.842, 17.402, 33.961), (-14.262, 14.109, 32.722), (-13.897, 10.985, 30.642), (-15.94, 11.909, 27.588), (-16.956, 9.863, 24.584), (-19.52, 8.641, 22.085), (-20.944, 5.365, 20.793), (-22.892, 4.716, 17.59), (-25.916, 2.641, 16.567), (-27.497, 4.085, 18.538), (-25.84, 7.493, 18.884), (-24.898, 8.099, 22.518), (-22.895, 10.794, 24.31), (-21.442, 9.918, 27.722), (-19.444, 11.437, 30.585), (-17.684, 10.279, 33.744), (-17.371, 13.569, 35.613), (-14.391, 13.571, 37.969), (-14.445, 17.26, 38.847), (-16.329, 20.411, 37.9), (-16.629, 24.1, 38.7), (-19.623, 26.286, 39.506), (-22.028, 24.074, 37.567), (-22.865, 20.385, 37.974), (-22.963, 19.328, 34.289), (-25.664, 16.756, 35.086), (-28.106, 19.523, 36.039), (-31.482, 18.987, 34.398), (-30.734, 15.33, 33.684), (-32.849, 12.529, 35.145), (-31.14, 10.688, 38.016), (-30.993, 6.941, 37.223), (-29.245, 5.424, 40.248), (-25.569, 5.235, 41.213), (-22.372, 3.215, 40.925), (-19.437, 2.697, 43.279), (-16.063, 2.308, 41.624), (-12.438, 3.413, 41.871), (-12.693, 4.492, 45.496), (-15.585, 6.823, 44.772), (-19.338, 6.933, 44.242), (-21.076, 8.347, 41.171), (-24.696, 9.274, 40.62), (-26.017, 8.196, 37.225), (-28.131, 10.478, 35.038), (-29.699, 10.304, 31.596), (-31.292, 12.878, 29.329), (-32.414, 13.1, 25.714), (-30.74, 15.772, 23.597), (-33.141, 16.894, 20.848), (-31.743, 18.156, 17.537), (-28.343, 18.917, 19.03), (-24.796, 19.008, 17.596), (-23.079, 15.616, 17.544), (-20.415, 13.563, 15.797), (-23.157, 12.65, 13.299), (-24.449, 16.175, 12.76), (-27.621, 17.679, 14.222), (-29.365, 14.746, 15.875), (-31.553, 13.551, 18.737), (-29.766, 11.205, 21.108), (-29.422, 10.195, 24.734), (-26.701, 11.516, 26.997), (-25.468, 9.503, 29.985), (-23.682, 10.868, 33.044), (-21.726, 9.466, 35.978), (-20.877, 12.244, 38.431), (-18.475, 11.622, 41.326), (-19.871, 12.729, 44.693), (-16.375, 13.832, 45.705), (-14.312, 15.817, 43.192), (-10.832, 14.612, 42.264), (-8.251, 15.439, 39.605), (-6.389, 12.178, 39.155), (-6.727, 8.433, 39.482), (-8.182, 5.422, 37.703), (-11.508, 5.135, 35.894), (-12.75, 1.591, 35.288), (-15.117, 2.141, 32.365), (-16.559, -1.375, 32.635), (-18.333, -0.393, 35.855), (-19.814, 2.735, 34.291), (-21.038, 1.088, 31.096), (-22.567, -1.749, 33.152), (-24.53, 0.818, 35.132), (-25.505, 2.802, 32.036), (-26.588, -0.29, 30.103), (-29.11, -1.079, 32.835), (-30.211, 2.555, 32.616), (-30.987, 2.152, 28.919), (-27.663, 2.525, 27.121), (-27.496, 0.244, 24.078), (-23.977, -0.151, 22.705), (-23.507, -0.528, 18.963), (-19.949, 0.615, 18.329), (-17.536, 2.932, 20.118), (-16.419, 6.262, 18.687), (-13.865, 8.333, 20.615), (-13.185, 8.067, 24.345), (-11.052, 10.769, 25.944), (-10.16, 12.376, 29.259), (-11.409, 15.861, 28.503), (-11.465, 19.316, 30.004), (-14.399, 21.599, 29.203), (-15.279, 25.249, 29.66), (-18.391, 27.387, 29.194), (-18.028, 31.17, 29.438), (-15.265, 30.594, 31.978), (-11.485, 30.817, 32.293), (-8.604, 28.788, 33.657), (-5.818, 26.393, 32.808), (-5.209, 22.676, 32.935), (-2.512, 20.158, 32.141), (-3.78, 16.602, 31.799), (-1.7, 13.474, 31.3), (-3.528, 10.37, 30.126), (-1.856, 7.052, 30.876)]\",\n          \"[(99.674, -54.557, 33.327), (95.992, -55.533, 33.142), (93.671, -55.157, 30.173), (92.856, -51.586, 31.259), (96.525, -50.693, 31.406), (96.992, -52.196, 27.918), (94.14, -50.252, 26.363), (95.625, -47.133, 27.967), (99.165, -47.788, 26.644), (97.579, -48.309, 23.153), (95.53, -45.12, 23.522), (98.747, -43.247, 24.395), (100.4, -44.382, 21.186), (97.371, -43.389, 19.136), (97.008, -39.952, 20.695), (100.656, -39.124, 20.189), (100.76, -40.105, 16.554), (99.774, -36.53, 15.477), (102.883, -34.883, 17.004), (106.032, -34.013, 15.078), (108.577, -36.737, 15.963), (110.821, -34.701, 18.331), (107.804, -33.658, 20.408), (106.208, -37.124, 20.175), (109.164, -38.725, 21.776), (109.404, -36.046, 24.488), (105.615, -36.431, 25.248), (105.762, -40.206, 25.485), (107.777, -39.796, 28.702), (105.019, -37.665, 30.276), (102.456, -40.478, 30.479), (102.297, -43.953, 32.022), (99.848, -46.591, 33.335), (99.824, -47.169, 37.094), (100.864, -50.296, 38.967), (99.569, -50.777, 42.531), (102.778, -52.601, 43.427), (104.932, -49.597, 42.52), (106.149, -47.3, 45.298), (108.2, -44.973, 43.151), (107.643, -43.846, 39.554), (111.371, -44.024, 38.865), (112.084, -43.75, 35.15), (108.47, -44.366, 34.154), (107.279, -40.717, 33.956), (109.117, -37.384, 33.709), (107.731, -33.997, 34.882), (108.529, -30.401, 35.766), (110.05, -29.94, 32.313), (109.513, -27.003, 29.915), (106.3, -27.137, 27.91), (106.374, -27.971, 24.204), (105.886, -24.728, 22.249), (102.556, -24.283, 20.498), (100.55, -27.096, 22.036), (98.006, -25.206, 24.16), (99.213, -26.227, 27.586), (97.071, -25.331, 30.606), (100.127, -24.096, 32.533), (100.575, -20.594, 33.903), (103.766, -18.731, 34.985), (105.939, -18.067, 38.037), (104.078, -14.817, 38.671), (100.81, -16.742, 38.977), (102.461, -19.156, 41.36), (103.877, -16.363, 43.547), (101.351, -13.488, 43.413), (98.342, -15.513, 42.456), (98.776, -18.504, 44.714), (101.104, -17.8, 47.656), (101.139, -14.056, 48.449), (97.845, -12.923, 46.92), (95.767, -14.479, 49.7), (97.517, -12.666, 52.604), (94.847, -10.163, 53.484), (96.659, -7.612, 55.659), (99.407, -6.904, 53.107), (99.075, -4.921, 49.866), (100.578, -6.809, 46.875), (102.024, -4.704, 44.024), (102.033, -6.169, 40.554), (105.209, -4.492, 39.321), (105.207, -4.216, 35.545), (102.664, -3.551, 32.761), (98.908, -3.561, 33.555), (97.881, -6.995, 32.234), (99.001, -9.611, 34.751), (97.387, -7.994, 37.794), (94.14, -7.347, 35.919), (93.655, -10.88, 34.692), (94.454, -12.26, 38.17), (92.292, -9.734, 40.04), (89.824, -11.746, 42.193), (89.783, -14.821, 39.89), (90.852, -17.193, 42.655), (88.413, -17.763, 45.487), (90.897, -17.505, 48.333), (88.322, -16.387, 50.895), (89.025, -13.965, 53.677), (87.135, -11.713, 56.056), (88.021, -8.042, 56.402), (87.088, -4.54, 55.245), (88.277, -4.395, 51.672), (90.555, -1.401, 50.886), (90.827, -0.449, 54.563), (92.285, -3.406, 56.568), (92.297, -6.126, 53.872), (93.521, -5.648, 50.266), (93.151, -8.947, 48.415), (90.596, -7.932, 45.795), (92.264, -4.615, 45.041), (94.716, -4.715, 42.134), (97.726, -2.504, 42.944), (100.065, -1.971, 39.962), (102.872, 0.261, 38.64), (101.27, 0.136, 35.168), (104.558, 0.265, 33.242), (104.523, 2.554, 30.208), (107.287, 2.665, 27.532), (109.182, 5.311, 29.629), (109.53, 2.719, 32.458), (110.956, 0.106, 30.112), (113.36, 2.717, 28.718), (114.595, 4.23, 32.019), (113.461, 1.574, 34.496), (110.883, 1.508, 37.203), (112.702, 1.634, 40.53), (111.958, 5.354, 41.006), (108.364, 4.8, 39.995), (108.164, 1.933, 42.506), (109.616, 3.946, 45.362), (107.077, 6.767, 44.739), (104.298, 4.208, 44.368), (105.103, 2.803, 47.849), (104.936, 6.357, 49.286), (101.498, 6.923, 47.726), (100.322, 3.512, 48.949), (101.294, 4.315, 52.551), (99.384, 7.569, 52.392), (96.184, 6.087, 50.846), (96.229, 2.67, 52.605), (97.561, 3.394, 56.126), (96.392, 0.135, 57.694), (98.447, -2.253, 55.6), (100.715, -4.412, 57.737), (103.21, -5.042, 55.011), (103.752, -4.659, 51.297), (104.958, -7.132, 48.632), (106.332, -6.404, 45.153), (105.512, -9.173, 42.631), (107.766, -8.819, 39.571), (106.21, -9.339, 36.124), (108.227, -10.247, 33.002), (109.818, -6.888, 32.133), (111.252, -6.072, 35.619), (114.046, -8.615, 35.54), (114.791, -8.141, 31.838), (115.386, -4.38, 32.144), (117.833, -4.618, 35.049), (115.671, -3.191, 37.834), (116.804, -3.656, 41.458), (113.64, -3.932, 43.579), (115.578, -5.222, 46.638), (117.242, -1.833, 47.086), (113.843, -0.191, 46.738), (112.39, -2.676, 49.22), (115.073, -2.105, 51.852), (114.864, 1.634, 51.332), (111.055, 1.863, 51.55), (110.736, -0.46, 54.562), (113.02, 1.777, 56.64), (111.047, 4.827, 55.576), (107.673, 3.124, 56.199), (108.666, 1.025, 59.199), (106.817, -2.073, 57.868), (108.118, -5.105, 55.931), (108.413, -4.34, 52.196), (109.119, -7.65, 50.44), (110.678, -8.069, 46.98), (109.716, -11.081, 44.852), (111.598, -11.9, 41.674), (108.755, -14.037, 40.271), (108.98, -12.891, 36.65), (106.799, -15.135, 34.529), (109.504, -16.282, 32.16), (111.373, -17.915, 35.093), (110.727, -21.624, 35.561), (108.397, -23.726, 33.426), (105.067, -23.208, 35.203), (103.434, -21.946, 38.393), (105.667, -24.276, 40.431), (108.797, -22.145, 40.392), (106.739, -19.175, 41.649), (105.826, -21.287, 44.684), (109.468, -21.989, 45.405), (110.614, -18.409, 44.876), (107.775, -17.33, 47.182), (108.778, -19.813, 49.911), (112.428, -18.57, 49.823), (111.253, -14.966, 50.068), (109.205, -15.91, 53.133), (112.227, -17.689, 54.588), (114.402, -14.548, 54.137), (112.113, -11.525, 54.543), (109.306, -12.543, 56.867), (109.759, -15.636, 58.951), (112.574, -14.385, 61.216), (110.911, -11.045, 62.039), (109.816, -12.166, 65.514), (112.781, -14.38, 66.492), (113.467, -13.997, 70.195), (110.179, -12.134, 70.783), (107.982, -13.35, 73.634), (105.068, -15.455, 72.451), (102.569, -17.213, 74.746), (102.575, -20.818, 73.554), (100.364, -23.809, 74.25), (101.348, -27.475, 73.862), (98.78, -27.961, 71.083), (99.91, -25.111, 68.837), (99.832, -25.756, 65.116), (100.081, -24.001, 61.744), (97.996, -24.956, 58.711), (100.388, -24.857, 55.761), (99.406, -24.739, 52.082), (95.645, -24.299, 52.441), (94.403, -21.389, 50.375), (90.83, -21.251, 51.752), (89.184, -21.806, 48.35), (85.483, -21.237, 49.0), (86.296, -21.254, 52.782), (88.057, -24.639, 52.759), (90.623, -23.593, 55.406), (87.855, -22.411, 57.782), (86.19, -25.817, 57.279), (89.563, -27.448, 58.213), (90.3, -25.128, 61.142), (86.949, -25.717, 62.887), (87.754, -29.444, 63.19), (91.337, -28.957, 64.464), (89.991, -26.625, 67.217), (87.163, -29.113, 67.894), (89.873, -31.733, 68.582), (91.58, -29.423, 71.141), (94.267, -27.85, 68.966), (95.09, -24.152, 68.645), (95.6, -23.038, 65.035), (98.015, -20.167, 65.573), (98.385, -19.63, 61.833), (96.973, -20.48, 58.388), (99.211, -20.243, 55.274), (97.936, -18.401, 53.351), (94.114, -18.601, 52.878), (92.687, -15.809, 55.106), (96.254, -15.477, 56.385), (98.071, -12.501, 57.787), (101.646, -11.276, 57.969), (101.385, -11.597, 61.777), (100.416, -15.286, 61.672), (103.366, -16.108, 59.379), (105.709, -14.266, 61.742), (104.342, -16.287, 64.697), (104.601, -19.582, 62.864), (108.087, -20.454, 64.156), (106.555, -20.702, 67.678), (104.232, -23.673, 67.028), (104.661, -27.354, 67.836), (103.738, -28.807, 64.455), (103.162, -27.859, 60.811), (100.219, -29.423, 59.009), (100.924, -29.304, 55.305), (97.937, -29.68, 52.974), (99.45, -28.511, 49.691), (102.78, -30.375, 49.611), (103.646, -28.993, 46.195), (103.601, -25.297, 47.04), (104.668, -25.207, 50.691), (106.913, -28.25, 51.367), (109.945, -25.956, 50.956), (109.176, -23.792, 53.999), (107.998, -26.748, 56.133), (111.323, -28.483, 55.489), (113.037, -25.188, 56.257), (111.219, -24.84, 59.618), (112.154, -28.42, 60.542), (115.847, -27.739, 59.732), (116.042, -24.333, 61.446), (113.416, -24.211, 64.232), (113.055, -27.961, 64.774), (109.287, -27.894, 64.1), (108.063, -31.22, 62.749), (105.625, -31.398, 59.875), (103.281, -33.806, 58.211), (100.99, -33.987, 55.189), (97.25, -34.528, 55.743), (94.146, -34.623, 53.586), (90.737, -33.291, 54.619), (88.884, -34.421, 51.47), (86.054, -36.908, 52.286), (85.232, -38.919, 55.428), (87.891, -41.615, 54.931), (90.822, -39.208, 54.572), (89.421, -36.92, 57.261), (89.357, -39.704, 59.854), (92.916, -40.859, 58.884), (94.293, -37.32, 59.121), (92.563, -36.48, 62.411), (93.887, -39.711, 64.011), (97.441, -39.255, 62.672), (97.588, -35.579, 63.722), (96.237, -36.405, 67.187), (98.882, -39.169, 67.511), (101.496, -36.367, 67.451), (100.108, -35.162, 70.786), (98.82, -37.361, 73.694), (95.875, -39.518, 74.754), (93.756, -36.475, 75.466), (94.022, -35.255, 71.87), (93.94, -38.852, 70.679), (90.653, -39.39, 72.498), (89.379, -36.037, 71.206), (90.007, -37.137, 67.58), (87.84, -40.204, 68.177), (85.082, -38.058, 69.656), (85.196, -35.603, 66.717), (85.008, -38.504, 64.253), (82.162, -40.212, 66.148), (80.093, -37.048, 66.436), (80.085, -36.446, 62.647), (79.438, -40.078, 61.796), (75.671, -39.869, 61.492), (76.122, -36.85, 59.181), (78.631, -38.729, 57.01), (76.414, -41.851, 56.89), (73.305, -39.984, 55.681), (75.39, -38.151, 53.004), (76.762, -41.51, 51.726), (73.332, -43.143, 51.818), (71.757, -40.326, 49.833), (74.378, -39.611, 47.213), (76.302, -42.85, 46.816), (73.402, -44.589, 45.083), (73.127, -41.69, 42.582), (76.833, -41.691, 41.807), (78.291, -45.161, 42.338), (80.12, -46.847, 39.502), (80.52, -43.686, 37.42), (84.003, -43.018, 36.039), (86.02, -39.799, 36.563), (88.875, -38.07, 34.697), (90.848, -35.501, 36.753), (93.328, -32.916, 35.545), (94.428, -30.047, 37.809), (97.614, -28.358, 39.063), (99.865, -29.979, 41.602), (97.873, -31.4, 44.512), (94.2, -31.986, 43.855), (94.288, -34.738, 41.225), (95.471, -37.23, 43.927), (94.037, -35.477, 46.981), (90.342, -35.751, 45.97), (90.439, -39.455, 45.062), (89.267, -40.725, 48.463), (86.21, -38.422, 48.46), (85.203, -39.747, 45.02), (85.701, -43.331, 46.315), (83.606, -42.525, 49.401), (80.752, -41.916, 46.87), (81.524, -45.133, 44.972), (83.038, -43.361, 41.917), (86.124, -44.56, 40.099), (89.106, -42.607, 38.735), (90.114, -43.76, 35.23), (92.622, -40.989, 34.432), (94.516, -38.488, 36.538), (96.915, -35.789, 35.459), (98.726, -32.736, 36.715), (99.73, -29.551, 34.933), (102.898, -28.97, 36.916), (103.484, -31.559, 39.625), (106.922, -33.022, 40.231), (107.979, -36.652, 40.326), (107.418, -36.758, 44.127), (103.734, -35.856, 43.504), (103.442, -38.703, 41.033), (105.129, -40.837, 43.678), (102.438, -39.648, 46.152), (99.699, -40.568, 43.619), (100.538, -44.254, 43.12), (98.867, -45.724, 46.219), (95.463, -44.187, 45.44), (95.125, -45.468, 41.82), (93.587, -48.69, 40.466), (95.72, -50.931, 38.307), (96.061, -49.716, 34.725), (95.024, -46.055, 35.348), (96.349, -43.456, 32.91), (98.672, -41.048, 34.794), (99.991, -38.076, 32.864), (101.757, -34.727, 32.996), (101.339, -31.295, 31.364), (98.806, -32.333, 28.747), (98.017, -30.225, 25.725), (94.37, -29.453, 24.957), (94.696, -31.811, 21.957), (95.765, -34.633, 24.306), (93.117, -33.882, 26.939), (90.286, -33.892, 24.373), (91.619, -37.089, 22.752), (91.835, -38.93, 26.126), (88.279, -37.833, 26.955), (86.796, -38.914, 23.597), (88.427, -42.332, 24.003), (87.942, -43.02, 27.736), (84.441, -41.547, 27.767), (84.493, -40.633, 31.443), (81.178, -39.991, 33.106), (82.505, -36.844, 34.783), (85.525, -34.605, 34.4), (87.136, -32.495, 37.103), (89.415, -29.779, 35.77), (89.77, -26.022, 35.343), (87.96, -23.121, 33.757), (89.995, -23.261, 30.536), (88.787, -26.849, 29.957), (85.195, -26.556, 31.231), (83.365, -25.158, 28.182), (85.089, -27.596, 25.804), (83.931, -30.729, 27.617), (80.447, -29.384, 28.236), (80.022, -29.095, 24.432), (81.253, -32.678, 24.074), (78.409, -33.658, 26.417), (80.583, -34.678, 29.396), (79.417, -33.502, 32.799), (82.154, -31.246, 34.143), (83.029, -29.839, 37.574), (85.454, -27.009, 38.088), (87.863, -28.513, 40.613), (90.053, -25.467, 41.132), (87.292, -22.946, 41.991), (84.766, -25.594, 43.0), (82.386, -24.086, 40.323), (81.636, -21.135, 42.683), (80.575, -23.608, 45.42), (82.442, -25.305, 48.314), (83.962, -25.537, 50.656), (84.842, -29.168, 49.949), (87.394, -29.763, 52.758), (86.246, -31.21, 56.102), (82.944, -32.642, 57.246), (80.933, -29.591, 56.189), (82.614, -29.696, 52.768), (81.835, -33.38, 52.015), (78.099, -32.723, 52.035), (78.624, -29.825, 49.558), (80.917, -31.963, 47.379), (78.422, -34.859, 47.258), (75.504, -32.532, 46.457), (77.529, -30.809, 43.747), (78.778, -33.973, 42.049), (75.238, -35.335, 41.898), (73.794, -32.037, 40.671), (76.113, -32.012, 37.663), (76.381, -35.709, 36.788), (72.613, -36.28, 36.937), (71.335, -32.969, 35.574), (68.466, -33.414, 33.074), (70.22, -31.213, 30.483), (72.839, -33.871, 29.666), (70.069, -35.92, 27.987), (69.305, -33.084, 25.541), (72.475, -32.67, 23.59), (70.989, -34.564, 20.686), (67.874, -33.129, 19.031), (65.057, -35.673, 19.358), (64.556, -35.917, 15.62), (68.137, -37.219, 15.166)]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"coords_B\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"[(47.873, 32.351, 21.457), (46.719, 29.304, 23.415), (43.076, 28.156, 23.346), (41.247, 25.443, 25.275), (39.753, 22.396, 23.56), (36.237, 20.99, 24.122), (37.526, 19.745, 27.471), (39.108, 22.915, 29.02), (41.704, 20.71, 30.731), (43.36, 20.398, 27.315), (44.633, 23.165, 25.033), (46.778, 24.016, 22.034), (48.697, 26.881, 20.466), (46.816, 28.405, 17.546), (48.24, 28.125, 14.017), (49.022, 31.068, 11.755), (45.816, 32.816, 10.734), (43.682, 32.035, 13.79), (44.184, 34.697, 16.467), (40.995, 34.123, 18.454), (39.7, 31.511, 20.875), (36.01, 30.648, 20.789), (33.423, 28.68, 22.721), (30.115, 27.677, 21.142), (26.989, 26.958, 23.207), (23.186, 27.097, 23.203), (21.778, 30.109, 25.071), (19.668, 27.921, 27.398), (22.589, 25.674, 28.356), (25.629, 27.97, 28.791), (27.351, 25.496, 31.137), (27.17, 22.77, 28.492), (29.31, 24.205, 25.635), (29.499, 22.243, 22.394), (32.668, 23.48, 20.724), (35.949, 25.131, 21.71), (39.043, 26.0, 19.695), (40.985, 28.524, 17.615), (39.375, 30.462, 14.782), (40.076, 33.463, 12.535), (36.685, 35.197, 12.682), (34.085, 35.203, 15.472), (31.092, 33.212, 14.187), (27.851, 35.197, 14.068), (25.646, 34.616, 17.128), (21.919, 33.946, 16.73), (18.863, 33.796, 19.003), (19.788, 30.389, 20.411), (23.367, 29.769, 19.233), (25.997, 31.913, 20.923), (29.725, 32.225, 20.431), (32.203, 33.547, 22.967), (35.355, 34.95, 21.348), (38.485, 36.279, 23.098), (42.256, 36.435, 22.557), (43.838, 35.859, 25.969), (45.587, 32.534, 26.596), (43.008, 29.871, 27.433), (40.26, 32.484, 27.8), (37.644, 30.265, 26.099)]\",\n          \"[(99.674, -54.557, 33.327), (95.992, -55.533, 33.142), (93.671, -55.157, 30.173), (92.856, -51.586, 31.259), (96.525, -50.693, 31.406), (96.992, -52.196, 27.918), (94.14, -50.252, 26.363), (95.625, -47.133, 27.967), (99.165, -47.788, 26.644), (97.579, -48.309, 23.153), (95.53, -45.12, 23.522), (98.747, -43.247, 24.395), (100.4, -44.382, 21.186), (97.371, -43.389, 19.136), (97.008, -39.952, 20.695), (100.656, -39.124, 20.189), (100.76, -40.105, 16.554), (99.774, -36.53, 15.477), (102.883, -34.883, 17.004), (106.032, -34.013, 15.078), (108.577, -36.737, 15.963), (110.821, -34.701, 18.331), (107.804, -33.658, 20.408), (106.208, -37.124, 20.175), (109.164, -38.725, 21.776), (109.404, -36.046, 24.488), (105.615, -36.431, 25.248), (105.762, -40.206, 25.485), (107.777, -39.796, 28.702), (105.019, -37.665, 30.276), (102.456, -40.478, 30.479), (102.297, -43.953, 32.022), (99.848, -46.591, 33.335), (99.824, -47.169, 37.094), (100.864, -50.296, 38.967), (99.569, -50.777, 42.531), (102.778, -52.601, 43.427), (104.932, -49.597, 42.52), (106.149, -47.3, 45.298), (108.2, -44.973, 43.151), (107.643, -43.846, 39.554), (111.371, -44.024, 38.865), (112.084, -43.75, 35.15), (108.47, -44.366, 34.154), (107.279, -40.717, 33.956), (109.117, -37.384, 33.709), (107.731, -33.997, 34.882), (108.529, -30.401, 35.766), (110.05, -29.94, 32.313), (109.513, -27.003, 29.915), (106.3, -27.137, 27.91), (106.374, -27.971, 24.204), (105.886, -24.728, 22.249), (102.556, -24.283, 20.498), (100.55, -27.096, 22.036), (98.006, -25.206, 24.16), (99.213, -26.227, 27.586), (97.071, -25.331, 30.606), (100.127, -24.096, 32.533), (100.575, -20.594, 33.903), (103.766, -18.731, 34.985), (105.939, -18.067, 38.037), (104.078, -14.817, 38.671), (100.81, -16.742, 38.977), (102.461, -19.156, 41.36), (103.877, -16.363, 43.547), (101.351, -13.488, 43.413), (98.342, -15.513, 42.456), (98.776, -18.504, 44.714), (101.104, -17.8, 47.656), (101.139, -14.056, 48.449), (97.845, -12.923, 46.92), (95.767, -14.479, 49.7), (97.517, -12.666, 52.604), (94.847, -10.163, 53.484), (96.659, -7.612, 55.659), (99.407, -6.904, 53.107), (99.075, -4.921, 49.866), (100.578, -6.809, 46.875), (102.024, -4.704, 44.024), (102.033, -6.169, 40.554), (105.209, -4.492, 39.321), (105.207, -4.216, 35.545), (102.664, -3.551, 32.761), (98.908, -3.561, 33.555), (97.881, -6.995, 32.234), (99.001, -9.611, 34.751), (97.387, -7.994, 37.794), (94.14, -7.347, 35.919), (93.655, -10.88, 34.692), (94.454, -12.26, 38.17), (92.292, -9.734, 40.04), (89.824, -11.746, 42.193), (89.783, -14.821, 39.89), (90.852, -17.193, 42.655), (88.413, -17.763, 45.487), (90.897, -17.505, 48.333), (88.322, -16.387, 50.895), (89.025, -13.965, 53.677), (87.135, -11.713, 56.056), (88.021, -8.042, 56.402), (87.088, -4.54, 55.245), (88.277, -4.395, 51.672), (90.555, -1.401, 50.886), (90.827, -0.449, 54.563), (92.285, -3.406, 56.568), (92.297, -6.126, 53.872), (93.521, -5.648, 50.266), (93.151, -8.947, 48.415), (90.596, -7.932, 45.795), (92.264, -4.615, 45.041), (94.716, -4.715, 42.134), (97.726, -2.504, 42.944), (100.065, -1.971, 39.962), (102.872, 0.261, 38.64), (101.27, 0.136, 35.168), (104.558, 0.265, 33.242), (104.523, 2.554, 30.208), (107.287, 2.665, 27.532), (109.182, 5.311, 29.629), (109.53, 2.719, 32.458), (110.956, 0.106, 30.112), (113.36, 2.717, 28.718), (114.595, 4.23, 32.019), (113.461, 1.574, 34.496), (110.883, 1.508, 37.203), (112.702, 1.634, 40.53), (111.958, 5.354, 41.006), (108.364, 4.8, 39.995), (108.164, 1.933, 42.506), (109.616, 3.946, 45.362), (107.077, 6.767, 44.739), (104.298, 4.208, 44.368), (105.103, 2.803, 47.849), (104.936, 6.357, 49.286), (101.498, 6.923, 47.726), (100.322, 3.512, 48.949), (101.294, 4.315, 52.551), (99.384, 7.569, 52.392), (96.184, 6.087, 50.846), (96.229, 2.67, 52.605), (97.561, 3.394, 56.126), (96.392, 0.135, 57.694), (98.447, -2.253, 55.6), (100.715, -4.412, 57.737), (103.21, -5.042, 55.011), (103.752, -4.659, 51.297), (104.958, -7.132, 48.632), (106.332, -6.404, 45.153), (105.512, -9.173, 42.631), (107.766, -8.819, 39.571), (106.21, -9.339, 36.124), (108.227, -10.247, 33.002), (109.818, -6.888, 32.133), (111.252, -6.072, 35.619), (114.046, -8.615, 35.54), (114.791, -8.141, 31.838), (115.386, -4.38, 32.144), (117.833, -4.618, 35.049), (115.671, -3.191, 37.834), (116.804, -3.656, 41.458), (113.64, -3.932, 43.579), (115.578, -5.222, 46.638), (117.242, -1.833, 47.086), (113.843, -0.191, 46.738), (112.39, -2.676, 49.22), (115.073, -2.105, 51.852), (114.864, 1.634, 51.332), (111.055, 1.863, 51.55), (110.736, -0.46, 54.562), (113.02, 1.777, 56.64), (111.047, 4.827, 55.576), (107.673, 3.124, 56.199), (108.666, 1.025, 59.199), (106.817, -2.073, 57.868), (108.118, -5.105, 55.931), (108.413, -4.34, 52.196), (109.119, -7.65, 50.44), (110.678, -8.069, 46.98), (109.716, -11.081, 44.852), (111.598, -11.9, 41.674), (108.755, -14.037, 40.271), (108.98, -12.891, 36.65), (106.799, -15.135, 34.529), (109.504, -16.282, 32.16), (111.373, -17.915, 35.093), (110.727, -21.624, 35.561), (108.397, -23.726, 33.426), (105.067, -23.208, 35.203), (103.434, -21.946, 38.393), (105.667, -24.276, 40.431), (108.797, -22.145, 40.392), (106.739, -19.175, 41.649), (105.826, -21.287, 44.684), (109.468, -21.989, 45.405), (110.614, -18.409, 44.876), (107.775, -17.33, 47.182), (108.778, -19.813, 49.911), (112.428, -18.57, 49.823), (111.253, -14.966, 50.068), (109.205, -15.91, 53.133), (112.227, -17.689, 54.588), (114.402, -14.548, 54.137), (112.113, -11.525, 54.543), (109.306, -12.543, 56.867), (109.759, -15.636, 58.951), (112.574, -14.385, 61.216), (110.911, -11.045, 62.039), (109.816, -12.166, 65.514), (112.781, -14.38, 66.492), (113.467, -13.997, 70.195), (110.179, -12.134, 70.783), (107.982, -13.35, 73.634), (105.068, -15.455, 72.451), (102.569, -17.213, 74.746), (102.575, -20.818, 73.554), (100.364, -23.809, 74.25), (101.348, -27.475, 73.862), (98.78, -27.961, 71.083), (99.91, -25.111, 68.837), (99.832, -25.756, 65.116), (100.081, -24.001, 61.744), (97.996, -24.956, 58.711), (100.388, -24.857, 55.761), (99.406, -24.739, 52.082), (95.645, -24.299, 52.441), (94.403, -21.389, 50.375), (90.83, -21.251, 51.752), (89.184, -21.806, 48.35), (85.483, -21.237, 49.0), (86.296, -21.254, 52.782), (88.057, -24.639, 52.759), (90.623, -23.593, 55.406), (87.855, -22.411, 57.782), (86.19, -25.817, 57.279), (89.563, -27.448, 58.213), (90.3, -25.128, 61.142), (86.949, -25.717, 62.887), (87.754, -29.444, 63.19), (91.337, -28.957, 64.464), (89.991, -26.625, 67.217), (87.163, -29.113, 67.894), (89.873, -31.733, 68.582), (91.58, -29.423, 71.141), (94.267, -27.85, 68.966), (95.09, -24.152, 68.645), (95.6, -23.038, 65.035), (98.015, -20.167, 65.573), (98.385, -19.63, 61.833), (96.973, -20.48, 58.388), (99.211, -20.243, 55.274), (97.936, -18.401, 53.351), (94.114, -18.601, 52.878), (92.687, -15.809, 55.106), (96.254, -15.477, 56.385), (98.071, -12.501, 57.787), (101.646, -11.276, 57.969), (101.385, -11.597, 61.777), (100.416, -15.286, 61.672), (103.366, -16.108, 59.379), (105.709, -14.266, 61.742), (104.342, -16.287, 64.697), (104.601, -19.582, 62.864), (108.087, -20.454, 64.156), (106.555, -20.702, 67.678), (104.232, -23.673, 67.028), (104.661, -27.354, 67.836), (103.738, -28.807, 64.455), (103.162, -27.859, 60.811), (100.219, -29.423, 59.009), (100.924, -29.304, 55.305), (97.937, -29.68, 52.974), (99.45, -28.511, 49.691), (102.78, -30.375, 49.611), (103.646, -28.993, 46.195), (103.601, -25.297, 47.04), (104.668, -25.207, 50.691), (106.913, -28.25, 51.367), (109.945, -25.956, 50.956), (109.176, -23.792, 53.999), (107.998, -26.748, 56.133), (111.323, -28.483, 55.489), (113.037, -25.188, 56.257), (111.219, -24.84, 59.618), (112.154, -28.42, 60.542), (115.847, -27.739, 59.732), (116.042, -24.333, 61.446), (113.416, -24.211, 64.232), (113.055, -27.961, 64.774), (109.287, -27.894, 64.1), (108.063, -31.22, 62.749), (105.625, -31.398, 59.875), (103.281, -33.806, 58.211), (100.99, -33.987, 55.189), (97.25, -34.528, 55.743), (94.146, -34.623, 53.586), (90.737, -33.291, 54.619), (88.884, -34.421, 51.47), (86.054, -36.908, 52.286), (85.232, -38.919, 55.428), (87.891, -41.615, 54.931), (90.822, -39.208, 54.572), (89.421, -36.92, 57.261), (89.357, -39.704, 59.854), (92.916, -40.859, 58.884), (94.293, -37.32, 59.121), (92.563, -36.48, 62.411), (93.887, -39.711, 64.011), (97.441, -39.255, 62.672), (97.588, -35.579, 63.722), (96.237, -36.405, 67.187), (98.882, -39.169, 67.511), (101.496, -36.367, 67.451), (100.108, -35.162, 70.786), (98.82, -37.361, 73.694), (95.875, -39.518, 74.754), (93.756, -36.475, 75.466), (94.022, -35.255, 71.87), (93.94, -38.852, 70.679), (90.653, -39.39, 72.498), (89.379, -36.037, 71.206), (90.007, -37.137, 67.58), (87.84, -40.204, 68.177), (85.082, -38.058, 69.656), (85.196, -35.603, 66.717), (85.008, -38.504, 64.253), (82.162, -40.212, 66.148), (80.093, -37.048, 66.436), (80.085, -36.446, 62.647), (79.438, -40.078, 61.796), (75.671, -39.869, 61.492), (76.122, -36.85, 59.181), (78.631, -38.729, 57.01), (76.414, -41.851, 56.89), (73.305, -39.984, 55.681), (75.39, -38.151, 53.004), (76.762, -41.51, 51.726), (73.332, -43.143, 51.818), (71.757, -40.326, 49.833), (74.378, -39.611, 47.213), (76.302, -42.85, 46.816), (73.402, -44.589, 45.083), (73.127, -41.69, 42.582), (76.833, -41.691, 41.807), (78.291, -45.161, 42.338), (80.12, -46.847, 39.502), (80.52, -43.686, 37.42), (84.003, -43.018, 36.039), (86.02, -39.799, 36.563), (88.875, -38.07, 34.697), (90.848, -35.501, 36.753), (93.328, -32.916, 35.545), (94.428, -30.047, 37.809), (97.614, -28.358, 39.063), (99.865, -29.979, 41.602), (97.873, -31.4, 44.512), (94.2, -31.986, 43.855), (94.288, -34.738, 41.225), (95.471, -37.23, 43.927), (94.037, -35.477, 46.981), (90.342, -35.751, 45.97), (90.439, -39.455, 45.062), (89.267, -40.725, 48.463), (86.21, -38.422, 48.46), (85.203, -39.747, 45.02), (85.701, -43.331, 46.315), (83.606, -42.525, 49.401), (80.752, -41.916, 46.87), (81.524, -45.133, 44.972), (83.038, -43.361, 41.917), (86.124, -44.56, 40.099), (89.106, -42.607, 38.735), (90.114, -43.76, 35.23), (92.622, -40.989, 34.432), (94.516, -38.488, 36.538), (96.915, -35.789, 35.459), (98.726, -32.736, 36.715), (99.73, -29.551, 34.933), (102.898, -28.97, 36.916), (103.484, -31.559, 39.625), (106.922, -33.022, 40.231), (107.979, -36.652, 40.326), (107.418, -36.758, 44.127), (103.734, -35.856, 43.504), (103.442, -38.703, 41.033), (105.129, -40.837, 43.678), (102.438, -39.648, 46.152), (99.699, -40.568, 43.619), (100.538, -44.254, 43.12), (98.867, -45.724, 46.219), (95.463, -44.187, 45.44), (95.125, -45.468, 41.82), (93.587, -48.69, 40.466), (95.72, -50.931, 38.307), (96.061, -49.716, 34.725), (95.024, -46.055, 35.348), (96.349, -43.456, 32.91), (98.672, -41.048, 34.794), (99.991, -38.076, 32.864), (101.757, -34.727, 32.996), (101.339, -31.295, 31.364), (98.806, -32.333, 28.747), (98.017, -30.225, 25.725), (94.37, -29.453, 24.957), (94.696, -31.811, 21.957), (95.765, -34.633, 24.306), (93.117, -33.882, 26.939), (90.286, -33.892, 24.373), (91.619, -37.089, 22.752), (91.835, -38.93, 26.126), (88.279, -37.833, 26.955), (86.796, -38.914, 23.597), (88.427, -42.332, 24.003), (87.942, -43.02, 27.736), (84.441, -41.547, 27.767), (84.493, -40.633, 31.443), (81.178, -39.991, 33.106), (82.505, -36.844, 34.783), (85.525, -34.605, 34.4), (87.136, -32.495, 37.103), (89.415, -29.779, 35.77), (89.77, -26.022, 35.343), (87.96, -23.121, 33.757), (89.995, -23.261, 30.536), (88.787, -26.849, 29.957), (85.195, -26.556, 31.231), (83.365, -25.158, 28.182), (85.089, -27.596, 25.804), (83.931, -30.729, 27.617), (80.447, -29.384, 28.236), (80.022, -29.095, 24.432), (81.253, -32.678, 24.074), (78.409, -33.658, 26.417), (80.583, -34.678, 29.396), (79.417, -33.502, 32.799), (82.154, -31.246, 34.143), (83.029, -29.839, 37.574), (85.454, -27.009, 38.088), (87.863, -28.513, 40.613), (90.053, -25.467, 41.132), (87.292, -22.946, 41.991), (84.766, -25.594, 43.0), (82.386, -24.086, 40.323), (81.636, -21.135, 42.683), (80.575, -23.608, 45.42), (82.442, -25.305, 48.314), (83.962, -25.537, 50.656), (84.842, -29.168, 49.949), (87.394, -29.763, 52.758), (86.246, -31.21, 56.102), (82.944, -32.642, 57.246), (80.933, -29.591, 56.189), (82.614, -29.696, 52.768), (81.835, -33.38, 52.015), (78.099, -32.723, 52.035), (78.624, -29.825, 49.558), (80.917, -31.963, 47.379), (78.422, -34.859, 47.258), (75.504, -32.532, 46.457), (77.529, -30.809, 43.747), (78.778, -33.973, 42.049), (75.238, -35.335, 41.898), (73.794, -32.037, 40.671), (76.113, -32.012, 37.663), (76.381, -35.709, 36.788), (72.613, -36.28, 36.937), (71.335, -32.969, 35.574), (68.466, -33.414, 33.074), (70.22, -31.213, 30.483), (72.839, -33.871, 29.666), (70.069, -35.92, 27.987), (69.305, -33.084, 25.541), (72.475, -32.67, 23.59), (70.989, -34.564, 20.686), (67.874, -33.129, 19.031), (65.057, -35.673, 19.358), (64.556, -35.917, 15.62), (68.137, -37.219, 15.166)]\",\n          \"[(31.71, 11.761, 52.412), (31.514, 9.193, 49.625), (29.806, 9.814, 46.288), (28.997, 7.649, 43.263), (30.723, 8.167, 39.909), (29.557, 8.082, 36.274), (28.425, 4.482, 36.793), (26.436, 3.921, 40.053), (28.42, 0.731, 40.678), (31.502, 2.819, 41.473), (32.137, 5.564, 44.037), (34.895, 7.446, 45.873), (35.64, 9.566, 48.946), (35.353, 13.329, 48.486), (38.41, 15.548, 49.044), (38.537, 18.376, 51.577), (36.422, 21.198, 50.205), (33.629, 19.115, 48.705), (30.888, 18.063, 51.091), (28.204, 17.303, 48.511), (27.334, 14.519, 46.096), (25.61, 15.561, 42.889), (23.872, 14.274, 39.786), (23.511, 16.307, 36.594), (20.58, 15.592, 34.257), (19.775, 16.859, 30.773), (15.989, 17.066, 30.307), (16.381, 15.039, 27.09), (17.798, 12.164, 29.134), (15.804, 12.462, 32.424), (16.547, 8.893, 33.501), (20.343, 8.881, 33.205), (22.538, 11.647, 34.655), (25.367, 12.788, 32.387), (27.643, 13.656, 35.304), (28.097, 12.398, 38.885), (30.557, 12.982, 41.728), (31.618, 15.351, 44.523), (31.234, 19.104, 44.436), (31.608, 22.247, 46.545), (28.841, 24.57, 45.389), (25.592, 22.91, 44.312), (24.92, 24.034, 40.701), (21.77, 25.977, 39.816), (19.026, 24.022, 38.035), (17.55, 25.37, 34.813), (14.705, 24.605, 32.433), (16.88, 22.113, 30.526), (19.556, 21.117, 33.043), (18.644, 19.609, 36.411), (20.921, 19.089, 39.409), (20.406, 16.906, 42.472), (22.637, 17.605, 45.479), (22.719, 15.634, 48.717), (25.172, 15.126, 51.589), (24.92, 11.422, 52.467), (27.049, 8.395, 51.55), (26.396, 7.322, 47.956), (23.204, 9.397, 47.911), (23.814, 10.282, 44.261)]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Embeddings_A\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"tensor([ 0.1932, -0.0438,  0.7770,  0.2603, -0.3194, -1.7208, -1.4078, -0.6996,\\n         0.0189, -0.1269, -0.1114,  0.0289, -0.0243, -0.4486,  0.3975, -0.9184,\\n        -0.5971, -0.6794,  0.5902, -0.6271,  0.7077, -1.0189, -0.6838,  0.1389,\\n         0.5883,  0.3247, -0.3489,  0.8280, -0.3439,  0.5802, -0.7491, -0.2291,\\n        -0.5009, -0.7292, -0.1835,  0.5171,  1.4258,  1.8320,  0.5417,  0.8492,\\n        -0.8175,  2.7374,  1.0742,  0.1407,  0.4130,  0.5768,  0.7884,  0.2863,\\n        -1.2062, -0.0364,  1.6911, -0.0357,  0.2333, -0.4863, -0.4992,  0.1613,\\n        -0.0985, -0.6607, -1.8245,  0.5379, -0.9565,  1.0563, -1.4902,  1.6625,\\n         1.3829,  1.7928,  1.1537,  0.2394, -0.4484,  1.8955, -0.8740, -0.8465,\\n         0.3923, -1.5792,  0.4460,  0.4548, -2.0304,  1.0429,  0.7816,  0.6989,\\n        -0.1515,  0.1173,  0.4065,  0.6353, -0.4137,  1.4591,  1.0226, -0.9343,\\n         0.8768, -0.5274, -0.7074, -0.4752,  0.9429, -0.6944,  1.4983, -0.7199,\\n         0.1939,  0.4820, -0.2272,  0.7446, -0.7093, -1.4029, -1.5726,  1.4787,\\n         0.9897, -0.1934,  2.2563, -2.1228, -1.4990,  0.4615,  0.7031, -1.9541,\\n         0.0641,  0.5342,  1.0368,  1.1606,  1.1048,  0.6281,  0.7778, -1.0983,\\n        -1.7432,  0.2546,  0.9125, -0.3288,  1.5194, -1.6024, -1.2193, -0.0449])\",\n          \"tensor([ 0.7350, -0.2884, -0.7331, -0.7960, -0.8200,  1.0313, -0.1044,  0.4783,\\n        -0.2676,  0.2732, -2.1090,  0.0986,  0.4673,  0.3303,  0.5112,  1.1833,\\n        -0.6922,  0.8333,  0.5963,  0.2443, -1.1902,  0.6678, -0.3492,  0.9420,\\n         1.2009, -0.8483, -0.9095, -0.3401,  1.0914,  0.1586,  0.2882, -1.5543,\\n        -0.9222,  0.0636,  1.9948, -0.3277, -1.8707,  2.4491, -0.4610, -0.2472,\\n         0.3108, -0.5968, -1.0566, -0.1052,  0.6031,  0.9572,  1.2935,  0.0418,\\n        -0.3512,  1.3487,  1.3821, -0.9277,  0.2236, -0.2478,  0.1683,  0.2058,\\n        -0.0521, -1.3163,  0.7822, -0.9709, -0.1929, -0.3891, -0.9428, -0.7208,\\n        -0.4438,  2.0569,  0.1037, -0.9490,  0.7021, -0.1087,  0.3758,  0.1162,\\n         0.5819, -0.3010,  1.3580, -0.5831, -1.5392,  0.3374, -1.0424,  0.9891,\\n         0.2113,  0.7609,  0.8256, -0.8018, -0.0363, -1.5735, -0.0380, -2.2124,\\n        -0.3428, -0.2282, -0.8958,  2.1578,  1.5648, -0.1831, -0.7047, -0.7464,\\n        -0.5548,  0.5114, -0.8352, -0.1087,  0.5404, -1.0571,  0.4857, -0.7313,\\n        -0.6553,  0.6990, -0.5941,  0.5032,  0.3857, -0.4530,  0.4655, -1.3029,\\n        -1.8165, -1.8961, -0.7004,  2.5454, -0.1494,  0.6350,  1.0086, -0.4680,\\n        -0.0762,  0.0207,  1.9094, -0.4845, -0.3726,  0.5891, -0.2158,  0.1434])\",\n          \"tensor([ 0.6414, -0.0384,  0.3825, -1.9274, -1.2375,  0.9715,  1.3125,  0.1222,\\n         0.8542, -0.3368, -0.7017,  0.3348,  0.8067,  2.0314,  1.3682,  1.9385,\\n         0.3900, -0.8299, -0.5781,  1.2312,  0.1148,  1.0726, -0.5933,  1.1959,\\n         0.9550,  0.0872, -0.2074, -0.4723,  1.2389,  0.5543, -0.7691, -1.7917,\\n        -2.0782, -0.7530, -0.0769, -0.6472, -2.2443, -0.0963, -0.7792, -0.4724,\\n        -0.6498,  0.3413, -1.1136, -0.7316, -0.2607,  0.2467, -1.3681,  0.6591,\\n         0.4668,  1.4563, -0.1728, -0.6572,  1.0865, -0.9022,  0.4764, -0.6187,\\n         1.0957,  0.6055, -1.1806,  0.6724,  0.3493, -0.1033, -0.3005,  0.9061,\\n        -1.4639, -0.5080,  1.3173, -0.2266,  0.3324, -0.1230,  0.2792,  0.9839,\\n         0.0706, -0.8127,  0.0749,  0.2836, -0.2764, -1.0946,  0.7785,  1.0780,\\n         2.1112,  0.5491, -0.3664,  0.0259, -0.4443, -0.5874,  1.2959, -1.1893,\\n        -0.1709, -0.8653,  0.8819, -0.5614,  1.5166, -1.0842, -0.8888, -0.0302,\\n         0.9849, -0.9981,  2.7772,  1.3222, -0.9373, -0.2855, -0.6465,  0.6447,\\n         1.4073,  0.7010, -1.4945,  0.0205, -1.6256, -0.6832, -1.4431,  2.1343,\\n         0.3944, -0.5088, -0.3539, -0.0480, -1.9542, -0.4614, -0.7048,  0.0281,\\n        -0.2833,  0.7880, -0.3362, -1.3542,  0.3187, -1.5059, -0.8576,  0.2455])\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sequence_A\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"LKCNKLVPLFYKTCPAGKNLCYKMFMVATPKVPVKRGCIDVCPKSSLLVKYVCCNTDRCN\",\n          \"?TLTSSATGTHNGYYYSFWTDGQGNIRFNLESGGQYSVTWSGNGNWVGGKGWNPGTDNRVINYTADYRPNGNSYLAVYGWTRNPLIEYYVVESFGTYDPSTGATRMGSVTTDGGTYNIYRTQRVNAPSIEGTKTFYQYWSVRTSKRTGGTVTMANHFNAWRQAGLQLGSHDYQIVATEGYYSSGSATVNVG\",\n          \"TNATGERNLALIQEVLEVFPETARKERRKHMMVSDPKMKSVGKCIISNRKSQPGVMTVRGCAYAGSKGVVFGPIKDMAHISHGPVGCGQYSRAGRRNYYTGVSGVDSFGTLNFTSDFQERDIVFGGDKKLSKLIEEMELLFPLTKGITIQSECPVGLIGDDISAVANASSKALDKPVIPVRCEGFRGVSQSLGHHIANDVVRDWILNNREGQPFETTPYDVAIIGDYNIGGDAWASRILLEEMGLRVVAQWSGDGTLVEMENTPFVKLNLVHCYRSMNYIARHMEEKHQIPWMEYNFFGPTKIAESLRKIADQFDDTIRANAEAVIARYEGQMAAIIAKYRPRLEGRKVLLYMGGLRPRHVIGAYEDLGMEIIAAGYEFAHNDDYDRTLPDLKEGTLLFDDASSYELEAFVKALKPDLIGSGIKEKYIFQKMGVPFRQMHSWDYSGPYHGYDGFAIFARDMDMTLNNPAWNELTAPWL\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Embeddings_B\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"tensor([ 0.2015,  0.3117, -0.5839,  1.0137, -0.5239,  0.2147, -1.0775, -0.5789,\\n         1.1240,  0.6256,  0.6179,  1.0665,  0.5054, -1.2133, -0.5947, -0.6140,\\n        -0.1339, -1.6003,  2.8583, -1.8930,  1.5101,  0.6939, -1.2322, -1.6160,\\n         1.1362, -0.8558,  0.8675, -1.0341,  1.5845, -0.7635, -0.4443, -0.3923,\\n        -0.0405,  2.4791,  0.3188, -0.6055,  0.5539, -0.0529, -0.5174,  0.1098,\\n        -0.8764,  0.7357, -1.6964,  1.4774, -0.4675, -0.4977, -1.1832, -0.7054,\\n         0.5766,  1.3424,  1.3411,  2.7719, -0.8330, -1.8117,  2.6831,  1.1887,\\n         0.8531, -1.3065, -1.1182, -0.2140, -0.4845, -0.8717, -0.7939,  0.9647,\\n         0.9732, -0.1867,  1.1045, -0.1404,  0.0866, -0.2026, -1.1066, -0.3748,\\n        -0.5274,  1.2247,  0.5664,  0.2532, -0.7114, -0.5454, -1.7090,  0.0734,\\n         0.9922, -2.3459, -1.0884, -1.3438,  1.8885,  0.9382,  0.8759,  0.4443,\\n        -0.7112,  1.7171, -0.9193,  0.9294, -0.4971, -1.3532, -0.3289,  1.4214,\\n        -0.5983, -0.8562, -0.0198,  0.7314,  0.3883,  0.1812,  1.7668, -0.4402,\\n         0.6933, -2.3000, -0.8928,  1.8306, -0.8122, -0.7518, -0.2742, -0.7141,\\n        -1.4239,  0.6780, -1.0558,  0.3919, -0.1764, -0.5467,  0.1651,  0.6444,\\n         0.2843,  1.8141, -1.0795, -1.0203,  2.4535,  0.5315,  1.2934, -1.4929])\",\n          \"tensor([ 0.6414, -0.0384,  0.3825, -1.9274, -1.2375,  0.9715,  1.3125,  0.1222,\\n         0.8542, -0.3368, -0.7017,  0.3348,  0.8067,  2.0314,  1.3682,  1.9385,\\n         0.3900, -0.8299, -0.5781,  1.2312,  0.1148,  1.0726, -0.5933,  1.1959,\\n         0.9550,  0.0872, -0.2074, -0.4723,  1.2389,  0.5543, -0.7691, -1.7917,\\n        -2.0782, -0.7530, -0.0769, -0.6472, -2.2443, -0.0963, -0.7792, -0.4724,\\n        -0.6498,  0.3413, -1.1136, -0.7316, -0.2607,  0.2467, -1.3681,  0.6591,\\n         0.4668,  1.4563, -0.1728, -0.6572,  1.0865, -0.9022,  0.4764, -0.6187,\\n         1.0957,  0.6055, -1.1806,  0.6724,  0.3493, -0.1033, -0.3005,  0.9061,\\n        -1.4639, -0.5080,  1.3173, -0.2266,  0.3324, -0.1230,  0.2792,  0.9839,\\n         0.0706, -0.8127,  0.0749,  0.2836, -0.2764, -1.0946,  0.7785,  1.0780,\\n         2.1112,  0.5491, -0.3664,  0.0259, -0.4443, -0.5874,  1.2959, -1.1893,\\n        -0.1709, -0.8653,  0.8819, -0.5614,  1.5166, -1.0842, -0.8888, -0.0302,\\n         0.9849, -0.9981,  2.7772,  1.3222, -0.9373, -0.2855, -0.6465,  0.6447,\\n         1.4073,  0.7010, -1.4945,  0.0205, -1.6256, -0.6832, -1.4431,  2.1343,\\n         0.3944, -0.5088, -0.3539, -0.0480, -1.9542, -0.4614, -0.7048,  0.0281,\\n        -0.2833,  0.7880, -0.3362, -1.3542,  0.3187, -1.5059, -0.8576,  0.2455])\",\n          \"tensor([-1.9606, -0.0789, -1.0516,  1.0233, -1.9376,  0.0048,  1.4564, -0.2826,\\n         0.9928, -0.7321,  0.2605, -1.1617, -1.9045, -0.5094,  1.6443,  0.8993,\\n        -0.5728, -0.4137,  1.3776,  0.3589, -0.1369, -0.1433,  0.2082, -2.1259,\\n         0.5695, -0.1116,  0.1403, -1.2130,  0.0179, -1.8902, -1.0072,  0.2011,\\n        -1.3431, -0.0556,  1.9390, -1.0001, -0.0221, -0.4739, -0.3450,  1.5603,\\n        -0.9024, -2.3155, -1.3712, -0.6494,  0.0187,  0.7018,  1.1420, -0.2633,\\n         0.5399, -0.8688, -0.2663,  0.4806,  0.4817, -1.5876,  1.9451,  0.4453,\\n         0.2921, -0.0622, -1.0344,  0.0873,  0.5145, -1.0429,  0.9856,  1.5482,\\n        -1.6083, -1.8402, -3.4806,  1.2436, -0.7513,  1.2344,  0.0140, -0.5014,\\n        -0.5876, -0.6243,  1.6610, -1.1263, -2.0587, -1.0505,  0.3005,  0.4084,\\n         0.1140, -1.8573,  1.1576, -1.1603,  0.6983, -0.0543, -0.1374,  1.4700,\\n        -2.2551,  1.9024, -0.4222, -1.7369,  1.1721, -0.7713, -1.2732,  0.1466,\\n        -0.1775,  0.2976,  1.2071,  0.9345,  0.6621,  1.1680, -1.3691, -1.1835,\\n        -0.9677,  0.2014,  1.4170, -1.3794, -0.0754,  1.3146, -0.8791, -0.5325,\\n        -1.9114,  2.4167, -0.9692, -1.5768,  0.3330,  0.5996, -0.4446, -1.9533,\\n        -0.4040,  0.5546,  0.2771,  0.8048,  1.1107,  1.0745,  0.2420,  0.7703])\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sequence_B\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"LKCNKLVPLFYKTCPAGKNLCYKMFMVATPKVPVKRGCIDVCPKSSLLVKYVCCNTDRCN\",\n          \"?TLTSSATGTHNGYYYSFWTDGQGNIRFNLESGGQYSVTWSGNGNWVGGKGWNPGTDNRVINYTADYRPNGNSYLAVYGWTRNPLIEYYVVESFGTYDPSTGATRMGSVTTDGGTYNIYRTQRVNAPSIEGTKTFYQYWSVRTSKRTGGTVTMANHFNAWRQAGLQLGSHDYQIVATEGYYSSGSATVNVG?\",\n          \"SQTIDKINSCYPLFEQDEYQELFRNKRQLEEAHDAQRVQEVFAWTTTAEYEALNFRREALTVDPAKACQPLGAVLCSLGFANTLPYVHGSQGCVAYFRTYFNRHFKEPIACVSDSMTEDAAVFGGNNNMNLGLQNASALYKPEIIAVSTTCMAEVIGDDLQAFIANAKKDGFVDSSIAVPHAHTPSFIGSHVTGWDNMFEGFAKTFTADYQGQPGKLPKLNLVTGFETYLGNFRVLKRMMEQMAVPCSLLSDPSEVLDTPADGHYRMYSGGTTQQEMKEAPDAIDTLLLQPWQLLKSKKVVQEMWNQPATEVAIPLGLAATDELLMTVSQLSGKPIADALTLERGRLVDMMLDSHTWLHGKKFGLYGDPDFVMGLTRFLLELGCEPTVILSHNANKRWQKAMNKMLDASPYGRDSEVFINCDLWHFRSLMFTRQPDFMIGNSYGKFIQRDTLAKGKAFEVPLIRLGFPLFDRHHLHRQTTWGYEGAMNIVTTLVNAVLEKLDSDTSQLGKTDYSFDLVR\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokenized_sequence_A\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"[5, 12, 23, 17, 12, 5, 8, 16, 5, 19, 20, 12, 15, 23, 16, 6, 7, 12, 17, 5, 23, 20, 12, 21, 19, 21, 8, 6, 15, 16, 12, 8, 16, 8, 12, 13, 7, 23, 11, 14, 8, 23, 16, 12, 10, 10, 5, 5, 8, 12, 20, 8, 23, 23, 17, 15, 14, 13, 23, 17]\",\n          \"[1, 15, 5, 15, 10, 10, 6, 15, 7, 15, 22, 17, 7, 20, 20, 20, 10, 19, 24, 15, 14, 7, 18, 7, 17, 11, 13, 19, 17, 5, 9, 10, 7, 7, 18, 20, 10, 8, 15, 24, 10, 7, 17, 7, 17, 24, 8, 7, 7, 12, 7, 24, 17, 16, 7, 15, 14, 17, 13, 8, 11, 17, 20, 15, 6, 14, 20, 13, 16, 17, 7, 17, 10, 20, 5, 6, 8, 20, 7, 24, 15, 13, 17, 16, 5, 11, 9, 20, 20, 8, 8, 9, 10, 19, 7, 15, 20, 14, 16, 10, 15, 7, 6, 15, 13, 21, 7, 10, 8, 15, 15, 14, 7, 7, 15, 20, 17, 11, 20, 13, 15, 18, 13, 8, 17, 6, 16, 10, 11, 9, 7, 15, 12, 15, 19, 20, 18, 20, 24, 10, 8, 13, 15, 10, 12, 13, 15, 7, 7, 15, 8, 15, 21, 6, 17, 22, 19, 17, 6, 24, 13, 18, 6, 7, 5, 18, 5, 7, 10, 22, 14, 20, 18, 11, 8, 6, 15, 9, 7, 20, 20, 10, 10, 7, 10, 6, 15, 8, 17, 8, 7]\",\n          \"[15, 17, 6, 15, 7, 9, 13, 17, 5, 6, 5, 11, 18, 9, 8, 5, 9, 8, 19, 16, 9, 15, 6, 13, 12, 9, 13, 13, 12, 22, 21, 21, 8, 10, 14, 16, 12, 21, 12, 10, 8, 7, 12, 23, 11, 11, 10, 17, 13, 12, 10, 18, 16, 7, 8, 21, 15, 8, 13, 7, 23, 6, 20, 6, 7, 10, 12, 7, 8, 8, 19, 7, 16, 11, 12, 14, 21, 6, 22, 11, 10, 22, 7, 16, 8, 7, 23, 7, 18, 20, 10, 13, 6, 7, 13, 13, 17, 20, 20, 15, 7, 8, 10, 7, 8, 14, 10, 19, 7, 15, 5, 17, 19, 15, 10, 14, 19, 18, 9, 13, 14, 11, 8, 19, 7, 7, 14, 12, 12, 5, 10, 12, 5, 11, 9, 9, 21, 9, 5, 5, 19, 16, 5, 15, 12, 7, 11, 15, 11, 18, 10, 9, 23, 16, 8, 7, 5, 11, 7, 14, 14, 11, 10, 6, 8, 6, 17, 6, 10, 10, 12, 6, 5, 14, 12, 16, 8, 11, 16, 8, 13, 23, 9, 7, 19, 13, 7, 8, 10, 18, 10, 5, 7, 22, 22, 11, 6, 17, 14, 8, 8, 13, 14, 24, 11, 5, 17, 17, 13, 9, 7, 18, 16, 19, 9, 15, 15, 16, 20, 14, 8, 6, 11, 11, 7, 14, 20, 17, 11, 7, 7, 14, 6, 24, 6, 10, 13, 11, 5, 5, 9, 9, 21, 7, 5, 13, 8, 8, 6, 18, 24, 10, 7, 14, 7, 15, 5, 8, 9, 21, 9, 17, 15, 16, 19, 8, 12, 5, 17, 5, 8, 22, 23, 20, 13, 10, 21, 17, 20, 11, 6, 13, 22, 21, 9, 9, 12, 22, 18, 11, 16, 24, 21, 9, 20, 17, 19, 19, 7, 16, 15, 12, 11, 6, 9, 10, 5, 13, 12, 11, 6, 14, 18, 19, 14, 14, 15, 11, 13, 6, 17, 6, 9, 6, 8, 11, 6, 13, 20, 9, 7, 18, 21, 6, 6, 11, 11, 6, 12, 20, 13, 16, 13, 5, 9, 7, 13, 12, 8, 5, 5, 20, 21, 7, 7, 5, 13, 16, 13, 22, 8, 11, 7, 6, 20, 9, 14, 5, 7, 21, 9, 11, 11, 6, 6, 7, 20, 9, 19, 6, 22, 17, 14, 14, 20, 14, 13, 15, 5, 16, 14, 5, 12, 9, 7, 15, 5, 5, 19, 14, 14, 6, 10, 10, 20, 9, 5, 9, 6, 19, 8, 12, 6, 5, 12, 16, 14, 5, 11, 7, 10, 7, 11, 12, 9, 12, 20, 11, 19, 18, 12, 21, 7, 8, 16, 19, 13, 18, 21, 22, 10, 24, 14, 20, 10, 7, 16, 20, 22, 7, 20, 14, 7, 19, 6, 11, 19, 6, 13, 14, 21, 14, 21, 15, 5, 17, 17, 16, 6, 24, 17, 9, 5, 15, 6, 16, 24, 5]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokenized_sequence_B\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"[5, 12, 23, 17, 12, 5, 8, 16, 5, 19, 20, 12, 15, 23, 16, 6, 7, 12, 17, 5, 23, 20, 12, 21, 19, 21, 8, 6, 15, 16, 12, 8, 16, 8, 12, 13, 7, 23, 11, 14, 8, 23, 16, 12, 10, 10, 5, 5, 8, 12, 20, 8, 23, 23, 17, 15, 14, 13, 23, 17]\",\n          \"[1, 15, 5, 15, 10, 10, 6, 15, 7, 15, 22, 17, 7, 20, 20, 20, 10, 19, 24, 15, 14, 7, 18, 7, 17, 11, 13, 19, 17, 5, 9, 10, 7, 7, 18, 20, 10, 8, 15, 24, 10, 7, 17, 7, 17, 24, 8, 7, 7, 12, 7, 24, 17, 16, 7, 15, 14, 17, 13, 8, 11, 17, 20, 15, 6, 14, 20, 13, 16, 17, 7, 17, 10, 20, 5, 6, 8, 20, 7, 24, 15, 13, 17, 16, 5, 11, 9, 20, 20, 8, 8, 9, 10, 19, 7, 15, 20, 14, 16, 10, 15, 7, 6, 15, 13, 21, 7, 10, 8, 15, 15, 14, 7, 7, 15, 20, 17, 11, 20, 13, 15, 18, 13, 8, 17, 6, 16, 10, 11, 9, 7, 15, 12, 15, 19, 20, 18, 20, 24, 10, 8, 13, 15, 10, 12, 13, 15, 7, 7, 15, 8, 15, 21, 6, 17, 22, 19, 17, 6, 24, 13, 18, 6, 7, 5, 18, 5, 7, 10, 22, 14, 20, 18, 11, 8, 6, 15, 9, 7, 20, 20, 10, 10, 7, 10, 6, 15, 8, 17, 8, 7, 1]\",\n          \"[10, 18, 15, 11, 14, 12, 11, 17, 10, 23, 20, 16, 5, 19, 9, 18, 14, 9, 20, 18, 9, 5, 19, 13, 17, 12, 13, 18, 5, 9, 9, 6, 22, 14, 6, 18, 13, 8, 18, 9, 8, 19, 6, 24, 15, 15, 15, 6, 9, 20, 9, 6, 5, 17, 19, 13, 13, 9, 6, 5, 15, 8, 14, 16, 6, 12, 6, 23, 18, 16, 5, 7, 6, 8, 5, 23, 10, 5, 7, 19, 6, 17, 15, 5, 16, 20, 8, 22, 7, 10, 18, 7, 23, 8, 6, 20, 19, 13, 15, 20, 19, 17, 13, 22, 19, 12, 9, 16, 11, 6, 23, 8, 10, 14, 10, 21, 15, 9, 14, 6, 6, 8, 19, 7, 7, 17, 17, 17, 21, 17, 5, 7, 5, 18, 17, 6, 10, 6, 5, 20, 12, 16, 9, 11, 11, 6, 8, 10, 15, 15, 23, 21, 6, 9, 8, 11, 7, 14, 14, 5, 18, 6, 19, 11, 6, 17, 6, 12, 12, 14, 7, 19, 8, 14, 10, 10, 11, 6, 8, 16, 22, 6, 22, 15, 16, 10, 19, 11, 7, 10, 22, 8, 15, 7, 24, 14, 17, 21, 19, 9, 7, 19, 6, 12, 15, 19, 15, 6, 14, 20, 18, 7, 18, 16, 7, 12, 5, 16, 12, 5, 17, 5, 8, 15, 7, 19, 9, 15, 20, 5, 7, 17, 19, 13, 8, 5, 12, 13, 21, 21, 9, 18, 21, 6, 8, 16, 23, 10, 5, 5, 10, 14, 16, 10, 9, 8, 5, 14, 15, 16, 6, 14, 7, 22, 20, 13, 21, 20, 10, 7, 7, 15, 15, 18, 18, 9, 21, 12, 9, 6, 16, 14, 6, 11, 14, 15, 5, 5, 5, 18, 16, 24, 18, 5, 5, 12, 10, 12, 12, 8, 8, 18, 9, 21, 24, 17, 18, 16, 6, 15, 9, 8, 6, 11, 16, 5, 7, 5, 6, 6, 15, 14, 9, 5, 5, 21, 15, 8, 10, 18, 5, 10, 7, 12, 16, 11, 6, 14, 6, 5, 15, 5, 9, 13, 7, 13, 5, 8, 14, 21, 21, 5, 14, 10, 22, 15, 24, 5, 22, 7, 12, 12, 19, 7, 5, 20, 7, 14, 16, 14, 19, 8, 21, 7, 5, 15, 13, 19, 5, 5, 9, 5, 7, 23, 9, 16, 15, 8, 11, 5, 10, 22, 17, 6, 17, 12, 13, 24, 18, 12, 6, 21, 17, 12, 21, 5, 14, 6, 10, 16, 20, 7, 13, 14, 10, 9, 8, 19, 11, 17, 23, 14, 5, 24, 22, 19, 13, 10, 5, 21, 19, 15, 13, 18, 16, 14, 19, 21, 11, 7, 17, 10, 20, 7, 12, 19, 11, 18, 13, 14, 15, 5, 6, 12, 7, 12, 6, 19, 9, 8, 16, 5, 11, 13, 5, 7, 19, 16, 5, 19, 14, 13, 22, 22, 5, 22, 13, 18, 15, 15, 24, 7, 20, 9, 7, 6, 21, 17, 11, 8, 15, 15, 5, 8, 17, 6, 8, 5, 9, 12, 5, 14, 10, 14, 15, 10, 18, 5, 7, 12, 15, 14, 20, 10, 19, 14, 5, 8, 13]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokenized_masked_sequence_A\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 5, 17, 19, 13, 32, 32, 32, 5, 15, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 5, 32, 20, 32, 32, 32, 10, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 19, 32, 32, 32, 32, 32, 23, 8, 10, 14, 10, 32, 15, 9, 14, 32, 32, 32, 32, 32, 32, 32, 32, 32, 21, 32, 32, 32, 5, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 6, 9, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 16, 10, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 14, 15, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 24, 32, 32, 13, 10, 32, 32, 19, 15, 13, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 12, 19, 32, 32, 32, 32, 32, 5, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 22, 22, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32]\",\n          \"[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 8, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 5, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32]\",\n          \"[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 14, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 15, 14, 15, 15, 22, 32, 32, 32, 11, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 14, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 11, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 8, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 9, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 10, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 9, 32, 10, 10, 32, 10, 19, 32, 32, 32, 32]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokenized_masked_sequence_B\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"[15, 17, 32, 32, 32, 32, 32, 32, 5, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 22, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 7, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 13, 7, 23, 6, 20, 32, 32, 32, 12, 7, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 20, 32, 13, 32, 32, 13, 13, 32, 32, 20, 32, 32, 32, 32, 32, 32, 32, 10, 19, 32, 32, 32, 17, 19, 15, 10, 32, 19, 18, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 12, 32, 32, 9, 9, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 9, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 7, 8, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 9, 17, 15, 16, 19, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32]\",\n          \"[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 8, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 5, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32]\",\n          \"[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 14, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 15, 15, 22, 15, 16, 32, 32, 32, 10, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 6, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 16, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 8, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 8, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 12, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 19, 32, 6, 14, 32]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sum_tokenized_sequence_A\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"[42, 50, 47, 43, 46, 44, 43, 49, 42, 55, 52, 48, 37, 51, 41, 50, 46, 41, 52, 50, 41, 37, 51, 45, 49, 44, 45, 50, 37, 41, 41, 38, 54, 46, 38, 50, 45, 40, 50, 41, 40, 51, 38, 56, 47, 47, 47, 38, 41, 52, 41, 38, 10, 34, 38, 26, 45, 41, 38, 10, 30, 40, 46, 48, 38, 44, 38, 55, 50, 48, 37, 39, 38, 40, 37, 55, 42, 37, 39, 51, 38, 49, 47, 10, 48, 40, 40, 54, 39, 20, 50, 39, 55, 40, 38, 52, 51, 45, 47, 52, 51, 49, 45, 54, 38, 44, 41, 48, 43, 38, 46, 16, 20, 28, 20, 53, 30, 18, 28, 38, 38, 40, 51, 39, 39, 49, 49, 49, 42, 49, 37, 39, 10, 50, 49, 38, 42, 38, 37, 52, 44, 48, 41, 43, 43, 38, 40, 42, 47, 47, 55, 53, 12, 18, 40, 43, 39, 46, 46, 37, 50, 38, 51, 43, 38, 49, 38, 44, 44, 46, 39, 51, 40, 46, 42, 42, 43, 38, 40, 48, 54, 38, 54, 47, 32, 20, 51, 43, 39, 42, 54, 40, 47, 39, 56, 46, 49, 53, 51, 41, 39, 51, 38, 44, 47, 51, 47, 38, 46, 52, 50, 39, 50, 48, 39, 44, 37, 48, 44, 37, 49, 37, 40, 47, 39, 51, 41, 47, 52, 37, 39, 49, 51, 45, 40, 37, 44, 45, 53, 53, 41, 50, 53, 38, 40, 48, 55, 42, 37, 37, 42, 46, 48, 42, 41, 40, 37, 28, 30, 48, 38, 46, 39, 54, 52, 45, 53, 52, 42, 39, 39, 47, 47, 50, 50, 41, 53, 44, 41, 38, 48, 46, 38, 43, 46, 47, 37, 37, 37, 50, 48, 56, 50, 37, 37, 44, 42, 44, 44, 40, 40, 50, 41, 53, 56, 49, 50, 48, 38, 47, 41, 40, 38, 43, 48, 37, 39, 37, 38, 38, 47, 46, 41, 37, 37, 53, 47, 40, 42, 50, 37, 42, 39, 44, 48, 43, 38, 46, 38, 37, 47, 37, 41, 45, 39, 45, 37, 40, 46, 53, 53, 37, 46, 42, 54, 47, 56, 37, 54, 39, 44, 44, 51, 39, 37, 52, 39, 46, 48, 46, 51, 40, 53, 39, 37, 47, 45, 51, 37, 37, 41, 37, 39, 55, 41, 48, 47, 40, 43, 37, 42, 54, 49, 38, 49, 44, 45, 56, 50, 44, 38, 53, 49, 44, 53, 37, 46, 38, 42, 48, 52, 39, 45, 46, 42, 41, 40, 51, 43, 49, 55, 46, 37, 48, 54, 51, 26, 20, 37, 53, 38, 30, 26, 50, 48, 46, 51, 53, 43, 39, 49, 42, 52, 39, 24, 38, 43, 50, 45, 46, 47, 10, 38, 44, 39, 44, 38, 51, 41, 40, 48, 37, 43, 45, 37, 39, 51, 48, 37, 51, 46, 45, 44, 44, 37, 54, 45, 50, 47, 47, 56, 39, 52, 41, 39, 38, 53, 49, 43, 40, 47, 47, 37, 40, 49, 38, 40, 37, 41, 44, 37, 46, 42, 46, 47, 42, 50, 37, 39, 44, 47, 46, 52, 42, 51, 46, 37, 40, 45]\",\n          \"[37, 44, 55, 49, 44, 37, 40, 48, 37, 51, 52, 44, 47, 55, 48, 38, 39, 44, 49, 37, 55, 52, 44, 53, 51, 53, 40, 38, 47, 48, 44, 40, 48, 16, 44, 45, 39, 55, 43, 46, 40, 55, 48, 44, 42, 42, 37, 10, 40, 44, 52, 40, 55, 55, 49, 47, 46, 45, 55, 49]\",\n          \"[42, 42, 37, 43, 40, 41, 46, 38, 48, 46, 54, 40, 45, 48, 52, 40, 43, 45, 54, 52, 42, 54, 38, 45, 38, 40, 47, 40, 46, 47, 50, 37, 52, 45, 51, 52, 40, 47, 39, 48, 42, 42, 39, 52, 38, 51, 47, 37, 53, 39, 47, 49, 38, 48, 54, 42, 46, 38, 37, 39, 40, 37, 48, 54, 43, 54, 50, 44, 54, 52, 41, 49, 51, 52, 55, 49, 44, 39, 42, 51, 50, 37, 56, 38, 50, 42, 39, 49, 41, 47, 50, 50, 47, 45, 40, 37, 42, 42, 39, 46, 52, 39, 42, 40, 48, 45, 49, 40, 47, 54, 47, 51, 50, 43, 50, 28, 48, 46, 47, 41, 53, 47, 39, 40, 43, 40, 48, 39, 39, 51, 41, 46, 37, 51, 52, 52, 37, 39, 47, 49, 38, 30, 28, 30, 30, 44, 47, 48, 52, 22, 48, 42, 42, 42, 42, 42, 47, 47, 39, 48, 46, 42, 42, 47, 43, 42, 47, 37, 50, 42, 51, 46, 40, 52, 38, 41, 37, 42, 51, 47, 48, 45, 47, 28, 47, 40, 49, 39, 47, 38, 48, 38, 49, 47, 40, 56, 54, 47, 39, 38, 49, 38, 37, 38, 42, 47, 38, 39, 46, 48, 52, 51, 22, 38, 49, 39, 56, 39, 48, 44, 52, 37, 49, 42, 50, 52, 39, 52, 50, 43, 40, 38, 48, 51, 40, 47, 38, 47, 50, 38, 50, 46, 47, 49, 52, 47, 37, 42, 47, 43, 42, 53, 42, 47, 47, 48, 42, 47, 40, 47, 40, 48, 47, 56, 42, 51, 48, 39, 38, 55, 38, 51, 50, 16, 50, 41, 39, 45, 40, 40, 40, 50, 43, 39, 46, 52, 38, 38, 47, 18, 37, 39, 42, 39, 46, 40, 38, 51, 43, 48, 39, 39, 40, 41, 51, 44, 52, 52, 20, 41, 38, 52, 51, 42, 44, 40, 37, 51, 40, 42, 42, 39, 42, 46, 39, 37, 46, 50, 49, 37, 40, 49, 39, 39, 41, 18, 56, 20, 20, 40, 20, 38, 48, 38, 46, 56]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sum_tokenized_sequence_B\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"[30, 34, 38, 47, 39, 41, 45, 49, 10, 38, 37, 43, 50, 41, 40, 37, 41, 40, 51, 48, 41, 47, 38, 45, 44, 41, 45, 45, 44, 44, 53, 53, 40, 42, 46, 48, 44, 53, 44, 42, 40, 14, 44, 55, 43, 43, 42, 49, 45, 44, 42, 50, 48, 39, 40, 53, 47, 40, 26, 14, 46, 12, 40, 38, 39, 42, 24, 14, 40, 40, 51, 39, 48, 43, 44, 46, 53, 38, 54, 43, 42, 54, 39, 48, 40, 39, 55, 39, 50, 40, 42, 26, 38, 39, 26, 26, 49, 52, 40, 47, 39, 40, 42, 39, 40, 46, 20, 38, 39, 47, 37, 34, 38, 30, 20, 46, 38, 36, 41, 45, 46, 43, 40, 51, 39, 39, 46, 44, 44, 37, 42, 24, 37, 43, 18, 18, 53, 41, 37, 37, 51, 48, 37, 47, 44, 39, 43, 47, 43, 50, 42, 18, 55, 48, 40, 39, 37, 43, 39, 46, 46, 43, 42, 38, 40, 38, 49, 38, 42, 42, 44, 38, 37, 46, 44, 48, 40, 43, 48, 40, 45, 55, 41, 39, 51, 45, 14, 16, 42, 50, 42, 37, 39, 54, 54, 43, 38, 49, 46, 40, 40, 45, 46, 56, 43, 37, 49, 49, 45, 41, 39, 50, 48, 51, 41, 47, 47, 48, 52, 46, 40, 38, 43, 43, 39, 46, 52, 49, 43, 39, 39, 46, 38, 56, 38, 42, 45, 43, 37, 37, 41, 41, 53, 39, 37, 45, 40, 40, 38, 50, 56, 42, 39, 46, 39, 47, 37, 40, 41, 53, 18, 34, 30, 32, 38, 40, 44, 37, 49, 37, 40, 54, 55, 52, 45, 42, 53, 49, 52, 43, 38, 45, 54, 53, 41, 41, 44, 54, 50, 43, 48, 56, 53, 41, 52, 49, 51, 51, 39, 48, 47, 44, 43, 38, 41, 42, 37, 45, 44, 43, 38, 46, 50, 51, 46, 46, 47, 43, 45, 38, 49, 38, 41, 38, 40, 43, 38, 45, 52, 41, 39, 50, 53, 38, 38, 43, 43, 38, 44, 52, 45, 48, 45, 37, 41, 39, 45, 44, 40, 37, 37, 52, 53, 39, 39, 37, 45, 48, 45, 54, 40, 43, 39, 38, 52, 41, 46, 37, 39, 53, 41, 43, 43, 38, 38, 39, 52, 41, 51, 38, 54, 49, 46, 46, 52, 46, 45, 47, 37, 48, 46, 37, 44, 41, 39, 47, 37, 37, 51, 46, 46, 38, 42, 42, 52, 41, 37, 41, 38, 51, 40, 44, 38, 37, 44, 48, 46, 37, 43, 39, 42, 39, 43, 44, 41, 44, 52, 43, 51, 50, 44, 53, 39, 40, 48, 51, 45, 50, 53, 54, 42, 56, 46, 52, 42, 39, 48, 52, 54, 39, 52, 46, 39, 51, 38, 43, 51, 38, 45, 46, 53, 46, 53, 47, 37, 49, 49, 48, 38, 56, 49, 41, 37, 47, 38, 48, 56, 37]\",\n          \"[37, 44, 55, 49, 44, 37, 40, 48, 37, 51, 52, 44, 47, 55, 48, 38, 39, 44, 49, 37, 55, 52, 44, 53, 51, 53, 40, 38, 47, 48, 44, 40, 48, 16, 44, 45, 39, 55, 43, 46, 40, 55, 48, 44, 42, 42, 37, 10, 40, 44, 52, 40, 55, 55, 49, 47, 46, 45, 55, 49]\",\n          \"[42, 37, 43, 40, 41, 46, 38, 48, 46, 54, 40, 45, 48, 52, 40, 43, 45, 54, 52, 42, 54, 38, 45, 38, 40, 47, 40, 46, 47, 50, 37, 52, 45, 51, 52, 40, 47, 39, 48, 42, 42, 39, 52, 38, 51, 47, 37, 53, 39, 47, 49, 38, 48, 54, 42, 46, 38, 37, 39, 40, 37, 48, 54, 43, 54, 50, 44, 54, 52, 41, 49, 51, 52, 55, 49, 44, 39, 42, 51, 50, 37, 56, 38, 50, 42, 39, 49, 41, 47, 50, 50, 47, 45, 40, 37, 42, 42, 39, 46, 52, 39, 42, 40, 48, 45, 49, 40, 47, 54, 47, 51, 50, 43, 50, 46, 48, 28, 47, 41, 53, 47, 39, 40, 43, 40, 48, 39, 39, 51, 41, 46, 37, 51, 52, 52, 37, 39, 47, 49, 38, 47, 46, 30, 30, 44, 30, 32, 52, 43, 48, 20, 42, 42, 47, 47, 39, 48, 46, 42, 42, 47, 43, 42, 47, 37, 50, 42, 51, 46, 40, 52, 38, 41, 37, 42, 51, 47, 48, 45, 47, 46, 47, 40, 49, 39, 47, 12, 48, 38, 49, 47, 40, 56, 54, 47, 39, 38, 49, 38, 37, 38, 42, 47, 38, 39, 46, 48, 52, 51, 43, 38, 49, 39, 56, 39, 32, 44, 52, 37, 49, 42, 50, 52, 39, 52, 50, 43, 40, 38, 48, 51, 40, 47, 38, 47, 50, 38, 50, 46, 47, 49, 52, 47, 37, 42, 47, 43, 42, 53, 42, 47, 47, 48, 42, 47, 40, 47, 40, 48, 47, 56, 42, 51, 48, 39, 38, 55, 38, 51, 50, 40, 50, 41, 39, 45, 40, 16, 40, 50, 43, 39, 46, 52, 38, 38, 47, 41, 37, 39, 42, 39, 46, 16, 38, 51, 43, 48, 39, 39, 40, 41, 51, 44, 52, 52, 42, 41, 38, 52, 51, 42, 24, 40, 37, 51, 40, 42, 42, 39, 42, 46, 39, 37, 46, 50, 49, 37, 40, 49, 39, 39, 41, 41, 56, 42, 42, 40, 42, 38, 48, 12, 28, 56]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#theres some values that have ? so we should remove those rows in 'masked_sequence_A', 'masked_sequence_B'\n",
        "# Remove rows where 'masked_sequence_A' or 'masked_sequence_B' contains '?' anywhere in the sequence\n",
        "cleaned_df = pairs_df[~(pairs_df['masked_sequence_A'].str.contains('\\?', na=False) | pairs_df['masked_sequence_B'].str.contains('\\?', na=False))]\n",
        "\n",
        "# Assign the cleaned DataFrame back to pairs_df\n",
        "pairs_df = cleaned_df\n",
        "\n",
        "# Printing the sizes of the original and cleaned dataframes to verify the number of rows removed\n",
        "print(f\"Original DataFrame size: {len(pairs_df)}\")\n",
        "print(f\"Cleaned DataFrame size: {len(cleaned_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AWdcKavZwxf",
        "outputId": "2920e2e8-112e-435c-aecf-5c3d557616aa"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame size: 9\n",
            "Cleaned DataFrame size: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMXbw_ebK8lb",
        "outputId": "d4967a08-8bba-4de7-e790-cf815ed7e1fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-e2e2eece8b35>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  pairs_df[col] = pairs_df[col].apply(lambda x: x[:500] if len(x) > 500 else x)\n",
            "<ipython-input-27-e2e2eece8b35>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  pairs_df[col] = pairs_df[col].apply(lambda x: x[:500] if len(x) > 500 else x)\n",
            "<ipython-input-27-e2e2eece8b35>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  pairs_df[col] = pairs_df[col].apply(lambda x: x[:500] if len(x) > 500 else x)\n",
            "<ipython-input-27-e2e2eece8b35>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  pairs_df[col] = pairs_df[col].apply(lambda x: x[:500] if len(x) > 500 else x)\n",
            "<ipython-input-27-e2e2eece8b35>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  pairs_df['Length'] = pairs_df['masked_sequence_A'].apply(len)\n"
          ]
        }
      ],
      "source": [
        "# Truncate each specified column to a maximum length of 500 characters\n",
        "columns = ['masked_sequence_A', 'masked_sequence_B', 'Sequence_A', 'Sequence_B']\n",
        "for col in columns:\n",
        "    pairs_df[col] = pairs_df[col].apply(lambda x: x[:500] if len(x) > 500 else x)\n",
        "\n",
        "# Find the longest string by length\n",
        "pairs_df['Length'] = pairs_df['masked_sequence_A'].apply(len)\n",
        "longest_string = pairs_df.loc[pairs_df['Length'].idxmax(), 'masked_sequence_A']\n",
        "print(len(longest_string))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#the number of protein chains we have\n",
        "pairs_df.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZMlodbXmDye",
        "outputId": "7af47813-04a6-44a7-bf42-45085f686645"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r0l1YXCvOFH"
      },
      "source": [
        "Create dataset class that handles both global sequences and local sequences for protein pairs, and potentially prepares for the inclusion of 3D structural data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32Lx8aF-7PEj"
      },
      "source": [
        "#the base Model (without coordinates at this point):\n",
        "\n",
        "  ### Modeling Interactions:\n",
        "  The mdel could be trained to recognize which amino acids interact by learning representations of local sequences that highlight these interactions. During training, the MLM objective helps the model learn contextual embeddings that are rich in information about which amino acids tend to be near each other and under what structural contexts they interact.\n",
        "\n",
        "  ### Attention Mechanism:\n",
        "   The custom attention mechanism can be used to weigh the importance of different amino acids in the global context when predicting the masked amino acids in the local sequences. This allows your model to focus more on the parts of the global sequences that are relevant to the interactions highlighted by the local sequences.\n",
        "\n",
        "  ### Utilizing Global Sequences:\n",
        "  While the local sequences are your primary interest, the global sequences provide the context necessary for your model to understand the broader environment in which the interactions occur. Even during prediction, you should feed the model the global sequences to utilize the learned context.\n",
        "\n",
        "  #### This modular design not only meets your current requirements but also provides a scalable framework to incorporate additional dimensions of protein sequence data analysis in the future\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/Checkpoints"
      ],
      "metadata": {
        "id": "1lLTXwI4wUKU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3c84216-baba-492e-8379-1a9bb538a831"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "added_tokens.json     generation_config.json   tokenizer_config.json\n",
            "config.json\t      model.safetensors        tokenizer.json\n",
            "final_checkpoint.pth  special_tokens_map.json  vocab.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#in this block of code we will intialize our core model for the architecture\n",
        "import torch\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# Define the checkpoint directory and the specific checkpoint file\n",
        "checkpoint_path = '/content/drive/MyDrive/Checkpoints/final_checkpoint.pth'\n",
        "\n",
        "# Load the tokenizer first\n",
        "tokenizer = AutoTokenizer.from_pretrained('/content/drive/MyDrive/Checkpoints')\n",
        "\n",
        "# Initialize the model from the pre-trained configuration in the Checkpoints directory\n",
        "model = AutoModelForMaskedLM.from_pretrained('/content/drive/MyDrive/Checkpoints')\n",
        "model.resize_token_embeddings(len(tokenizer))  # Important if you've added tokens\n",
        "\n",
        "# Determine the device to use (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)  # Move model to the appropriate device\n",
        "\n",
        "# Load the checkpoint\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)  # Ensure checkpoint is loaded to the correct device\n",
        "\n",
        "# Ensure all keys in the checkpoint can be loaded to the model\n",
        "missing_keys, unexpected_keys = model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
        "if missing_keys or unexpected_keys:\n",
        "    print(f\"Missing keys in state dict: {missing_keys}\")\n",
        "    print(f\"Unexpected keys in state dict: {unexpected_keys}\")\n",
        "else:\n",
        "    print(\"Model state loaded successfully.\")\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "An Optimizer and a Learning rate scheduler are being set up and their states are loaded from a checkpoint.\n",
        "This allows for the continuation of model training with the exact parameters and learning rate adjustments that were in use when the\n",
        "training was last saved, ensuring a seamless transition and consistency in the training process.\n",
        "\"\"\"\n",
        "\n",
        "# If you need the optimizer and scheduler states\n",
        "optimizer = Adam(model.parameters(), lr=checkpoint.get('learning_rate', 0.001))  # Fallback to default if not in checkpoint\n",
        "\n",
        "# Check if the optimizer state exists in the checkpoint before loading\n",
        "if 'optimizer_state_dict' in checkpoint:\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "else:\n",
        "    print(\"No optimizer state found in checkpoint; starting with a fresh optimizer.\")\n",
        "\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
        "if 'scheduler_state_dict' in checkpoint:\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "else:\n",
        "    print(\"No scheduler state found in checkpoint; using default settings.\")\n",
        "\n",
        "print(\"Model, tokenizer, optimizer, and scheduler loaded from checkpoint.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rs_CVu1caUC",
        "outputId": "c3e9cc8d-2d03-45c9-aa6d-bef6eb352f94"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model state loaded successfully.\n",
            "Model, tokenizer, optimizer, and scheduler loaded from checkpoint.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#the error is in this Dataset class. -> FIXED"
      ],
      "metadata": {
        "id": "H-KR-yTD6zId"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "ApUN1McupeKe"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "class ProteinInteractionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    This block is to manage multiple types of data inputs (modes) such as global sequence data, local sequence data,\n",
        "    and coordinates data. This class is built on PyTorch's Dataset class, making it compatible with\n",
        "    PyTorch's DataLoader for efficient data handling and batching during model training.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, mask_probability=0.15, modes=None):\n",
        "        \"\"\"\n",
        "        Initializes the ProteinInteractionDataset.\n",
        "\n",
        "        Args:\n",
        "            dataframe (pd.DataFrame): A DataFrame containing the protein interaction data.\n",
        "            tokenizer (Tokenizer): A tokenizer instance used to convert text sequences into token IDs.\n",
        "            mask_probability (float): The probability of masking a token, used in some modes for data augmentation.\n",
        "            modes (list, optional): A list of modes (types of data) the dataset should prepare. Defaults to\n",
        "                                    ['global', 'local', 'coords'] if not specified.\n",
        "        \"\"\"\n",
        "        self.dataframe = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.mask_probability = mask_probability\n",
        "        self.modes = modes if modes is not None else ['global', 'local', 'coords']\n",
        "        self.debug = True  # Can be set to False to turn off debugging outputs during data loading.\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the number of items in the dataset.\"\"\"\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieves an item from the dataset at the specified index.\n",
        "\n",
        "        Args:\n",
        "            idx (int): The index of the item.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing processed features for each mode.\n",
        "        \"\"\"\n",
        "        row = self.dataframe.iloc[idx]\n",
        "        data = {}\n",
        "\n",
        "        for mode in self.modes:\n",
        "            sequence = self.get_sequence(row, mode)\n",
        "            if sequence:\n",
        "                input_ids, attention_mask = self.tokenize_sequence(sequence)\n",
        "                data[f'input_ids_{mode}'] = input_ids\n",
        "                data[f'attention_mask_{mode}'] = attention_mask\n",
        "\n",
        "                if self.debug:\n",
        "                    print(f\"{mode} mode: input_ids dimension {input_ids.shape}, attention mask dimension {attention_mask.shape}\")\n",
        "                    print(f\"{mode} mode: input_ids {input_ids}, attention mask {attention_mask}\")\n",
        "\n",
        "        return data\n",
        "\n",
        "    def get_sequence(self, row, mode):\n",
        "        \"\"\"\n",
        "        Formats a sequence based on the given mode by adding special tokens and structure to the sequence.\n",
        "\n",
        "        Args:\n",
        "            row (pd.Series): A row from the DataFrame.\n",
        "            mode (str): The mode specifying which sequence type to format.\n",
        "\n",
        "        Returns:\n",
        "            str: The formatted sequence or None if the mode is unrecognized.\n",
        "        \"\"\"\n",
        "        if mode == 'global':\n",
        "            return f\"[CLS] {row['Sequence_A']} [ENTITY1] [SEP] {row['Sequence_B']} [ENTITY2] [SEP]\"\n",
        "        elif mode == 'local':\n",
        "            return f\"[CLS] {row['masked_sequence_A']} [ENTITY1] [SEP] {row['masked_sequence_B']} [ENTITY2] [SEP]\"\n",
        "        elif mode == 'coords':\n",
        "            return f\"[CLS] {row['coords_A']} [ENTITY1] [SEP] {row['coords_B']} [ENTITY2] [SEP]\"\n",
        "        return None\n",
        "\n",
        "    def tokenize_sequence(self, sequence):\n",
        "        \"\"\"\n",
        "        Converts a text sequence into token IDs and an attention mask using the provided tokenizer.\n",
        "\n",
        "        Args:\n",
        "            sequence (str): The text sequence to tokenize.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing a tensor of input IDs and a tensor of attention mask.\n",
        "        \"\"\"\n",
        "        encoded = self.tokenizer.encode_plus(\n",
        "            sequence,\n",
        "            add_special_tokens=True,  # Adds special tokens like [CLS] and [SEP]\n",
        "            return_tensors='pt',  # Return PyTorch tensors\n",
        "            padding='max_length',  # Pad to a maximum length specified by `max_length`\n",
        "            truncation=True,  # Truncate to a maximum length\n",
        "            max_length=1024  # Maximum length of the sequence\n",
        "        )\n",
        "        return encoded['input_ids'].squeeze(0), encoded['attention_mask'].squeeze(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame from the first row just for debug\n",
        "debug_df = pairs_df.iloc[[0]]"
      ],
      "metadata": {
        "id": "KtpgJFzA5LPo"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "debug_df"
      ],
      "metadata": {
        "id": "vObowD7b5MyM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "8226231e-2d41-42b2-c473-3ac3015d546a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  pair_id Protein Name A Protein Name B  \\\n",
              "0    1H0J         1H0J_B         1H0J_C   \n",
              "\n",
              "                                   masked_sequence_A  \\\n",
              "0  ---------------------------A------------------...   \n",
              "\n",
              "                                   masked_sequence_B  \\\n",
              "0  ----------------------------T-----------------...   \n",
              "\n",
              "                                            coords_A  \\\n",
              "0  [(15.18, 27.908, -3.171), (13.094, 28.367, -0....   \n",
              "\n",
              "                                            coords_B  \\\n",
              "0  [(31.71, 11.761, 52.412), (31.514, 9.193, 49.6...   \n",
              "\n",
              "                                        Embeddings_A  \\\n",
              "0  tensor([ 0.1932, -0.0438,  0.7770,  0.2603, -0...   \n",
              "\n",
              "                                          Sequence_A  \\\n",
              "0  LKCNKLVPLFYKTCPAGKNLCYKMFMVATPKVPVKRGCIDVCPKSS...   \n",
              "\n",
              "                                        Embeddings_B  \\\n",
              "0  tensor([-1.9606, -0.0789, -1.0516,  1.0233, -1...   \n",
              "\n",
              "                                          Sequence_B  \\\n",
              "0  LKCNKLVPLFYKTCPAGKNLCYKMFMVATPKVPVKRGCIDVCPKSS...   \n",
              "\n",
              "                                tokenized_sequence_A  \\\n",
              "0  [5, 12, 23, 17, 12, 5, 8, 16, 5, 19, 20, 12, 1...   \n",
              "\n",
              "                                tokenized_sequence_B  \\\n",
              "0  [5, 12, 23, 17, 12, 5, 8, 16, 5, 19, 20, 12, 1...   \n",
              "\n",
              "                         tokenized_masked_sequence_A  \\\n",
              "0  [32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...   \n",
              "\n",
              "                         tokenized_masked_sequence_B  \\\n",
              "0  [32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...   \n",
              "\n",
              "                            sum_tokenized_sequence_A  \\\n",
              "0  [37, 44, 55, 49, 44, 37, 40, 48, 37, 51, 52, 4...   \n",
              "\n",
              "                            sum_tokenized_sequence_B  Length  \n",
              "0  [37, 44, 55, 49, 44, 37, 40, 48, 37, 51, 52, 4...      60  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-95512069-acc7-4f0b-939b-3a8a20f2910d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pair_id</th>\n",
              "      <th>Protein Name A</th>\n",
              "      <th>Protein Name B</th>\n",
              "      <th>masked_sequence_A</th>\n",
              "      <th>masked_sequence_B</th>\n",
              "      <th>coords_A</th>\n",
              "      <th>coords_B</th>\n",
              "      <th>Embeddings_A</th>\n",
              "      <th>Sequence_A</th>\n",
              "      <th>Embeddings_B</th>\n",
              "      <th>Sequence_B</th>\n",
              "      <th>tokenized_sequence_A</th>\n",
              "      <th>tokenized_sequence_B</th>\n",
              "      <th>tokenized_masked_sequence_A</th>\n",
              "      <th>tokenized_masked_sequence_B</th>\n",
              "      <th>sum_tokenized_sequence_A</th>\n",
              "      <th>sum_tokenized_sequence_B</th>\n",
              "      <th>Length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1H0J</td>\n",
              "      <td>1H0J_B</td>\n",
              "      <td>1H0J_C</td>\n",
              "      <td>---------------------------A------------------...</td>\n",
              "      <td>----------------------------T-----------------...</td>\n",
              "      <td>[(15.18, 27.908, -3.171), (13.094, 28.367, -0....</td>\n",
              "      <td>[(31.71, 11.761, 52.412), (31.514, 9.193, 49.6...</td>\n",
              "      <td>tensor([ 0.1932, -0.0438,  0.7770,  0.2603, -0...</td>\n",
              "      <td>LKCNKLVPLFYKTCPAGKNLCYKMFMVATPKVPVKRGCIDVCPKSS...</td>\n",
              "      <td>tensor([-1.9606, -0.0789, -1.0516,  1.0233, -1...</td>\n",
              "      <td>LKCNKLVPLFYKTCPAGKNLCYKMFMVATPKVPVKRGCIDVCPKSS...</td>\n",
              "      <td>[5, 12, 23, 17, 12, 5, 8, 16, 5, 19, 20, 12, 1...</td>\n",
              "      <td>[5, 12, 23, 17, 12, 5, 8, 16, 5, 19, 20, 12, 1...</td>\n",
              "      <td>[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...</td>\n",
              "      <td>[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...</td>\n",
              "      <td>[37, 44, 55, 49, 44, 37, 40, 48, 37, 51, 52, 4...</td>\n",
              "      <td>[37, 44, 55, 49, 44, 37, 40, 48, 37, 51, 52, 4...</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-95512069-acc7-4f0b-939b-3a8a20f2910d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-95512069-acc7-4f0b-939b-3a8a20f2910d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-95512069-acc7-4f0b-939b-3a8a20f2910d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_f33e7623-e129-432a-8b45-4c00ac5996f7\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('debug_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f33e7623-e129-432a-8b45-4c00ac5996f7 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('debug_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "debug_df",
              "summary": "{\n  \"name\": \"debug_df\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"pair_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"1H0J\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Protein Name A\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"1H0J_B\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Protein Name B\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"1H0J_C\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"masked_sequence_A\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"---------------------------A--------------------------------\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"masked_sequence_B\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"----------------------------T-------------------------------\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"coords_A\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"[(15.18, 27.908, -3.171), (13.094, 28.367, -0.025), (14.194, 27.287, 3.457), (12.376, 27.246, 6.78), (13.433, 29.565, 9.596), (13.855, 28.657, 13.272), (10.063, 28.507, 13.604), (9.22, 26.247, 10.59), (5.886, 28.037, 10.185), (7.91, 30.686, 8.37), (10.378, 30.565, 5.496), (12.587, 32.72, 3.312), (13.688, 32.767, -0.313), (17.42, 32.129, -0.37), (19.611, 34.968, -1.697), (22.141, 34.483, -4.495), (24.969, 32.155, -3.525), (22.935, 30.08, -1.064), (21.355, 26.982, -2.619), (21.126, 24.751, 0.439), (18.858, 24.501, 3.494), (20.056, 23.276, 6.853), (18.983, 22.483, 10.395), (21.401, 22.584, 13.332), (20.945, 20.18, 16.248), (22.591, 20.201, 19.69), (23.191, 16.561, 20.751), (22.52, 17.342, 24.43), (19.176, 19.036, 23.636), (18.214, 17.435, 20.249), (14.432, 17.709, 20.578), (14.362, 20.494, 17.989), (16.898, 22.328, 15.824), (18.34, 25.483, 17.385), (19.029, 27.114, 14.031), (17.637, 26.781, 10.494), (17.993, 28.647, 7.228), (19.683, 29.036, 3.859), (23.419, 28.55, 3.401), (26.071, 27.909, 0.717), (27.966, 24.951, 2.152), (27.015, 22.249, 4.62), (28.735, 22.95, 7.965), (30.877, 20.111, 9.276), (29.133, 18.142, 12.029), (30.991, 17.409, 15.266), (30.651, 15.387, 18.464), (27.983, 17.624, 20.035), (26.63, 19.551, 17.03), (24.85, 17.861, 14.11), (23.835, 19.4, 10.796), (21.174, 18.167, 8.375), (21.516, 19.645, 4.882), (19.112, 19.429, 1.93), (18.293, 21.434, -1.206), (14.524, 21.148, -1.678), (11.945, 23.848, -0.918), (11.402, 23.89, 2.848), (13.097, 20.472, 3.271), (14.564, 21.644, 6.587)]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"coords_B\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"[(31.71, 11.761, 52.412), (31.514, 9.193, 49.625), (29.806, 9.814, 46.288), (28.997, 7.649, 43.263), (30.723, 8.167, 39.909), (29.557, 8.082, 36.274), (28.425, 4.482, 36.793), (26.436, 3.921, 40.053), (28.42, 0.731, 40.678), (31.502, 2.819, 41.473), (32.137, 5.564, 44.037), (34.895, 7.446, 45.873), (35.64, 9.566, 48.946), (35.353, 13.329, 48.486), (38.41, 15.548, 49.044), (38.537, 18.376, 51.577), (36.422, 21.198, 50.205), (33.629, 19.115, 48.705), (30.888, 18.063, 51.091), (28.204, 17.303, 48.511), (27.334, 14.519, 46.096), (25.61, 15.561, 42.889), (23.872, 14.274, 39.786), (23.511, 16.307, 36.594), (20.58, 15.592, 34.257), (19.775, 16.859, 30.773), (15.989, 17.066, 30.307), (16.381, 15.039, 27.09), (17.798, 12.164, 29.134), (15.804, 12.462, 32.424), (16.547, 8.893, 33.501), (20.343, 8.881, 33.205), (22.538, 11.647, 34.655), (25.367, 12.788, 32.387), (27.643, 13.656, 35.304), (28.097, 12.398, 38.885), (30.557, 12.982, 41.728), (31.618, 15.351, 44.523), (31.234, 19.104, 44.436), (31.608, 22.247, 46.545), (28.841, 24.57, 45.389), (25.592, 22.91, 44.312), (24.92, 24.034, 40.701), (21.77, 25.977, 39.816), (19.026, 24.022, 38.035), (17.55, 25.37, 34.813), (14.705, 24.605, 32.433), (16.88, 22.113, 30.526), (19.556, 21.117, 33.043), (18.644, 19.609, 36.411), (20.921, 19.089, 39.409), (20.406, 16.906, 42.472), (22.637, 17.605, 45.479), (22.719, 15.634, 48.717), (25.172, 15.126, 51.589), (24.92, 11.422, 52.467), (27.049, 8.395, 51.55), (26.396, 7.322, 47.956), (23.204, 9.397, 47.911), (23.814, 10.282, 44.261)]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Embeddings_A\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"tensor([ 0.1932, -0.0438,  0.7770,  0.2603, -0.3194, -1.7208, -1.4078, -0.6996,\\n         0.0189, -0.1269, -0.1114,  0.0289, -0.0243, -0.4486,  0.3975, -0.9184,\\n        -0.5971, -0.6794,  0.5902, -0.6271,  0.7077, -1.0189, -0.6838,  0.1389,\\n         0.5883,  0.3247, -0.3489,  0.8280, -0.3439,  0.5802, -0.7491, -0.2291,\\n        -0.5009, -0.7292, -0.1835,  0.5171,  1.4258,  1.8320,  0.5417,  0.8492,\\n        -0.8175,  2.7374,  1.0742,  0.1407,  0.4130,  0.5768,  0.7884,  0.2863,\\n        -1.2062, -0.0364,  1.6911, -0.0357,  0.2333, -0.4863, -0.4992,  0.1613,\\n        -0.0985, -0.6607, -1.8245,  0.5379, -0.9565,  1.0563, -1.4902,  1.6625,\\n         1.3829,  1.7928,  1.1537,  0.2394, -0.4484,  1.8955, -0.8740, -0.8465,\\n         0.3923, -1.5792,  0.4460,  0.4548, -2.0304,  1.0429,  0.7816,  0.6989,\\n        -0.1515,  0.1173,  0.4065,  0.6353, -0.4137,  1.4591,  1.0226, -0.9343,\\n         0.8768, -0.5274, -0.7074, -0.4752,  0.9429, -0.6944,  1.4983, -0.7199,\\n         0.1939,  0.4820, -0.2272,  0.7446, -0.7093, -1.4029, -1.5726,  1.4787,\\n         0.9897, -0.1934,  2.2563, -2.1228, -1.4990,  0.4615,  0.7031, -1.9541,\\n         0.0641,  0.5342,  1.0368,  1.1606,  1.1048,  0.6281,  0.7778, -1.0983,\\n        -1.7432,  0.2546,  0.9125, -0.3288,  1.5194, -1.6024, -1.2193, -0.0449])\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sequence_A\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"LKCNKLVPLFYKTCPAGKNLCYKMFMVATPKVPVKRGCIDVCPKSSLLVKYVCCNTDRCN\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Embeddings_B\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"tensor([-1.9606, -0.0789, -1.0516,  1.0233, -1.9376,  0.0048,  1.4564, -0.2826,\\n         0.9928, -0.7321,  0.2605, -1.1617, -1.9045, -0.5094,  1.6443,  0.8993,\\n        -0.5728, -0.4137,  1.3776,  0.3589, -0.1369, -0.1433,  0.2082, -2.1259,\\n         0.5695, -0.1116,  0.1403, -1.2130,  0.0179, -1.8902, -1.0072,  0.2011,\\n        -1.3431, -0.0556,  1.9390, -1.0001, -0.0221, -0.4739, -0.3450,  1.5603,\\n        -0.9024, -2.3155, -1.3712, -0.6494,  0.0187,  0.7018,  1.1420, -0.2633,\\n         0.5399, -0.8688, -0.2663,  0.4806,  0.4817, -1.5876,  1.9451,  0.4453,\\n         0.2921, -0.0622, -1.0344,  0.0873,  0.5145, -1.0429,  0.9856,  1.5482,\\n        -1.6083, -1.8402, -3.4806,  1.2436, -0.7513,  1.2344,  0.0140, -0.5014,\\n        -0.5876, -0.6243,  1.6610, -1.1263, -2.0587, -1.0505,  0.3005,  0.4084,\\n         0.1140, -1.8573,  1.1576, -1.1603,  0.6983, -0.0543, -0.1374,  1.4700,\\n        -2.2551,  1.9024, -0.4222, -1.7369,  1.1721, -0.7713, -1.2732,  0.1466,\\n        -0.1775,  0.2976,  1.2071,  0.9345,  0.6621,  1.1680, -1.3691, -1.1835,\\n        -0.9677,  0.2014,  1.4170, -1.3794, -0.0754,  1.3146, -0.8791, -0.5325,\\n        -1.9114,  2.4167, -0.9692, -1.5768,  0.3330,  0.5996, -0.4446, -1.9533,\\n        -0.4040,  0.5546,  0.2771,  0.8048,  1.1107,  1.0745,  0.2420,  0.7703])\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sequence_B\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"LKCNKLVPLFYKTCPAGKNLCYKMFMVATPKVPVKRGCIDVCPKSSLLVKYVCCNTDRCN\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokenized_sequence_A\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"[5, 12, 23, 17, 12, 5, 8, 16, 5, 19, 20, 12, 15, 23, 16, 6, 7, 12, 17, 5, 23, 20, 12, 21, 19, 21, 8, 6, 15, 16, 12, 8, 16, 8, 12, 13, 7, 23, 11, 14, 8, 23, 16, 12, 10, 10, 5, 5, 8, 12, 20, 8, 23, 23, 17, 15, 14, 13, 23, 17]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokenized_sequence_B\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"[5, 12, 23, 17, 12, 5, 8, 16, 5, 19, 20, 12, 15, 23, 16, 6, 7, 12, 17, 5, 23, 20, 12, 21, 19, 21, 8, 6, 15, 16, 12, 8, 16, 8, 12, 13, 7, 23, 11, 14, 8, 23, 16, 12, 10, 10, 5, 5, 8, 12, 20, 8, 23, 23, 17, 15, 14, 13, 23, 17]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokenized_masked_sequence_A\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 6, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokenized_masked_sequence_B\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 15, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sum_tokenized_sequence_A\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"[37, 44, 55, 49, 44, 37, 40, 48, 37, 51, 52, 44, 47, 55, 48, 38, 39, 44, 49, 37, 55, 52, 44, 53, 51, 53, 40, 12, 47, 48, 44, 40, 48, 40, 44, 45, 39, 55, 43, 46, 40, 55, 48, 44, 42, 42, 37, 37, 40, 44, 52, 40, 55, 55, 49, 47, 46, 45, 55, 49]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sum_tokenized_sequence_B\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"[37, 44, 55, 49, 44, 37, 40, 48, 37, 51, 52, 44, 47, 55, 48, 38, 39, 44, 49, 37, 55, 52, 44, 53, 51, 53, 40, 38, 30, 48, 44, 40, 48, 40, 44, 45, 39, 55, 43, 46, 40, 55, 48, 44, 42, 42, 37, 37, 40, 44, 52, 40, 55, 55, 49, 47, 46, 45, 55, 49]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 60,\n        \"max\": 60,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          60\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "'''\n",
        "# Initialize the ProtBERT tokenizer -> we dont need it since our tokenizer works perfectly. if we dont have access to the checkpoint we can use this:\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('Rostlab/prot_bert')\n",
        "'''\n",
        "\n",
        "# Print the length of each sequence column, considering each element as a string\n",
        "print(f\"Sequence_A length: {sum(len(seq) for seq in debug_df['Sequence_A'])}\")\n",
        "print(f\"Sequence_B length: {sum(len(seq) for seq in debug_df['Sequence_B'])}\")\n",
        "print(f\"Masked_sequence_A length: {sum(len(seq) for seq in debug_df['masked_sequence_A'])}\")\n",
        "print(f\"Masked_sequence_B length: {sum(len(seq) for seq in debug_df['masked_sequence_B'])}\")\n",
        "\n",
        "# Sample global sequence preparation\n",
        "global_seq = \"[CLS] \" + \" [ENTITY1] \".join(debug_df['Sequence_A']) + \" [SEP] \" + \" [ENTITY2] \".join(debug_df['Sequence_B']) + \" [SEP]\"\n",
        "local_seq = \"[CLS] \" + \" [ENTITY1] \".join(debug_df['masked_sequence_A']) + \" [SEP] \" + \" [ENTITY2] \".join(debug_df['masked_sequence_B']) + \" [SEP]\"\n",
        "coords_seq = \"[CLS] \" + \" [ENTITY1] \".join(debug_df['coords_A']) + \" [SEP] \" + \" [ENTITY2] \".join(debug_df['coords_B']) + \" [SEP]\"\n",
        "\n",
        "# Calculate the total length of the sequences\n",
        "global_seq_length = len(global_seq)\n",
        "local_seq_length = len(local_seq)\n",
        "coords_seq_length = len(coords_seq)\n",
        "\n",
        "# Print the length\n",
        "print(f\"Global sequence length: {global_seq_length}\")\n",
        "print(f\"Local sequence length: {local_seq_length}\")\n",
        "print(f\"coords sequence length: {coords_seq_length}\")\n",
        "\n",
        "# Assuming ProteinInteractionDataset is properly defined\n",
        "dataset_global = ProteinInteractionDataset(debug_df, tokenizer, modes=['global'])\n",
        "dataset_local = ProteinInteractionDataset(debug_df, tokenizer, modes=['local'])\n",
        "dataset_coords = ProteinInteractionDataset(debug_df, tokenizer, modes=['coords'])\n",
        "\n",
        "# Fetch data for the first entry in each dataset\n",
        "data_global = dataset_global[0]\n",
        "data_local = dataset_local[0]\n",
        "data_coords = dataset_coords[0]\n",
        "\n",
        "# Print the data for debugging\n",
        "print(\"Global Mode Data:\")\n",
        "for key, value in data_global.items():\n",
        "    print(f\"{key}: {value.shape}\")\n",
        "\n",
        "print(\"\\nLocal Mode Data:\")\n",
        "for key, value in data_local.items():\n",
        "    print(f\"{key}: {value.shape}\")\n",
        "\n",
        "print(\"\\ncoords Mode Data:\")\n",
        "for key, value in data_coords.items():\n",
        "    print(f\"{key}: {value.shape}\")\n"
      ],
      "metadata": {
        "id": "F88JVYsZltdC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74c8456c-b820-4cbd-c8b6-3d55d8e7b1c0"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence_A length: 60\n",
            "Sequence_B length: 60\n",
            "Masked_sequence_A length: 60\n",
            "Masked_sequence_B length: 60\n",
            "Global sequence length: 139\n",
            "Local sequence length: 139\n",
            "coords sequence length: 3054\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "Global Mode Data:\n",
            "input_ids_global: torch.Size([1024])\n",
            "attention_mask_global: torch.Size([1024])\n",
            "\n",
            "Local Mode Data:\n",
            "input_ids_local: torch.Size([1024])\n",
            "attention_mask_local: torch.Size([1024])\n",
            "\n",
            "coords Mode Data:\n",
            "input_ids_coords: torch.Size([1024])\n",
            "attention_mask_coords: torch.Size([1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "uRffIAsuVZo6"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Initialize containers for batch data\n",
        "    global_input_ids = []\n",
        "    global_attention_masks = []\n",
        "    local_input_ids = []\n",
        "    local_attention_masks = []\n",
        "\n",
        "    # Collect data for each sample in the batch\n",
        "    for item in batch:\n",
        "        global_input_ids.append(item['input_ids_global'])\n",
        "        global_attention_masks.append(item['attention_mask_global'])\n",
        "        local_input_ids.append(item['input_ids_local'])\n",
        "        local_attention_masks.append(item['attention_mask_local'])\n",
        "\n",
        "    # Pad sequences so they all have the same length within the batch\n",
        "    global_input_ids = pad_sequence(global_input_ids, batch_first=True, padding_value=0)\n",
        "    global_attention_masks = pad_sequence(global_attention_masks, batch_first=True, padding_value=0)\n",
        "    local_input_ids = pad_sequence(local_input_ids, batch_first=True, padding_value=0)\n",
        "    local_attention_masks = pad_sequence(local_attention_masks, batch_first=True, padding_value=0)\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    return {\n",
        "        'global_input_ids': global_input_ids,\n",
        "        'global_attention_masks': global_attention_masks,\n",
        "        'local_input_ids': local_input_ids,\n",
        "        'local_attention_masks': local_attention_masks\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ7xnO7FseKP"
      },
      "source": [
        "#pretraining the bert model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSaaU5j6rHdO"
      },
      "source": [
        "#main class for training:\n",
        "  1. Process two sets of sequences (global and local) using BERT to extract contextual embeddings.\n",
        "  2. Integrate these two sets of embeddings using a custom attention mechanism that focuses on relevant parts of the global features for each part of the local features.\n",
        "  3. Predict an output (like interaction sites or effects) using the combined features."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#simple class for only 2 channel input of global and local sequneces"
      ],
      "metadata": {
        "id": "XpGLeEjQF5Iu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "M2qm1dRzz2pr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import AutoModel\n",
        "\n",
        "class SequenceProcessor(nn.Module):\n",
        "    \"\"\"A module to process sequences using a pre-trained Transformer model.\n",
        "    It extracts the last hidden states, which serve as features for further processing.\"\"\"\n",
        "    def __init__(self, model):\n",
        "        super(SequenceProcessor, self).__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        \"\"\"Perform the forward pass to get sequence features.\n",
        "        Args:\n",
        "            input_ids (torch.Tensor): Tensor of input token IDs.\n",
        "            attention_mask (torch.Tensor, optional): Tensor indicating which tokens should be attended to.\n",
        "        Returns:\n",
        "            torch.Tensor: Tensor of last hidden states representing features for each token.\"\"\"\n",
        "        return self.model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
        "\n",
        "class GuidedAttention(nn.Module):\n",
        "    \"\"\"A module that implements a guided attention mechanism to focus on specific parts of a sequence\n",
        "    based on additional local sequence context.\"\"\"\n",
        "    def __init__(self, hidden_size):\n",
        "        super(GuidedAttention, self).__init__()\n",
        "        self.key_layer = nn.Linear(hidden_size, hidden_size)\n",
        "        self.query_layer = nn.Linear(hidden_size, hidden_size)\n",
        "        self.value_layer = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.context_layer = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, global_features, local_features):\n",
        "        \"\"\"Calculate the attention-driven context vector.\n",
        "        Args:\n",
        "            global_features (torch.Tensor): Feature representations of the global sequence.\n",
        "            local_features (torch.Tensor): Feature representations of the local sequence used for queries.\n",
        "        Returns:\n",
        "            torch.Tensor: Processed context vector after applying guided attention.\"\"\"\n",
        "        keys = self.key_layer(global_features)\n",
        "        queries = self.query_layer(local_features)\n",
        "        values = self.value_layer(global_features)\n",
        "        attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / (keys.size(-1) ** 0.5)\n",
        "        attention_weights = self.softmax(attention_scores)\n",
        "        context = torch.matmul(attention_weights, values)\n",
        "        processed_context = self.context_layer(context)\n",
        "        return processed_context\n",
        "\n",
        "class ProteinInteractionModel(nn.Module):\n",
        "    \"\"\"Main model class for predicting protein interaction sites using global and local sequence data.\n",
        "    This model integrates sequence processing, guided attention, and classification.\"\"\"\n",
        "    def __init__(self, model_identifier_or_path, num_labels=21):\n",
        "        super(ProteinInteractionModel, self).__init__()\n",
        "        self.base_model = AutoModel.from_pretrained(model_identifier_or_path)\n",
        "        hidden_size = self.base_model.config.hidden_size\n",
        "\n",
        "        self.sequence_processor_global = SequenceProcessor(self.base_model)\n",
        "        self.sequence_processor_local = SequenceProcessor(self.base_model)\n",
        "        self.guided_attention = GuidedAttention(hidden_size)\n",
        "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids_global, attention_mask_global, input_ids_local, attention_mask_local):\n",
        "        \"\"\"Forward computation to predict labels for each sequence position.\n",
        "        Args:\n",
        "            input_ids_global (torch.Tensor): Input IDs for the global sequence.\n",
        "            attention_mask_global (torch.Tensor): Attention mask for the global sequence.\n",
        "            input_ids_local (torch.Tensor): Input IDs for the local sequence.\n",
        "            attention_mask_local (torch.Tensor): Attention mask for the local sequence.\n",
        "        Returns:\n",
        "            torch.Tensor: Logits for each sequence position indicating predicted label probabilities.\"\"\"\n",
        "        global_features = self.sequence_processor_global(input_ids_global, attention_mask_global)\n",
        "        local_features = self.sequence_processor_local(input_ids_local, attention_mask_local)\n",
        "        focused_features = self.guided_attention(global_features, local_features)\n",
        "        logits = self.classifier(focused_features)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#more complecated and flexible architecture for adding more channels of input"
      ],
      "metadata": {
        "id": "ofraCddXF-o-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''import torch\n",
        "from torch import nn\n",
        "from transformers import AutoModel\n",
        "\n",
        "class SequenceProcessor(nn.Module):\n",
        "    \"\"\"Processes sequences using a pre-trained Transformer model to extract features.\"\"\"\n",
        "    def __init__(self, model):\n",
        "        super(SequenceProcessor, self).__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        \"\"\"Extracts the last hidden states as features.\"\"\"\n",
        "        return self.model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
        "\n",
        "class GuidedAttention(nn.Module):\n",
        "    \"\"\"Implements attention to focus on relevant parts of a sequence using additional context from other channels.\"\"\"\n",
        "    def __init__(self, hidden_size):\n",
        "        super(GuidedAttention, self).__init__()\n",
        "        # Define layers for attention mechanism\n",
        "        self.key_layer = nn.Linear(hidden_size, hidden_size)\n",
        "        self.query_layer = nn.Linear(hidden_size, hidden_size)\n",
        "        self.value_layer = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.context_layer = nn.Linear(hidden_size * 2, hidden_size)  # Combines main and context features\n",
        "\n",
        "    def forward(self, main_features, additional_features):\n",
        "        \"\"\"Combines main sequence features with additional features through attention.\"\"\"\n",
        "        keys = self.key_layer(main_features)\n",
        "        queries = self.query_layer(additional_features)\n",
        "        values = self.value_layer(main_features)\n",
        "\n",
        "        # Compute attention scores and weights\n",
        "        attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / (keys.size(-1) ** 0.5)\n",
        "        attention_weights = self.softmax(attention_scores)\n",
        "\n",
        "        # Form context vector and combine it with main features\n",
        "        context = torch.matmul(attention_weights, values)\n",
        "        combined_features = torch.cat((main_features, context), dim=-1)\n",
        "        processed_context = self.context_layer(combined_features)\n",
        "        return processed_context\n",
        "\n",
        "class FlexibleProteinInteractionModel(nn.Module):\n",
        "    \"\"\"Predicts protein interaction sites using multiple sequence data channels with attention integration.\"\"\"\n",
        "    def __init__(self, model_identifier_or_path, num_labels):\n",
        "        super(FlexibleProteinInteractionModel, self).__init__()\n",
        "        self.base_model = AutoModel.from_pretrained(model_identifier_or_path)\n",
        "        hidden_size = self.base_model.config.hidden_size\n",
        "\n",
        "        # Processor for primary sequence features\n",
        "        self.global_sequence_processor = SequenceProcessor(self.base_model)\n",
        "        # Processor for secondary features (e.g., local sequences or coordinates)\n",
        "        self.additional_feature_processor = SequenceProcessor(self.base_model)\n",
        "        # Guided attention to integrate features from both processors\n",
        "        self.guided_attention = GuidedAttention(hidden_size)\n",
        "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids_global, attention_mask_global, input_ids_additional, attention_mask_additional):\n",
        "        \"\"\"Processes multiple channels of input data and predicts outcomes.\"\"\"\n",
        "        global_features = self.global_sequence_processor(input_ids_global, attention_mask_global)\n",
        "        additional_features = self.additional_feature_processor(input_ids_additional, attention_mask_additional)\n",
        "\n",
        "        # Integrate global and additional features using guided attention\n",
        "        integrated_features = self.guided_attention(global_features, additional_features)\n",
        "\n",
        "        # Classify based on integrated features\n",
        "        logits = self.classifier(integrated_features)\n",
        "        return logits\n",
        "'''"
      ],
      "metadata": {
        "id": "TXs0Krz-F-Vr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "2518c0f2-d00a-49e9-9cb9-8c4433c4d46f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import torch\\nfrom torch import nn\\nfrom transformers import AutoModel\\n\\nclass SequenceProcessor(nn.Module):\\n    \"\"\"Processes sequences using a pre-trained Transformer model to extract features.\"\"\"\\n    def __init__(self, model):\\n        super(SequenceProcessor, self).__init__()\\n        self.model = model\\n\\n    def forward(self, input_ids, attention_mask=None):\\n        \"\"\"Extracts the last hidden states as features.\"\"\"\\n        return self.model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\\n\\nclass GuidedAttention(nn.Module):\\n    \"\"\"Implements attention to focus on relevant parts of a sequence using additional context from other channels.\"\"\"\\n    def __init__(self, hidden_size):\\n        super(GuidedAttention, self).__init__()\\n        # Define layers for attention mechanism\\n        self.key_layer = nn.Linear(hidden_size, hidden_size)\\n        self.query_layer = nn.Linear(hidden_size, hidden_size)\\n        self.value_layer = nn.Linear(hidden_size, hidden_size)\\n        self.softmax = nn.Softmax(dim=-1)\\n        self.context_layer = nn.Linear(hidden_size * 2, hidden_size)  # Combines main and context features\\n\\n    def forward(self, main_features, additional_features):\\n        \"\"\"Combines main sequence features with additional features through attention.\"\"\"\\n        keys = self.key_layer(main_features)\\n        queries = self.query_layer(additional_features)\\n        values = self.value_layer(main_features)\\n\\n        # Compute attention scores and weights\\n        attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / (keys.size(-1) ** 0.5)\\n        attention_weights = self.softmax(attention_scores)\\n\\n        # Form context vector and combine it with main features\\n        context = torch.matmul(attention_weights, values)\\n        combined_features = torch.cat((main_features, context), dim=-1)\\n        processed_context = self.context_layer(combined_features)\\n        return processed_context\\n\\nclass FlexibleProteinInteractionModel(nn.Module):\\n    \"\"\"Predicts protein interaction sites using multiple sequence data channels with attention integration.\"\"\"\\n    def __init__(self, model_identifier_or_path, num_labels):\\n        super(FlexibleProteinInteractionModel, self).__init__()\\n        self.base_model = AutoModel.from_pretrained(model_identifier_or_path)\\n        hidden_size = self.base_model.config.hidden_size\\n\\n        # Processor for primary sequence features\\n        self.global_sequence_processor = SequenceProcessor(self.base_model)\\n        # Processor for secondary features (e.g., local sequences or coordinates)\\n        self.additional_feature_processor = SequenceProcessor(self.base_model)\\n        # Guided attention to integrate features from both processors\\n        self.guided_attention = GuidedAttention(hidden_size)\\n        self.classifier = nn.Linear(hidden_size, num_labels)\\n\\n    def forward(self, input_ids_global, attention_mask_global, input_ids_additional, attention_mask_additional):\\n        \"\"\"Processes multiple channels of input data and predicts outcomes.\"\"\"\\n        global_features = self.global_sequence_processor(input_ids_global, attention_mask_global)\\n        additional_features = self.additional_feature_processor(input_ids_additional, attention_mask_additional)\\n\\n        # Integrate global and additional features using guided attention\\n        integrated_features = self.guided_attention(global_features, additional_features)\\n\\n        # Classify based on integrated features\\n        logits = self.classifier(integrated_features)\\n        return logits\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiPlAiY8j7aY"
      },
      "source": [
        "#Training and Validation data creator:\n",
        "The DataLoader in PyTorch is a powerful utility that simplifies data handling when training neural networks. It's particularly useful when working with large datasets that cannot fit entirely into memory, or when you need sophisticated data loading and batching capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "3l4rd9Hej7Gj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9a6899a-1cfe-40f0-ad8e-6034a912295b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total dataset size: 9\n",
            "Training data size: 7\n",
            "Testing data size: 2\n",
            "Total dataset size: 9\n",
            "Training data size: 7\n",
            "Validation data size: 2\n",
            "Number of batches in train_loader: 1, Each batch has 16 samples.\n",
            "Number of batches in val_loader: 1, Each batch has 16 samples.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting the dataset into training and validation\n",
        "train_df, val_df = train_test_split(pairs_df, test_size=0.2, random_state=42)\n",
        "print(f\"Total dataset size: {len(pairs_df)}\")\n",
        "print(f\"Training data size: {len(train_df)}\")\n",
        "print(f\"Testing data size: {len(test_df)}\")\n",
        "\n",
        "\n",
        "# Informative print statements\n",
        "print(f\"Total dataset size: {len(pairs_df)}\")\n",
        "print(f\"Training data size: {len(train_df)}\")\n",
        "print(f\"Validation data size: {len(val_df)}\")\n",
        "\n",
        "# Setup DataLoaders for training and validation\n",
        "train_dataset = ProteinInteractionDataset(train_df, tokenizer)\n",
        "val_dataset = ProteinInteractionDataset(val_df, tokenizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Log the setup\n",
        "print(f\"Number of batches in train_loader: {len(train_loader)}, Each batch has {train_loader.batch_size} samples.\")\n",
        "print(f\"Number of batches in val_loader: {len(val_loader)}, Each batch has {val_loader.batch_size} samples.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouA_WiDqRUqr"
      },
      "source": [
        "# Model Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5RnrPjjRhip"
      },
      "source": [
        "#Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract some batches and check their max token IDs\n",
        "for batch in train_loader:  # Assuming loader is your DataLoader instance\n",
        "    max_id_global = batch['input_ids_global'].max()\n",
        "    max_id_local = batch['input_ids_local'].max()\n",
        "    max_id_coords = batch['input_ids_coords'].max()  # Assuming you have coords\n",
        "    print(\"Max ID Global:\", max_id_global.item())\n",
        "    print(\"Max ID Local:\", max_id_local.item())\n",
        "    print(\"Max ID Coords:\", max_id_coords.item())\n",
        "    break  # Remove break to check more batches\n",
        "\n",
        "# Check against the model's embedding layer size\n",
        "max_embedding_id = model.base_model.embeddings.word_embeddings.num_embeddings - 1\n",
        "print(\"Max embedding index allowed:\", max_embedding_id)\n",
        "\n",
        "# Adjusting the model embeddings to be slightly larger than the max token ID used\n",
        "required_embedding_size = max(max_id_global.item(), max_id_local.item(), max_id_coords.item()) + 1  # Plus one for safe measure\n",
        "if required_embedding_size > model.base_model.embeddings.word_embeddings.num_embeddings:\n",
        "    model.base_model.resize_token_embeddings(required_embedding_size)\n"
      ],
      "metadata": {
        "id": "exrXvVXqYrVt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b07ff04-59d2-4736-cd58-3850036612bc"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "Max ID Global: 31\n",
            "Max ID Local: 32\n",
            "Max ID Coords: 32\n",
            "Max embedding index allowed: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "l02vBOukRiqq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b599cfd-6d0e-423e-ac97-b50cce73b088"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "Epoch: 1, Loss: 10.7981\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "Epoch: 2, Loss: 10.7820\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "# Assume model, optimizer, scheduler, train_loader are already defined and configured.\n",
        "criterion = CrossEntropyLoss()\n",
        "num_epochs = 2  # You can adjust the number of epochs based on your training requirements\n",
        "scaler = GradScaler()  # Initialize the GradScaler for mixed precision training\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    total_loss = 0\n",
        "    batch_count = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Move data to the appropriate device (GPU or CPU)\n",
        "        input_ids_global = batch['input_ids_global'].to(device)\n",
        "        attention_mask_global = batch['attention_mask_global'].to(device)\n",
        "        labels = batch['input_ids_local'].to(device)  # Treat local sequences as labels\n",
        "        attention_mask_local = batch['attention_mask_local'].to(device)  # Might be used depending on model\n",
        "\n",
        "        # Use automatic mixed precision\n",
        "        with autocast():\n",
        "            # Perform a forward pass through the model using global sequences\n",
        "            output = model(input_ids_global, attention_mask=attention_mask_global)\n",
        "\n",
        "            # Extract logits from the model's output; this assumes that the output is an object with a logits attribute\n",
        "            logits = output.logits\n",
        "\n",
        "            # Compute loss using the logits from the model and the local sequences as labels\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "\n",
        "        # Scale the loss and call backward() to create scaled gradients\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # Perform a scaler step. This unscales gradients and calls or skips optimizer.step()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Accumulate the loss\n",
        "        total_loss += loss.item()\n",
        "        batch_count += 1\n",
        "\n",
        "    # Clear any unused memory to prevent CUDA out of memory errors\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Update the learning rate scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # Calculate average loss over all batches\n",
        "    average_loss = total_loss / batch_count\n",
        "    print(f'Epoch: {epoch+1}, Loss: {average_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your session crashed after using all available RAM"
      ],
      "metadata": {
        "id": "IkJ_CTKayCX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation phase\n",
        "model.eval()  # Set model to evaluation mode\n",
        "val_loss = 0\n",
        "val_batches = 0\n",
        "with torch.no_grad():  # No gradients needed for validation, which saves memory and computations\n",
        "        for val_batch in val_loader:\n",
        "            input_ids_global = val_batch['input_ids_global'].to(device)\n",
        "            attention_mask_global = val_batch['attention_mask_global'].to(device)\n",
        "            labels = val_batch['input_ids_local'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(input_ids_global, attention_mask=attention_mask_global)\n",
        "            logits = output.logits\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "            val_loss += loss.item()\n",
        "            val_batches += 1\n",
        "\n",
        "    # Calculate and print average validation loss\n",
        "average_val_loss = val_loss / val_batches\n",
        "print(f'Epoch: {epoch+1}, Validation Loss: {average_val_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfEvz5ebeIPz",
        "outputId": "016a6d6c-f1e4-4d6e-93dc-380aae852cd3"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "Epoch: 2, Validation Loss: 11.4164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#prediction:\n",
        "\n",
        "# Example global sequence\n",
        "#this is the path for the file\n",
        "csv_file = '/content/drive/MyDrive/pairs.csv'\n",
        "pairs_df = pd.read_csv(csv_file)\n",
        "prediction_input = pairs_df.iloc[9]  # 10th row, since indexing starts at 0\n",
        "\n"
      ],
      "metadata": {
        "id": "s9uEIkvBe9-v"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxC915iPgCvf",
        "outputId": "17127bcd-4a75-4b3f-8d6a-11bd3d905605"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pair_id                                                                     1H1L\n",
              "Protein Name A                                                            1H1L_C\n",
              "Protein Name B                                                            1H1L_D\n",
              "masked_sequence_A              TN------LA-------------------H-----------G----...\n",
              "masked_sequence_B              ----------------------------------------------...\n",
              "coords_A                       [(99.674, -54.557, 33.327), (95.992, -55.533, ...\n",
              "coords_B                       [(77.66, -28.403, 60.796), (75.28, -29.4, 57.9...\n",
              "Embeddings_A                   tensor([ 0.6414, -0.0384,  0.3825, -1.9274, -1...\n",
              "Sequence_A                     TNATGERNLALIQEVLEVFPETARKERRKHMMVSDPKMKSVGKCII...\n",
              "Embeddings_B                   tensor([ 0.4332,  1.7368, -0.3487,  0.7947, -1...\n",
              "Sequence_B                     SQTIDKINSCYPLFEQDEYQELFRNKRQLEEAHDAQRVQEVFAWTT...\n",
              "tokenized_sequence_A           [15, 17, 6, 15, 7, 9, 13, 17, 5, 6, 5, 11, 18,...\n",
              "tokenized_sequence_B           [10, 18, 15, 11, 14, 12, 11, 17, 10, 23, 20, 1...\n",
              "tokenized_masked_sequence_A    [15, 17, 32, 32, 32, 32, 32, 32, 5, 6, 32, 32,...\n",
              "tokenized_masked_sequence_B    [32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...\n",
              "sum_tokenized_sequence_A       [30, 34, 38, 47, 39, 41, 45, 49, 10, 12, 37, 4...\n",
              "sum_tokenized_sequence_B       [42, 50, 47, 43, 46, 44, 43, 49, 42, 55, 52, 4...\n",
              "Name: 9, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Ensure prediction_input is a DataFrame, not a Series:\n",
        "if isinstance(prediction_input, pd.Series):\n",
        "    prediction_input = prediction_input.to_frame().transpose()\n",
        "\n",
        "# Initialize dataset with the correct modes list\n",
        "prediction_dataset = ProteinInteractionDataset(prediction_input, tokenizer, modes=['global'])\n",
        "\n",
        "# DataLoader for handling batches\n",
        "prediction_loader = DataLoader(prediction_dataset, batch_size=10, shuffle=False)\n",
        "\n",
        "# Assuming 'your_model' is already loaded and set to evaluation mode\n",
        "for batch in prediction_loader:\n",
        "    input_ids = batch['input_ids_global'].to(device)  # Ensure this key matches what's generated in Dataset\n",
        "    attention_mask = batch['attention_mask_global'].to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(input_ids, attention_mask=attention_mask)\n",
        "        # Assuming the output needs further processing to extract results\n",
        "        # For example, you might need to access `output.logits` or similar, depending on the model\n",
        "        predictions = output.logits.argmax(dim=-1)  # Example to extract predicted token indices\n",
        "        predicted_sequence = tokenizer.decode(predictions[0])  # Decode tokens to sequence\n",
        "\n",
        "        print(f\"Predicted Sequence: {predicted_sequence}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqhWEH9AfsaO",
        "outputId": "a20f460b-feac-409e-94e4-edff9d1d92c8"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "Predicted Sequence: [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LtggzoYehToV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}