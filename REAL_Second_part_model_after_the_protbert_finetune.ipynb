{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kattens/Protein-Interaction-with-LLMs/blob/main/REAL_Second_part_model_after_the_protbert_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "id": "_KZ0eV_ywnM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3O1GOBJjjNNw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IS-wzrL6thZf",
        "outputId": "c0e438b2-283c-4efc-d117-6fe49c0aa225"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "CUDA not available\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CUDA not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0S1_IYhuzu6",
        "outputId": "d73254f3-107e-4834-d01e-8af4a5cf9fe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dD0ew8E8jXFJ"
      },
      "outputs": [],
      "source": [
        "csv_file = '/content/drive/MyDrive/Phase3Project/Pairs_df_new(1).csv'\n",
        "pairs_df = pd.read_csv(csv_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMXbw_ebK8lb",
        "outputId": "cdd41dcd-7623-4723-8ea3-e1efc502477d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500\n"
          ]
        }
      ],
      "source": [
        "# Truncate each specified column to a maximum length of 500 characters\n",
        "columns = ['masked_sequence_A', 'masked_sequence_B', 'Sequence_A', 'Sequence_B']\n",
        "for col in columns:\n",
        "    pairs_df[col] = pairs_df[col].apply(lambda x: x[:500] if len(x) > 500 else x)\n",
        "\n",
        "# Find the longest string by length\n",
        "pairs_df['Length'] = pairs_df['masked_sequence_A'].apply(len)\n",
        "longest_string = pairs_df.loc[pairs_df['Length'].idxmax(), 'masked_sequence_A']\n",
        "print(len(longest_string))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r0l1YXCvOFH"
      },
      "source": [
        "Create dataset class that handles both global sequences and local sequences for protein pairs, and potentially prepares for the inclusion of 3D structural data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32Lx8aF-7PEj"
      },
      "source": [
        "#the base Model (without coordinates at this point):\n",
        "\n",
        "  ### Modeling Interactions:\n",
        "  The mdel could be trained to recognize which amino acids interact by learning representations of local sequences that highlight these interactions. During training, the MLM objective helps the model learn contextual embeddings that are rich in information about which amino acids tend to be near each other and under what structural contexts they interact.\n",
        "\n",
        "  ### Attention Mechanism:\n",
        "   The custom attention mechanism can be used to weigh the importance of different amino acids in the global context when predicting the masked amino acids in the local sequences. This allows your model to focus more on the parts of the global sequences that are relevant to the interactions highlighted by the local sequences.\n",
        "\n",
        "  ### Utilizing Global Sequences:\n",
        "  While the local sequences are your primary interest, the global sequences provide the context necessary for your model to understand the broader environment in which the interactions occur. Even during prediction, you should feed the model the global sequences to utilize the learned context.\n",
        "\n",
        "  #### This modular design not only meets your current requirements but also provides a scalable framework to incorporate additional dimensions of protein sequence data analysis in the future\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/Checkpoints"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lLTXwI4wUKU",
        "outputId": "735af41b-9c39-4b3b-d357-c4c17457c2a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "added_tokens.json  generation_config.json  special_tokens_map.json  tokenizer.json\n",
            "config.json\t   model.safetensors\t   tokenizer_config.json    vocab.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#in this block of code we will intialize our core model for the architecture\n",
        "import torch\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# Define the checkpoint directory and the specific checkpoint file\n",
        "checkpoint_path = '/content/drive/MyDrive/Checkpoints/final_checkpoint.pth'\n",
        "\n",
        "# Load the tokenizer first\n",
        "tokenizer = AutoTokenizer.from_pretrained('/content/drive/MyDrive/Checkpoints')\n",
        "\n",
        "# Initialize the model from the pre-trained configuration in the Checkpoints directory\n",
        "model = AutoModelForMaskedLM.from_pretrained('/content/drive/MyDrive/Checkpoints')\n",
        "model.resize_token_embeddings(len(tokenizer))  # Important if you've added tokens\n",
        "\n",
        "# Determine the device to use (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)  # Move model to the appropriate device\n",
        "\n",
        "# Load the checkpoint\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)  # Ensure checkpoint is loaded to the correct device\n",
        "\n",
        "# Ensure all keys in the checkpoint can be loaded to the model\n",
        "missing_keys, unexpected_keys = model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
        "if missing_keys or unexpected_keys:\n",
        "    print(f\"Missing keys in state dict: {missing_keys}\")\n",
        "    print(f\"Unexpected keys in state dict: {unexpected_keys}\")\n",
        "else:\n",
        "    print(\"Model state loaded successfully.\")\n",
        "\n",
        "# If you need the optimizer and scheduler states\n",
        "optimizer = Adam(model.parameters(), lr=checkpoint.get('learning_rate', 0.001))  # Fallback to default if not in checkpoint\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
        "if 'scheduler_state_dict' in checkpoint:\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "else:\n",
        "    print(\"No scheduler state found in checkpoint; using default settings.\")\n",
        "\n",
        "print(\"Model, tokenizer, optimizer, and scheduler loaded from checkpoint.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "3rs_CVu1caUC",
        "outputId": "8d9be478-474f-44d1-9d24-cb97bf232f07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Checkpoints/final_checkpoint.pth'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-e8e5cd6462e0>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Load the checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Ensure checkpoint is loaded to the correct device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Ensure all keys in the checkpoint can be loaded to the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Checkpoints/final_checkpoint.pth'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApUN1McupeKe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "class ProteinInteractionDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length=512, mask_probability=0.15, modes=None):\n",
        "        \"\"\"\n",
        "        Initializes the dataset.\n",
        "        Args:\n",
        "            dataframe (pandas.DataFrame): The dataframe containing protein sequences.\n",
        "            tokenizer (transformers.BertTokenizer): The tokenizer for encoding sequences.\n",
        "            max_length (int): Maximum sequence length.\n",
        "            mask_probability (float): The probability of masking a token for the masked language model.\n",
        "            modes (list of str): List of modes to prepare data.\n",
        "        \"\"\"\n",
        "        self.dataframe = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.mask_probability = mask_probability\n",
        "        self.modes = modes if modes else ['global_masked']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataframe.iloc[idx]\n",
        "        data = {}\n",
        "\n",
        "        for mode in self.modes:\n",
        "            if mode == 'global_masked':\n",
        "                sequence = f\"[CLS] {row['Sequence_A']} [ENTITY1] [SEP] {row['Sequence_B']} [ENTITY2] [SEP]\"\n",
        "                input_ids, attention_mask, labels = self.random_mask_sequence(sequence)\n",
        "                key_suffix = 'global_masked'\n",
        "            elif mode == 'local':\n",
        "                sequence = f\"[CLS] {row['Sequence_A']} [ENTITY1] [SEP] {row['Sequence_B']} [ENTITY2] [SEP]\"\n",
        "                input_ids, attention_mask = self.tokenize_sequence(sequence)\n",
        "                labels = torch.full_like(input_ids, -100)\n",
        "                key_suffix = 'local'\n",
        "\n",
        "            data[f'input_ids_{key_suffix}'] = input_ids\n",
        "            data[f'attention_mask_{key_suffix}'] = attention_mask\n",
        "            data[f'labels_{key_suffix}'] = labels\n",
        "\n",
        "        return data\n",
        "\n",
        "    def tokenize_sequence(self, sequence):\n",
        "        encoded = self.tokenizer.encode_plus(\n",
        "            sequence,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return encoded['input_ids'].squeeze(0), encoded['attention_mask'].squeeze(0)\n",
        "\n",
        "    def random_mask_sequence(self, sequence):\n",
        "        encoded = self.tokenizer.encode_plus(\n",
        "            sequence,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        input_ids = encoded['input_ids'].squeeze(0)\n",
        "        labels = torch.full_like(input_ids, -100)\n",
        "\n",
        "        # Decide where to mask tokens\n",
        "        mask_indices = (torch.rand(input_ids.shape) < self.mask_probability) & (input_ids != self.tokenizer.pad_token_id)\n",
        "        labels[mask_indices] = input_ids[mask_indices]\n",
        "\n",
        "        # 80% time replace with [MASK]\n",
        "        actual_mask = mask_indices & (torch.rand(input_ids.shape) < 0.8)\n",
        "        input_ids[actual_mask] = self.tokenizer.mask_token_id\n",
        "\n",
        "        # 20% time replace with a random token\n",
        "        random_tokens = torch.randint(2, self.tokenizer.vocab_size, input_ids.shape)\n",
        "        input_ids[mask_indices & ~actual_mask] = random_tokens[mask_indices & ~actual_mask]\n",
        "\n",
        "        return input_ids, torch.ones_like(input_ids), labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRffIAsuVZo6"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Collate function to form a batch from multiple data samples.\n",
        "    Each sample contains tokenized inputs for global and local sequences,\n",
        "    along with corresponding attention masks and labels if they are part of the batch.\n",
        "    \"\"\"\n",
        "    input_ids_global, attention_mask_global, labels_global = [], [], []\n",
        "    input_ids_local, attention_mask_local = [], []\n",
        "\n",
        "    for item in batch:\n",
        "        if 'input_ids_global_masked' in item:\n",
        "            input_ids_global.append(item['input_ids_global_masked'])\n",
        "            attention_mask_global.append(item['attention_mask_global_masked'])\n",
        "            labels_global.append(item['labels_global_masked'])\n",
        "\n",
        "        if 'input_ids_local' in item:\n",
        "            input_ids_local.append(item['input_ids_local'])\n",
        "            attention_mask_local.append(item['attention_mask_local'])\n",
        "\n",
        "    # Ensure that the sequence batching includes padding to align all tensors to the same size\n",
        "    if input_ids_global:\n",
        "        input_ids_global = pad_sequence(input_ids_global, batch_first=True, padding_value=0)\n",
        "        attention_mask_global = pad_sequence(attention_mask_global, batch_first=True, padding_value=0)\n",
        "        labels_global = pad_sequence(labels_global, batch_first=True, padding_value=-100)\n",
        "    else:\n",
        "        input_ids_global = torch.Tensor()\n",
        "        attention_mask_global = torch.Tensor()\n",
        "        labels_global = torch.Tensor()\n",
        "\n",
        "    if input_ids_local:\n",
        "        input_ids_local = pad_sequence(input_ids_local, batch_first=True, padding_value=0)\n",
        "        attention_mask_local = pad_sequence(attention_mask_local, batch_first=True, padding_value=0)\n",
        "    else:\n",
        "        input_ids_local = torch.Tensor()\n",
        "        attention_mask_local = torch.Tensor()\n",
        "\n",
        "    return {\n",
        "        'input_ids_global': input_ids_global,\n",
        "        'attention_mask_global': attention_mask_global,\n",
        "        'labels_global': labels_global,\n",
        "        'input_ids_local': input_ids_local,\n",
        "        'attention_mask_local': attention_mask_local\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ7xnO7FseKP"
      },
      "source": [
        "#pretraining the bert model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSaaU5j6rHdO"
      },
      "source": [
        "#main class for training:\n",
        "  1. Process two sets of sequences (global and local) using BERT to extract contextual embeddings.\n",
        "  2. Integrate these two sets of embeddings using a custom attention mechanism that focuses on relevant parts of the global features for each part of the local features.\n",
        "  3. Predict an output (like interaction sites or effects) using the combined features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2qm1dRzz2pr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "c5ff2954-5068-43eb-b37c-3dc0fe1bdf38"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Example of initialization and usage\\ndef main():\\n    model_identifier = \\'bert-base-uncased\\'  # Or path to your fine-tuned BERT model\\n    model = ProteinInteractionModel(model_identifier)\\n    model.to(torch.device(\\'cuda\\' if torch.cuda.is_available() else \\'cpu\\'))\\n\\n    # Dummy data for testing\\n    tokenizer = AutoTokenizer.from_pretrained(model_identifier)\\n    text_global = \"Protein sequence A\"\\n    text_local = \"Protein sequence B\"\\n    inputs_global = tokenizer(text_global, return_tensors=\"pt\")\\n    inputs_local = tokenizer(text_local, return_tensors=\"pt\")\\n\\n    # Forward pass\\n    outputs = model(inputs_global[\\'input_ids\\'], inputs_global[\\'attention_mask\\'],\\n                    inputs_local[\\'input_ids\\'], inputs_local[\\'attention_mask\\'])\\n    print(outputs)\\n\\nif __name__ == \\'__main__\\':\\n    main()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# Define a sequence processor that can handle one channel of input (either global or local)\n",
        "class SequenceProcessor(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(SequenceProcessor, self).__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        return self.model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
        "\n",
        "# Custom attention mechanism to integrate features from both channels\n",
        "class CustomAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(CustomAttention, self).__init__()\n",
        "        self.key_layer = nn.Linear(hidden_size, hidden_size)\n",
        "        self.query_layer = nn.Linear(hidden_size, hidden_size)\n",
        "        self.value_layer = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.context_layer = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, global_features, local_features):\n",
        "        keys = self.key_layer(global_features)\n",
        "        queries = self.query_layer(local_features)\n",
        "        values = self.value_layer(global_features)\n",
        "        attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / (keys.size(-1) ** 0.5)\n",
        "        attention_weights = self.softmax(attention_scores)\n",
        "        context = torch.matmul(attention_weights, values)\n",
        "        processed_context = self.context_layer(context)\n",
        "        return processed_context\n",
        "\n",
        "# Main model to process dual input channels\n",
        "class ProteinInteractionModel(nn.Module):\n",
        "    def __init__(self, model_identifier_or_path):\n",
        "        super(ProteinInteractionModel, self).__init__()\n",
        "        self.base_model = AutoModel.from_pretrained(model_identifier_or_path)\n",
        "        hidden_size = self.base_model.config.hidden_size\n",
        "\n",
        "        self.sequence_processor_global = SequenceProcessor(self.base_model)\n",
        "        self.sequence_processor_local = SequenceProcessor(self.base_model)\n",
        "        self.custom_attention = CustomAttention(hidden_size)\n",
        "        self.mlm_head = nn.Linear(hidden_size, self.base_model.config.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids_global, attention_mask_global, input_ids_local, attention_mask_local):\n",
        "        global_features = self.sequence_processor_global(input_ids_global, attention_mask_global)\n",
        "        local_features = self.sequence_processor_local(input_ids_local, attention_mask_local)\n",
        "        combined_features = self.custom_attention(global_features, local_features)\n",
        "        prediction_scores = self.mlm_head(combined_features)\n",
        "        return prediction_scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiPlAiY8j7aY"
      },
      "source": [
        "#Training and Validation data creator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3l4rd9Hej7Gj"
      },
      "outputs": [],
      "source": [
        "#the most important part to check if the class definition and data management is correctly working\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting the dataset into 80% training and 20% testing\n",
        "train_df, test_df = train_test_split(pairs_df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Assuming ProteinInteractionDataset is implemented to handle your DataFrame structure\n",
        "train_dataset = ProteinInteractionDataset(train_df, tokenizer)\n",
        "test_dataset = ProteinInteractionDataset(test_df, tokenizer)\n",
        "\n",
        "#since the model isnt runnint we reduced the batch size to half\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouA_WiDqRUqr"
      },
      "source": [
        "# Model Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5RnrPjjRhip"
      },
      "source": [
        "#Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l02vBOukRiqq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f11bb87-220a-4cb8-bffd-b9b111234201"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Avg Loss: 0.0000\n",
            "Epoch 2, Avg Loss: 0.0000\n",
            "Epoch 3, Avg Loss: 0.0000\n",
            "Epoch 4, Avg Loss: 0.0000\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 4\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Ensure tensors are not empty and have the correct dimensions\n",
        "        if batch['input_ids_global'].nelement() == 0 or batch['input_ids_local'].nelement() == 0:\n",
        "            continue  # Skip the batch if it's empty\n",
        "\n",
        "        input_ids_global = batch['input_ids_global'].to(device)\n",
        "        attention_mask_global = batch['attention_mask_global'].to(device)\n",
        "        labels_global = batch['labels_global'].to(device)\n",
        "        input_ids_local = batch['input_ids_local'].to(device)\n",
        "        attention_mask_local = batch['attention_mask_local'].to(device)\n",
        "\n",
        "        # Check and reshape if necessary to ensure the correct dimensions\n",
        "        if input_ids_global.dim() == 1:\n",
        "            input_ids_global = input_ids_global.unsqueeze(0)\n",
        "        if input_ids_local.dim() == 1:\n",
        "            input_ids_local = input_ids_local.unsqueeze(0)\n",
        "\n",
        "        try:\n",
        "            with autocast():\n",
        "                prediction_scores = model(input_ids_global, attention_mask_global, input_ids_local, attention_mask_local)\n",
        "                loss = loss_fn(prediction_scores.view(-1, prediction_scores.size(-1)), labels_global.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to process batch: {e}\")\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}, Avg Loss: {avg_loss:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}