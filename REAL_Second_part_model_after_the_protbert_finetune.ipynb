{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kattens/Protein-Interaction-with-LLMs/blob/main/REAL_Second_part_model_after_the_protbert_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Goal of the project:\n",
        "the purpose of the model is to be able to efficiently change the number of channels which is a new way of introducing different variables and properties to the context of the protein-protein interaction, as well as being able to change the core model of the architecture to see what is the most suitable core for this approach\n",
        "\n",
        "###different core models to implement:\n",
        "1. ESM 2\n",
        "2. Llama 3\n",
        "3. ProtBert\n",
        "4. Bert\n",
        "\n",
        "###different versions of channels:\n",
        "1. local inputs with faiss\n",
        "2. local inputs from datasets based on the UniProt database\n",
        "3. coordinates of the amino acid's Calpha\n",
        "\n"
      ],
      "metadata": {
        "id": "Zm1sfYGEnRrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "id": "_KZ0eV_ywnM0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3O1GOBJjjNNw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IS-wzrL6thZf",
        "outputId": "403deed5-90b2-44dc-e46d-3772d94da0e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "CUDA not available\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CUDA not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "oYTHU87r4LVG",
        "outputId": "7925e264-f01f-46d2-cd1b-02eb81d5e561"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dD0ew8E8jXFJ"
      },
      "outputs": [],
      "source": [
        "#this is the path for the file\n",
        "csv_file = '/content/drive/MyDrive/pairs.csv'\n",
        "pairs_df = pd.read_csv(csv_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#theres some values that have ? so we should remove those rows in 'masked_sequence_A', 'masked_sequence_B'\n",
        "# Remove rows where 'masked_sequence_A' or 'masked_sequence_B' contains '?' anywhere in the sequence\n",
        "cleaned_df = pairs_df[~(pairs_df['masked_sequence_A'].str.contains('\\?', na=False) | pairs_df['masked_sequence_B'].str.contains('\\?', na=False))]\n",
        "\n",
        "# Assign the cleaned DataFrame back to pairs_df\n",
        "pairs_df = cleaned_df\n",
        "\n",
        "# Printing the sizes of the original and cleaned dataframes to verify the number of rows removed\n",
        "print(f\"Original DataFrame size: {len(pairs_df)}\")\n",
        "print(f\"Cleaned DataFrame size: {len(cleaned_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AWdcKavZwxf",
        "outputId": "f6f9376f-87af-4e00-eb1e-546c252bd4b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame size: 56340\n",
            "Cleaned DataFrame size: 56340\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMXbw_ebK8lb",
        "outputId": "02ea014b-020a-4f6c-dbaf-eb0ff912b969"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500\n"
          ]
        }
      ],
      "source": [
        "# Truncate each specified column to a maximum length of 500 characters\n",
        "columns = ['masked_sequence_A', 'masked_sequence_B', 'Sequence_A', 'Sequence_B']\n",
        "for col in columns:\n",
        "    pairs_df[col] = pairs_df[col].apply(lambda x: x[:500] if len(x) > 500 else x)\n",
        "\n",
        "# Find the longest string by length\n",
        "pairs_df['Length'] = pairs_df['masked_sequence_A'].apply(len)\n",
        "longest_string = pairs_df.loc[pairs_df['Length'].idxmax(), 'masked_sequence_A']\n",
        "print(len(longest_string))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#the number of protein chains we have\n",
        "pairs_df.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZMlodbXmDye",
        "outputId": "ce33c6c2-895d-4bef-93dc-a2c2dbe69117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "56340"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r0l1YXCvOFH"
      },
      "source": [
        "Create dataset class that handles both global sequences and local sequences for protein pairs, and potentially prepares for the inclusion of 3D structural data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32Lx8aF-7PEj"
      },
      "source": [
        "#the base Model (without coordinates at this point):\n",
        "\n",
        "  ### Modeling Interactions:\n",
        "  The mdel could be trained to recognize which amino acids interact by learning representations of local sequences that highlight these interactions. During training, the MLM objective helps the model learn contextual embeddings that are rich in information about which amino acids tend to be near each other and under what structural contexts they interact.\n",
        "\n",
        "  ### Attention Mechanism:\n",
        "   The custom attention mechanism can be used to weigh the importance of different amino acids in the global context when predicting the masked amino acids in the local sequences. This allows your model to focus more on the parts of the global sequences that are relevant to the interactions highlighted by the local sequences.\n",
        "\n",
        "  ### Utilizing Global Sequences:\n",
        "  While the local sequences are your primary interest, the global sequences provide the context necessary for your model to understand the broader environment in which the interactions occur. Even during prediction, you should feed the model the global sequences to utilize the learned context.\n",
        "\n",
        "  #### This modular design not only meets your current requirements but also provides a scalable framework to incorporate additional dimensions of protein sequence data analysis in the future\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/Checkpoints"
      ],
      "metadata": {
        "id": "1lLTXwI4wUKU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc7a7a42-1b06-40e9-95b7-d11e15fc7328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "added_tokens.json     generation_config.json   tokenizer_config.json\n",
            "config.json\t      model.safetensors        tokenizer.json\n",
            "final_checkpoint.pth  special_tokens_map.json  vocab.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#in this block of code we will intialize our core model for the architecture\n",
        "import torch\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# Define the checkpoint directory and the specific checkpoint file\n",
        "checkpoint_path = '/content/drive/MyDrive/Checkpoints/final_checkpoint.pth'\n",
        "\n",
        "# Load the tokenizer first\n",
        "tokenizer = AutoTokenizer.from_pretrained('/content/drive/MyDrive/Checkpoints')\n",
        "\n",
        "# Initialize the model from the pre-trained configuration in the Checkpoints directory\n",
        "model = AutoModelForMaskedLM.from_pretrained('/content/drive/MyDrive/Checkpoints')\n",
        "model.resize_token_embeddings(len(tokenizer))  # Important if you've added tokens\n",
        "\n",
        "# Determine the device to use (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)  # Move model to the appropriate device\n",
        "\n",
        "# Load the checkpoint\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)  # Ensure checkpoint is loaded to the correct device\n",
        "\n",
        "# Ensure all keys in the checkpoint can be loaded to the model\n",
        "missing_keys, unexpected_keys = model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
        "if missing_keys or unexpected_keys:\n",
        "    print(f\"Missing keys in state dict: {missing_keys}\")\n",
        "    print(f\"Unexpected keys in state dict: {unexpected_keys}\")\n",
        "else:\n",
        "    print(\"Model state loaded successfully.\")\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "An Optimizer and a Learning rate scheduler are being set up and their states are loaded from a checkpoint.\n",
        "This allows for the continuation of model training with the exact parameters and learning rate adjustments that were in use when the\n",
        "training was last saved, ensuring a seamless transition and consistency in the training process.\n",
        "\"\"\"\n",
        "\n",
        "# If you need the optimizer and scheduler states\n",
        "optimizer = Adam(model.parameters(), lr=checkpoint.get('learning_rate', 0.001))  # Fallback to default if not in checkpoint\n",
        "\n",
        "# Check if the optimizer state exists in the checkpoint before loading\n",
        "if 'optimizer_state_dict' in checkpoint:\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "else:\n",
        "    print(\"No optimizer state found in checkpoint; starting with a fresh optimizer.\")\n",
        "\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
        "if 'scheduler_state_dict' in checkpoint:\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "else:\n",
        "    print(\"No scheduler state found in checkpoint; using default settings.\")\n",
        "\n",
        "print(\"Model, tokenizer, optimizer, and scheduler loaded from checkpoint.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rs_CVu1caUC",
        "outputId": "6030f218-f467-4b66-a6b8-5cde088121d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model state loaded successfully.\n",
            "Model, tokenizer, optimizer, and scheduler loaded from checkpoint.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#the error is in this Dataset class."
      ],
      "metadata": {
        "id": "H-KR-yTD6zId"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApUN1McupeKe"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "class ProteinInteractionDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, mask_probability=0.15, modes=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.mask_probability = mask_probability\n",
        "        self.modes = modes if modes is not None else ['global', 'local', 'coords']\n",
        "        self.debug = True  # Set this to False to turn off debugging outputs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataframe.iloc[idx]\n",
        "        data = {}\n",
        "\n",
        "        for mode in self.modes:\n",
        "            sequence = self.get_sequence(row, mode)\n",
        "            if sequence:\n",
        "                input_ids, attention_mask = self.tokenize_sequence(sequence)\n",
        "                data[f'input_ids_{mode}'] = input_ids\n",
        "                data[f'attention_mask_{mode}'] = attention_mask\n",
        "\n",
        "                if self.debug:\n",
        "                    print(f\"{mode} mode: input_ids dimension {input_ids.shape}, attention mask dimension {attention_mask.shape}\")\n",
        "                    print(f\"{mode} mode: input_ids {input_ids}, attention mask {attention_mask}\")\n",
        "\n",
        "        return data\n",
        "\n",
        "    def get_sequence(self, row, mode):\n",
        "        \"\"\"Helper method to format sequences based on mode.\"\"\"\n",
        "        if mode == 'global':\n",
        "            return f\"[CLS] {row['Sequence_A']} [ENTITY1] [SEP] {row['Sequence_B']} [ENTITY2] [SEP]\"\n",
        "        elif mode == 'local':\n",
        "            return f\"[CLS] {row['masked_sequence_A']} [ENTITY1] [SEP] {row['masked_sequence_B']} [ENTITY2] [SEP]\"\n",
        "        elif mode == 'coords':\n",
        "            return f\"[CLS] {row['coords_A']} [ENTITY1] [SEP] {row['coords_B']} [ENTITY2] [SEP]\"\n",
        "        return None\n",
        "\n",
        "    def tokenize_sequence(self, sequence):\n",
        "        \"\"\"Tokenizes the sequence into model-ready input_ids and attention_mask.\"\"\"\n",
        "        encoded = self.tokenizer.encode_plus(\n",
        "            sequence,\n",
        "            add_special_tokens=True,  # Ensure special tokens are handled correctly\n",
        "            return_tensors='pt',\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=1024\n",
        "        )\n",
        "        return encoded['input_ids'].squeeze(0), encoded['attention_mask'].squeeze(0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame from the first row just for debug\n",
        "debug_df = pairs_df.iloc[[0]]"
      ],
      "metadata": {
        "id": "KtpgJFzA5LPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "debug_df"
      ],
      "metadata": {
        "id": "vObowD7b5MyM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "82d2c6ea-5e9c-49bb-e1c5-887eb9a2f8b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  pair_id Protein Name A Protein Name B  \\\n",
              "0    1H0J         1H0J_B         1H0J_C   \n",
              "\n",
              "                                   masked_sequence_A  \\\n",
              "0  ---------------------------A------------------...   \n",
              "\n",
              "                                   masked_sequence_B  \\\n",
              "0  ----------------------------T-----------------...   \n",
              "\n",
              "                                            coords_A  \\\n",
              "0  [(15.18, 27.908, -3.171), (13.094, 28.367, -0....   \n",
              "\n",
              "                                            coords_B  \\\n",
              "0  [(31.71, 11.761, 52.412), (31.514, 9.193, 49.6...   \n",
              "\n",
              "                                        Embeddings_A  \\\n",
              "0  tensor([ 0.1932, -0.0438,  0.7770,  0.2603, -0...   \n",
              "\n",
              "                                          Sequence_A  \\\n",
              "0  LKCNKLVPLFYKTCPAGKNLCYKMFMVATPKVPVKRGCIDVCPKSS...   \n",
              "\n",
              "                                        Embeddings_B  \\\n",
              "0  tensor([-1.9606, -0.0789, -1.0516,  1.0233, -1...   \n",
              "\n",
              "                                          Sequence_B  \\\n",
              "0  LKCNKLVPLFYKTCPAGKNLCYKMFMVATPKVPVKRGCIDVCPKSS...   \n",
              "\n",
              "                                tokenized_sequence_A  \\\n",
              "0  [5, 12, 23, 17, 12, 5, 8, 16, 5, 19, 20, 12, 1...   \n",
              "\n",
              "                                tokenized_sequence_B  \\\n",
              "0  [5, 12, 23, 17, 12, 5, 8, 16, 5, 19, 20, 12, 1...   \n",
              "\n",
              "                         tokenized_masked_sequence_A  \\\n",
              "0  [32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...   \n",
              "\n",
              "                         tokenized_masked_sequence_B  \\\n",
              "0  [32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...   \n",
              "\n",
              "                            sum_tokenized_sequence_A  \\\n",
              "0  [37, 44, 55, 49, 44, 37, 40, 48, 37, 51, 52, 4...   \n",
              "\n",
              "                            sum_tokenized_sequence_B  Length  \n",
              "0  [37, 44, 55, 49, 44, 37, 40, 48, 37, 51, 52, 4...      60  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2eaf05b0-ed96-4731-9227-c16d0b1ca5ae\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pair_id</th>\n",
              "      <th>Protein Name A</th>\n",
              "      <th>Protein Name B</th>\n",
              "      <th>masked_sequence_A</th>\n",
              "      <th>masked_sequence_B</th>\n",
              "      <th>coords_A</th>\n",
              "      <th>coords_B</th>\n",
              "      <th>Embeddings_A</th>\n",
              "      <th>Sequence_A</th>\n",
              "      <th>Embeddings_B</th>\n",
              "      <th>Sequence_B</th>\n",
              "      <th>tokenized_sequence_A</th>\n",
              "      <th>tokenized_sequence_B</th>\n",
              "      <th>tokenized_masked_sequence_A</th>\n",
              "      <th>tokenized_masked_sequence_B</th>\n",
              "      <th>sum_tokenized_sequence_A</th>\n",
              "      <th>sum_tokenized_sequence_B</th>\n",
              "      <th>Length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1H0J</td>\n",
              "      <td>1H0J_B</td>\n",
              "      <td>1H0J_C</td>\n",
              "      <td>---------------------------A------------------...</td>\n",
              "      <td>----------------------------T-----------------...</td>\n",
              "      <td>[(15.18, 27.908, -3.171), (13.094, 28.367, -0....</td>\n",
              "      <td>[(31.71, 11.761, 52.412), (31.514, 9.193, 49.6...</td>\n",
              "      <td>tensor([ 0.1932, -0.0438,  0.7770,  0.2603, -0...</td>\n",
              "      <td>LKCNKLVPLFYKTCPAGKNLCYKMFMVATPKVPVKRGCIDVCPKSS...</td>\n",
              "      <td>tensor([-1.9606, -0.0789, -1.0516,  1.0233, -1...</td>\n",
              "      <td>LKCNKLVPLFYKTCPAGKNLCYKMFMVATPKVPVKRGCIDVCPKSS...</td>\n",
              "      <td>[5, 12, 23, 17, 12, 5, 8, 16, 5, 19, 20, 12, 1...</td>\n",
              "      <td>[5, 12, 23, 17, 12, 5, 8, 16, 5, 19, 20, 12, 1...</td>\n",
              "      <td>[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...</td>\n",
              "      <td>[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 3...</td>\n",
              "      <td>[37, 44, 55, 49, 44, 37, 40, 48, 37, 51, 52, 4...</td>\n",
              "      <td>[37, 44, 55, 49, 44, 37, 40, 48, 37, 51, 52, 4...</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2eaf05b0-ed96-4731-9227-c16d0b1ca5ae')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2eaf05b0-ed96-4731-9227-c16d0b1ca5ae button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2eaf05b0-ed96-4731-9227-c16d0b1ca5ae');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_ae68710b-433b-47b8-ab3c-f0b539f5819b\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('debug_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_ae68710b-433b-47b8-ab3c-f0b539f5819b button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('debug_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "debug_df",
              "summary": "{\n  \"name\": \"debug_df\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"pair_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"1H0J\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Protein Name A\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"1H0J_B\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Protein Name B\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"1H0J_C\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"masked_sequence_A\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"---------------------------A--------------------------------\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"masked_sequence_B\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"----------------------------T-------------------------------\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"coords_A\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"[(15.18, 27.908, -3.171), (13.094, 28.367, -0.025), (14.194, 27.287, 3.457), (12.376, 27.246, 6.78), (13.433, 29.565, 9.596), (13.855, 28.657, 13.272), (10.063, 28.507, 13.604), (9.22, 26.247, 10.59), (5.886, 28.037, 10.185), (7.91, 30.686, 8.37), (10.378, 30.565, 5.496), (12.587, 32.72, 3.312), (13.688, 32.767, -0.313), (17.42, 32.129, -0.37), (19.611, 34.968, -1.697), (22.141, 34.483, -4.495), (24.969, 32.155, -3.525), (22.935, 30.08, -1.064), (21.355, 26.982, -2.619), (21.126, 24.751, 0.439), (18.858, 24.501, 3.494), (20.056, 23.276, 6.853), (18.983, 22.483, 10.395), (21.401, 22.584, 13.332), (20.945, 20.18, 16.248), (22.591, 20.201, 19.69), (23.191, 16.561, 20.751), (22.52, 17.342, 24.43), (19.176, 19.036, 23.636), (18.214, 17.435, 20.249), (14.432, 17.709, 20.578), (14.362, 20.494, 17.989), (16.898, 22.328, 15.824), (18.34, 25.483, 17.385), (19.029, 27.114, 14.031), (17.637, 26.781, 10.494), (17.993, 28.647, 7.228), (19.683, 29.036, 3.859), (23.419, 28.55, 3.401), (26.071, 27.909, 0.717), (27.966, 24.951, 2.152), (27.015, 22.249, 4.62), (28.735, 22.95, 7.965), (30.877, 20.111, 9.276), (29.133, 18.142, 12.029), (30.991, 17.409, 15.266), (30.651, 15.387, 18.464), (27.983, 17.624, 20.035), (26.63, 19.551, 17.03), (24.85, 17.861, 14.11), (23.835, 19.4, 10.796), (21.174, 18.167, 8.375), (21.516, 19.645, 4.882), (19.112, 19.429, 1.93), (18.293, 21.434, -1.206), (14.524, 21.148, -1.678), (11.945, 23.848, -0.918), (11.402, 23.89, 2.848), (13.097, 20.472, 3.271), (14.564, 21.644, 6.587)]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"coords_B\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"[(31.71, 11.761, 52.412), (31.514, 9.193, 49.625), (29.806, 9.814, 46.288), (28.997, 7.649, 43.263), (30.723, 8.167, 39.909), (29.557, 8.082, 36.274), (28.425, 4.482, 36.793), (26.436, 3.921, 40.053), (28.42, 0.731, 40.678), (31.502, 2.819, 41.473), (32.137, 5.564, 44.037), (34.895, 7.446, 45.873), (35.64, 9.566, 48.946), (35.353, 13.329, 48.486), (38.41, 15.548, 49.044), (38.537, 18.376, 51.577), (36.422, 21.198, 50.205), (33.629, 19.115, 48.705), (30.888, 18.063, 51.091), (28.204, 17.303, 48.511), (27.334, 14.519, 46.096), (25.61, 15.561, 42.889), (23.872, 14.274, 39.786), (23.511, 16.307, 36.594), (20.58, 15.592, 34.257), (19.775, 16.859, 30.773), (15.989, 17.066, 30.307), (16.381, 15.039, 27.09), (17.798, 12.164, 29.134), (15.804, 12.462, 32.424), (16.547, 8.893, 33.501), (20.343, 8.881, 33.205), (22.538, 11.647, 34.655), (25.367, 12.788, 32.387), (27.643, 13.656, 35.304), (28.097, 12.398, 38.885), (30.557, 12.982, 41.728), (31.618, 15.351, 44.523), (31.234, 19.104, 44.436), (31.608, 22.247, 46.545), (28.841, 24.57, 45.389), (25.592, 22.91, 44.312), (24.92, 24.034, 40.701), (21.77, 25.977, 39.816), (19.026, 24.022, 38.035), (17.55, 25.37, 34.813), (14.705, 24.605, 32.433), (16.88, 22.113, 30.526), (19.556, 21.117, 33.043), (18.644, 19.609, 36.411), (20.921, 19.089, 39.409), (20.406, 16.906, 42.472), (22.637, 17.605, 45.479), (22.719, 15.634, 48.717), (25.172, 15.126, 51.589), (24.92, 11.422, 52.467), (27.049, 8.395, 51.55), (26.396, 7.322, 47.956), (23.204, 9.397, 47.911), (23.814, 10.282, 44.261)]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Embeddings_A\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"tensor([ 0.1932, -0.0438,  0.7770,  0.2603, -0.3194, -1.7208, -1.4078, -0.6996,\\n         0.0189, -0.1269, -0.1114,  0.0289, -0.0243, -0.4486,  0.3975, -0.9184,\\n        -0.5971, -0.6794,  0.5902, -0.6271,  0.7077, -1.0189, -0.6838,  0.1389,\\n         0.5883,  0.3247, -0.3489,  0.8280, -0.3439,  0.5802, -0.7491, -0.2291,\\n        -0.5009, -0.7292, -0.1835,  0.5171,  1.4258,  1.8320,  0.5417,  0.8492,\\n        -0.8175,  2.7374,  1.0742,  0.1407,  0.4130,  0.5768,  0.7884,  0.2863,\\n        -1.2062, -0.0364,  1.6911, -0.0357,  0.2333, -0.4863, -0.4992,  0.1613,\\n        -0.0985, -0.6607, -1.8245,  0.5379, -0.9565,  1.0563, -1.4902,  1.6625,\\n         1.3829,  1.7928,  1.1537,  0.2394, -0.4484,  1.8955, -0.8740, -0.8465,\\n         0.3923, -1.5792,  0.4460,  0.4548, -2.0304,  1.0429,  0.7816,  0.6989,\\n        -0.1515,  0.1173,  0.4065,  0.6353, -0.4137,  1.4591,  1.0226, -0.9343,\\n         0.8768, -0.5274, -0.7074, -0.4752,  0.9429, -0.6944,  1.4983, -0.7199,\\n         0.1939,  0.4820, -0.2272,  0.7446, -0.7093, -1.4029, -1.5726,  1.4787,\\n         0.9897, -0.1934,  2.2563, -2.1228, -1.4990,  0.4615,  0.7031, -1.9541,\\n         0.0641,  0.5342,  1.0368,  1.1606,  1.1048,  0.6281,  0.7778, -1.0983,\\n        -1.7432,  0.2546,  0.9125, -0.3288,  1.5194, -1.6024, -1.2193, -0.0449])\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sequence_A\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"LKCNKLVPLFYKTCPAGKNLCYKMFMVATPKVPVKRGCIDVCPKSSLLVKYVCCNTDRCN\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Embeddings_B\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"tensor([-1.9606, -0.0789, -1.0516,  1.0233, -1.9376,  0.0048,  1.4564, -0.2826,\\n         0.9928, -0.7321,  0.2605, -1.1617, -1.9045, -0.5094,  1.6443,  0.8993,\\n        -0.5728, -0.4137,  1.3776,  0.3589, -0.1369, -0.1433,  0.2082, -2.1259,\\n         0.5695, -0.1116,  0.1403, -1.2130,  0.0179, -1.8902, -1.0072,  0.2011,\\n        -1.3431, -0.0556,  1.9390, -1.0001, -0.0221, -0.4739, -0.3450,  1.5603,\\n        -0.9024, -2.3155, -1.3712, -0.6494,  0.0187,  0.7018,  1.1420, -0.2633,\\n         0.5399, -0.8688, -0.2663,  0.4806,  0.4817, -1.5876,  1.9451,  0.4453,\\n         0.2921, -0.0622, -1.0344,  0.0873,  0.5145, -1.0429,  0.9856,  1.5482,\\n        -1.6083, -1.8402, -3.4806,  1.2436, -0.7513,  1.2344,  0.0140, -0.5014,\\n        -0.5876, -0.6243,  1.6610, -1.1263, -2.0587, -1.0505,  0.3005,  0.4084,\\n         0.1140, -1.8573,  1.1576, -1.1603,  0.6983, -0.0543, -0.1374,  1.4700,\\n        -2.2551,  1.9024, -0.4222, -1.7369,  1.1721, -0.7713, -1.2732,  0.1466,\\n        -0.1775,  0.2976,  1.2071,  0.9345,  0.6621,  1.1680, -1.3691, -1.1835,\\n        -0.9677,  0.2014,  1.4170, -1.3794, -0.0754,  1.3146, -0.8791, -0.5325,\\n        -1.9114,  2.4167, -0.9692, -1.5768,  0.3330,  0.5996, -0.4446, -1.9533,\\n        -0.4040,  0.5546,  0.2771,  0.8048,  1.1107,  1.0745,  0.2420,  0.7703])\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sequence_B\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"LKCNKLVPLFYKTCPAGKNLCYKMFMVATPKVPVKRGCIDVCPKSSLLVKYVCCNTDRCN\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokenized_sequence_A\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"[5, 12, 23, 17, 12, 5, 8, 16, 5, 19, 20, 12, 15, 23, 16, 6, 7, 12, 17, 5, 23, 20, 12, 21, 19, 21, 8, 6, 15, 16, 12, 8, 16, 8, 12, 13, 7, 23, 11, 14, 8, 23, 16, 12, 10, 10, 5, 5, 8, 12, 20, 8, 23, 23, 17, 15, 14, 13, 23, 17]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokenized_sequence_B\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"[5, 12, 23, 17, 12, 5, 8, 16, 5, 19, 20, 12, 15, 23, 16, 6, 7, 12, 17, 5, 23, 20, 12, 21, 19, 21, 8, 6, 15, 16, 12, 8, 16, 8, 12, 13, 7, 23, 11, 14, 8, 23, 16, 12, 10, 10, 5, 5, 8, 12, 20, 8, 23, 23, 17, 15, 14, 13, 23, 17]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokenized_masked_sequence_A\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 6, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokenized_masked_sequence_B\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"[32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 15, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sum_tokenized_sequence_A\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"[37, 44, 55, 49, 44, 37, 40, 48, 37, 51, 52, 44, 47, 55, 48, 38, 39, 44, 49, 37, 55, 52, 44, 53, 51, 53, 40, 12, 47, 48, 44, 40, 48, 40, 44, 45, 39, 55, 43, 46, 40, 55, 48, 44, 42, 42, 37, 37, 40, 44, 52, 40, 55, 55, 49, 47, 46, 45, 55, 49]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sum_tokenized_sequence_B\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"[37, 44, 55, 49, 44, 37, 40, 48, 37, 51, 52, 44, 47, 55, 48, 38, 39, 44, 49, 37, 55, 52, 44, 53, 51, 53, 40, 38, 30, 48, 44, 40, 48, 40, 44, 45, 39, 55, 43, 46, 40, 55, 48, 44, 42, 42, 37, 37, 40, 44, 52, 40, 55, 55, 49, 47, 46, 45, 55, 49]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 60,\n        \"max\": 60,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          60\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "'''\n",
        "# Initialize the ProtBERT tokenizer -> we dont need it since our tokenizer works perfectly. if we dont have access to the checkpoint we can use this:\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('Rostlab/prot_bert')\n",
        "'''\n",
        "\n",
        "# Print the length of each sequence column, considering each element as a string\n",
        "print(f\"Sequence_A length: {sum(len(seq) for seq in debug_df['Sequence_A'])}\")\n",
        "print(f\"Sequence_B length: {sum(len(seq) for seq in debug_df['Sequence_B'])}\")\n",
        "print(f\"Masked_sequence_A length: {sum(len(seq) for seq in debug_df['masked_sequence_A'])}\")\n",
        "print(f\"Masked_sequence_B length: {sum(len(seq) for seq in debug_df['masked_sequence_B'])}\")\n",
        "\n",
        "# Sample global sequence preparation\n",
        "global_seq = \"[CLS] \" + \" [ENTITY1] \".join(debug_df['Sequence_A']) + \" [SEP] \" + \" [ENTITY2] \".join(debug_df['Sequence_B']) + \" [SEP]\"\n",
        "local_seq = \"[CLS] \" + \" [ENTITY1] \".join(debug_df['masked_sequence_A']) + \" [SEP] \" + \" [ENTITY2] \".join(debug_df['masked_sequence_B']) + \" [SEP]\"\n",
        "coords_seq = \"[CLS] \" + \" [ENTITY1] \".join(debug_df['coords_A']) + \" [SEP] \" + \" [ENTITY2] \".join(debug_df['coords_B']) + \" [SEP]\"\n",
        "\n",
        "# Calculate the total length of the sequences\n",
        "global_seq_length = len(global_seq)\n",
        "local_seq_length = len(local_seq)\n",
        "coords_seq_length = len(coords_seq)\n",
        "\n",
        "# Print the length\n",
        "print(f\"Global sequence length: {global_seq_length}\")\n",
        "print(f\"Local sequence length: {local_seq_length}\")\n",
        "print(f\"coords sequence length: {coords_seq_length}\")\n",
        "\n",
        "# Assuming ProteinInteractionDataset is properly defined\n",
        "dataset_global = ProteinInteractionDataset(debug_df, tokenizer, modes=['global'])\n",
        "dataset_local = ProteinInteractionDataset(debug_df, tokenizer, modes=['local'])\n",
        "dataset_coords = ProteinInteractionDataset(debug_df, tokenizer, modes=['coords'])\n",
        "\n",
        "# Fetch data for the first entry in each dataset\n",
        "data_global = dataset_global[0]\n",
        "data_local = dataset_local[0]\n",
        "data_coords = dataset_coords[0]\n",
        "\n",
        "# Print the data for debugging\n",
        "print(\"Global Mode Data:\")\n",
        "for key, value in data_global.items():\n",
        "    print(f\"{key}: {value.shape}\")\n",
        "\n",
        "print(\"\\nLocal Mode Data:\")\n",
        "for key, value in data_local.items():\n",
        "    print(f\"{key}: {value.shape}\")\n",
        "\n",
        "print(\"\\ncoords Mode Data:\")\n",
        "for key, value in data_coords.items():\n",
        "    print(f\"{key}: {value.shape}\")\n"
      ],
      "metadata": {
        "id": "F88JVYsZltdC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de4e9d3a-3be2-49fe-c6cc-fb9e4f37a815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence_A length: 60\n",
            "Sequence_B length: 60\n",
            "Masked_sequence_A length: 60\n",
            "Masked_sequence_B length: 60\n",
            "Global sequence length: 139\n",
            "Local sequence length: 139\n",
            "coords sequence length: 3054\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "Global Mode Data:\n",
            "input_ids_global: torch.Size([1024])\n",
            "attention_mask_global: torch.Size([1024])\n",
            "\n",
            "Local Mode Data:\n",
            "input_ids_local: torch.Size([1024])\n",
            "attention_mask_local: torch.Size([1024])\n",
            "\n",
            "coords Mode Data:\n",
            "input_ids_coords: torch.Size([1024])\n",
            "attention_mask_coords: torch.Size([1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRffIAsuVZo6"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Initialize containers for batch data\n",
        "    global_input_ids = []\n",
        "    global_attention_masks = []\n",
        "    local_input_ids = []\n",
        "    local_attention_masks = []\n",
        "\n",
        "    # Collect data for each sample in the batch\n",
        "    for item in batch:\n",
        "        global_input_ids.append(item['input_ids_global'])\n",
        "        global_attention_masks.append(item['attention_mask_global'])\n",
        "        local_input_ids.append(item['input_ids_local'])\n",
        "        local_attention_masks.append(item['attention_mask_local'])\n",
        "\n",
        "    # Pad sequences so they all have the same length within the batch\n",
        "    global_input_ids = pad_sequence(global_input_ids, batch_first=True, padding_value=0)\n",
        "    global_attention_masks = pad_sequence(global_attention_masks, batch_first=True, padding_value=0)\n",
        "    local_input_ids = pad_sequence(local_input_ids, batch_first=True, padding_value=0)\n",
        "    local_attention_masks = pad_sequence(local_attention_masks, batch_first=True, padding_value=0)\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    return {\n",
        "        'global_input_ids': global_input_ids,\n",
        "        'global_attention_masks': global_attention_masks,\n",
        "        'local_input_ids': local_input_ids,\n",
        "        'local_attention_masks': local_attention_masks\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ7xnO7FseKP"
      },
      "source": [
        "#pretraining the bert model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSaaU5j6rHdO"
      },
      "source": [
        "#main class for training:\n",
        "  1. Process two sets of sequences (global and local) using BERT to extract contextual embeddings.\n",
        "  2. Integrate these two sets of embeddings using a custom attention mechanism that focuses on relevant parts of the global features for each part of the local features.\n",
        "  3. Predict an output (like interaction sites or effects) using the combined features."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#simple class for only 2 channel input of global and local sequneces"
      ],
      "metadata": {
        "id": "XpGLeEjQF5Iu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2qm1dRzz2pr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import AutoModel\n",
        "\n",
        "class SequenceProcessor(nn.Module):\n",
        "    \"\"\"A module to process sequences using a pre-trained Transformer model.\n",
        "    It extracts the last hidden states, which serve as features for further processing.\"\"\"\n",
        "    def __init__(self, model):\n",
        "        super(SequenceProcessor, self).__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        \"\"\"Perform the forward pass to get sequence features.\n",
        "        Args:\n",
        "            input_ids (torch.Tensor): Tensor of input token IDs.\n",
        "            attention_mask (torch.Tensor, optional): Tensor indicating which tokens should be attended to.\n",
        "        Returns:\n",
        "            torch.Tensor: Tensor of last hidden states representing features for each token.\"\"\"\n",
        "        return self.model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
        "\n",
        "class GuidedAttention(nn.Module):\n",
        "    \"\"\"A module that implements a guided attention mechanism to focus on specific parts of a sequence\n",
        "    based on additional local sequence context.\"\"\"\n",
        "    def __init__(self, hidden_size):\n",
        "        super(GuidedAttention, self).__init__()\n",
        "        self.key_layer = nn.Linear(hidden_size, hidden_size)\n",
        "        self.query_layer = nn.Linear(hidden_size, hidden_size)\n",
        "        self.value_layer = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.context_layer = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, global_features, local_features):\n",
        "        \"\"\"Calculate the attention-driven context vector.\n",
        "        Args:\n",
        "            global_features (torch.Tensor): Feature representations of the global sequence.\n",
        "            local_features (torch.Tensor): Feature representations of the local sequence used for queries.\n",
        "        Returns:\n",
        "            torch.Tensor: Processed context vector after applying guided attention.\"\"\"\n",
        "        keys = self.key_layer(global_features)\n",
        "        queries = self.query_layer(local_features)\n",
        "        values = self.value_layer(global_features)\n",
        "        attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / (keys.size(-1) ** 0.5)\n",
        "        attention_weights = self.softmax(attention_scores)\n",
        "        context = torch.matmul(attention_weights, values)\n",
        "        processed_context = self.context_layer(context)\n",
        "        return processed_context\n",
        "\n",
        "class ProteinInteractionModel(nn.Module):\n",
        "    \"\"\"Main model class for predicting protein interaction sites using global and local sequence data.\n",
        "    This model integrates sequence processing, guided attention, and classification.\"\"\"\n",
        "    def __init__(self, model_identifier_or_path, num_labels=21):\n",
        "        super(ProteinInteractionModel, self).__init__()\n",
        "        self.base_model = AutoModel.from_pretrained(model_identifier_or_path)\n",
        "        hidden_size = self.base_model.config.hidden_size\n",
        "\n",
        "        self.sequence_processor_global = SequenceProcessor(self.base_model)\n",
        "        self.sequence_processor_local = SequenceProcessor(self.base_model)\n",
        "        self.guided_attention = GuidedAttention(hidden_size)\n",
        "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids_global, attention_mask_global, input_ids_local, attention_mask_local):\n",
        "        \"\"\"Forward computation to predict labels for each sequence position.\n",
        "        Args:\n",
        "            input_ids_global (torch.Tensor): Input IDs for the global sequence.\n",
        "            attention_mask_global (torch.Tensor): Attention mask for the global sequence.\n",
        "            input_ids_local (torch.Tensor): Input IDs for the local sequence.\n",
        "            attention_mask_local (torch.Tensor): Attention mask for the local sequence.\n",
        "        Returns:\n",
        "            torch.Tensor: Logits for each sequence position indicating predicted label probabilities.\"\"\"\n",
        "        global_features = self.sequence_processor_global(input_ids_global, attention_mask_global)\n",
        "        local_features = self.sequence_processor_local(input_ids_local, attention_mask_local)\n",
        "        focused_features = self.guided_attention(global_features, local_features)\n",
        "        logits = self.classifier(focused_features)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#more complecated and flexible architecture for adding more channels of input"
      ],
      "metadata": {
        "id": "ofraCddXF-o-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''import torch\n",
        "from torch import nn\n",
        "from transformers import AutoModel\n",
        "\n",
        "class SequenceProcessor(nn.Module):\n",
        "    \"\"\"Processes sequences using a pre-trained Transformer model to extract features.\"\"\"\n",
        "    def __init__(self, model):\n",
        "        super(SequenceProcessor, self).__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        \"\"\"Extracts the last hidden states as features.\"\"\"\n",
        "        return self.model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
        "\n",
        "class GuidedAttention(nn.Module):\n",
        "    \"\"\"Implements attention to focus on relevant parts of a sequence using additional context from other channels.\"\"\"\n",
        "    def __init__(self, hidden_size):\n",
        "        super(GuidedAttention, self).__init__()\n",
        "        # Define layers for attention mechanism\n",
        "        self.key_layer = nn.Linear(hidden_size, hidden_size)\n",
        "        self.query_layer = nn.Linear(hidden_size, hidden_size)\n",
        "        self.value_layer = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.context_layer = nn.Linear(hidden_size * 2, hidden_size)  # Combines main and context features\n",
        "\n",
        "    def forward(self, main_features, additional_features):\n",
        "        \"\"\"Combines main sequence features with additional features through attention.\"\"\"\n",
        "        keys = self.key_layer(main_features)\n",
        "        queries = self.query_layer(additional_features)\n",
        "        values = self.value_layer(main_features)\n",
        "\n",
        "        # Compute attention scores and weights\n",
        "        attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / (keys.size(-1) ** 0.5)\n",
        "        attention_weights = self.softmax(attention_scores)\n",
        "\n",
        "        # Form context vector and combine it with main features\n",
        "        context = torch.matmul(attention_weights, values)\n",
        "        combined_features = torch.cat((main_features, context), dim=-1)\n",
        "        processed_context = self.context_layer(combined_features)\n",
        "        return processed_context\n",
        "\n",
        "class FlexibleProteinInteractionModel(nn.Module):\n",
        "    \"\"\"Predicts protein interaction sites using multiple sequence data channels with attention integration.\"\"\"\n",
        "    def __init__(self, model_identifier_or_path, num_labels):\n",
        "        super(FlexibleProteinInteractionModel, self).__init__()\n",
        "        self.base_model = AutoModel.from_pretrained(model_identifier_or_path)\n",
        "        hidden_size = self.base_model.config.hidden_size\n",
        "\n",
        "        # Processor for primary sequence features\n",
        "        self.global_sequence_processor = SequenceProcessor(self.base_model)\n",
        "        # Processor for secondary features (e.g., local sequences or coordinates)\n",
        "        self.additional_feature_processor = SequenceProcessor(self.base_model)\n",
        "        # Guided attention to integrate features from both processors\n",
        "        self.guided_attention = GuidedAttention(hidden_size)\n",
        "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids_global, attention_mask_global, input_ids_additional, attention_mask_additional):\n",
        "        \"\"\"Processes multiple channels of input data and predicts outcomes.\"\"\"\n",
        "        global_features = self.global_sequence_processor(input_ids_global, attention_mask_global)\n",
        "        additional_features = self.additional_feature_processor(input_ids_additional, attention_mask_additional)\n",
        "\n",
        "        # Integrate global and additional features using guided attention\n",
        "        integrated_features = self.guided_attention(global_features, additional_features)\n",
        "\n",
        "        # Classify based on integrated features\n",
        "        logits = self.classifier(integrated_features)\n",
        "        return logits\n",
        "'''"
      ],
      "metadata": {
        "id": "TXs0Krz-F-Vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiPlAiY8j7aY"
      },
      "source": [
        "#Training and Validation data creator:\n",
        "The DataLoader in PyTorch is a powerful utility that simplifies data handling when training neural networks. It's particularly useful when working with large datasets that cannot fit entirely into memory, or when you need sophisticated data loading and batching capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3l4rd9Hej7Gj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5710630-2b48-4061-edbc-0598ebc1177a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total dataset size: 56340\n",
            "Training data size: 45072\n",
            "Testing data size: 11268\n",
            "Number of batches in train_loader: 2817\n",
            "Each batch has 16 samples.\n",
            "Number of batches in test_loader: 705\n",
            "Each batch has 16 samples.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting the dataset into 80% training and 20% testing\n",
        "train_df, test_df = train_test_split(pairs_df, test_size=0.2, random_state=42)\n",
        "print(f\"Total dataset size: {len(pairs_df)}\")\n",
        "print(f\"Training data size: {len(train_df)}\")\n",
        "print(f\"Testing data size: {len(test_df)}\")\n",
        "\n",
        "# Assuming ProteinInteractionDataset is correctly implemented to handle DataFrame inputs and a tokenizer\n",
        "train_dataset = ProteinInteractionDataset(train_df, tokenizer)\n",
        "test_dataset = ProteinInteractionDataset(test_df, tokenizer)\n",
        "\n",
        "# Create DataLoader objects for both training and testing datasets\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Log the number of batches in each DataLoader\n",
        "print(f\"Number of batches in train_loader: {len(train_loader)}\")\n",
        "print(f\"Each batch has {train_loader.batch_size} samples.\")\n",
        "print(f\"Number of batches in test_loader: {len(test_loader)}\")\n",
        "print(f\"Each batch has {test_loader.batch_size} samples.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouA_WiDqRUqr"
      },
      "source": [
        "# Model Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5RnrPjjRhip"
      },
      "source": [
        "#Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract some batches and check their max token IDs\n",
        "for batch in train_loader:  # Assuming loader is your DataLoader instance\n",
        "    max_id_global = batch['input_ids_global'].max()\n",
        "    max_id_local = batch['input_ids_local'].max()\n",
        "    max_id_coords = batch['input_ids_coords'].max()  # Assuming you have coords\n",
        "    print(\"Max ID Global:\", max_id_global.item())\n",
        "    print(\"Max ID Local:\", max_id_local.item())\n",
        "    print(\"Max ID Coords:\", max_id_coords.item())\n",
        "    break  # Remove break to check more batches\n",
        "\n",
        "# Check against the model's embedding layer size\n",
        "max_embedding_id = model.base_model.embeddings.word_embeddings.num_embeddings - 1\n",
        "print(\"Max embedding index allowed:\", max_embedding_id)\n",
        "\n",
        "# Adjusting the model embeddings to be slightly larger than the max token ID used\n",
        "required_embedding_size = max(max_id_global.item(), max_id_local.item(), max_id_coords.item()) + 1  # Plus one for safe measure\n",
        "if required_embedding_size > model.base_model.embeddings.word_embeddings.num_embeddings:\n",
        "    model.base_model.resize_token_embeddings(required_embedding_size)\n"
      ],
      "metadata": {
        "id": "exrXvVXqYrVt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b275443-806e-4788-cc88-3814e24a4308"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([2, 2, 6,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([ 2,  2,  1,  ...,  1, 32,  3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "Max ID Global: 31\n",
            "Max ID Local: 32\n",
            "Max ID Coords: 32\n",
            "Max embedding index allowed: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l02vBOukRiqq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d557079d-53c7-4abc-e2bf-7599da544759"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([2, 2, 5,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([ 2,  2,  1,  ..., 32,  1,  3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "global mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "global mode: input_ids tensor([2, 2, 1,  ..., 0, 0, 0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "local mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "local mode: input_ids tensor([ 2,  2, 32,  ...,  0,  0,  0]), attention mask tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "coords mode: input_ids dimension torch.Size([1024]), attention mask dimension torch.Size([1024])\n",
            "coords mode: input_ids tensor([2, 2, 1,  ..., 1, 1, 3]), attention mask tensor([1, 1, 1,  ..., 1, 1, 1])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-b9959e0658f6>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Perform a forward pass through the model using global sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids_global\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask_global\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Compute loss using the outputs from the model and the local sequences as labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1485\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1487\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1488\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1489\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1137\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1138\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    688\u001b[0m                 )\n\u001b[1;32m    689\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    691\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    620\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    623\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "# Assume model, criterion, optimizer, scheduler, train_loader are already defined and configured.\n",
        "\n",
        "num_epochs = 2  # You can adjust the number of epochs based on your training requirements\n",
        "scaler = GradScaler()  # Initialize the GradScaler for mixed precision training\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    total_loss = 0\n",
        "    batch_count = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Move data to the appropriate device (GPU or CPU)\n",
        "        input_ids_global = batch['input_ids_global'].to(device)\n",
        "        attention_mask_global = batch['attention_mask_global'].to(device)\n",
        "        labels = batch['input_ids_local'].to(device)  # Treat local sequences as labels\n",
        "        attention_mask_local = batch['attention_mask_local'].to(device)  # Might be used depending on model\n",
        "\n",
        "        # Use automatic mixed precision\n",
        "        with autocast():\n",
        "            # Perform a forward pass through the model using global sequences\n",
        "            outputs = model(input_ids_global, attention_mask_global)\n",
        "\n",
        "            # Compute loss using the outputs from the model and the local sequences as labels\n",
        "            loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
        "\n",
        "        # Scale the loss and call backward() to create scaled gradients\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # Perform a scaler step. This unscales gradients and calls or skips optimizer.step()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Accumulate the loss\n",
        "        total_loss += loss.item()\n",
        "        batch_count += 1\n",
        "\n",
        "    # Clear any unused memory to prevent CUDA out of memory errors\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Update the learning rate scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # Calculate average loss over all batches\n",
        "    average_loss = total_loss / batch_count\n",
        "    print(f'Epoch: {epoch+1}, Loss: {average_loss:.4f}')\n",
        "\n",
        "    # Optionally, add code for validation here to evaluate the model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fbn1dIZs4LVX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}